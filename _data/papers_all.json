[{"url": "https://arxiv.org/abs/1706.03762", "title": "Attention Is All You Need", "cites": 70261}, {"url": "https://arxiv.org/abs/1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 60973}, {"url": "https://arxiv.org/abs/1310.4546", "title": "Distributed Representations of Words and Phrases and their\n  Compositionality", "cites": 30228}, {"url": "https://arxiv.org/abs/1301.3781", "title": "Efficient Estimation of Word Representations in Vector Space", "cites": 27018}, {"url": "https://arxiv.org/abs/1409.0473", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "cites": 23683}, {"url": "https://arxiv.org/abs/1406.1078", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "cites": 19189}, {"url": "https://arxiv.org/abs/1409.3215", "title": "Sequence to Sequence Learning with Neural Networks", "cites": 17740}, {"url": "https://arxiv.org/abs/2005.14165", "title": "Language Models are Few-Shot Learners", "cites": 15614}, {"url": "https://arxiv.org/abs/1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 14845}, {"url": "https://arxiv.org/abs/1408.5882", "title": "Convolutional Neural Networks for Sentence Classification", "cites": 12041}, {"url": "https://arxiv.org/abs/1802.05365", "title": "Deep contextualized word representations", "cites": 10004}, {"url": "https://arxiv.org/abs/1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 9737}, {"url": "https://arxiv.org/abs/1607.04606", "title": "Enriching Word Vectors with Subword Information", "cites": 8353}, {"url": "https://arxiv.org/abs/1405.4053", "title": "Distributed Representations of Sentences and Documents", "cites": 8320}, {"url": "https://arxiv.org/abs/1303.5778", "title": "Speech Recognition with Deep Recurrent Neural Networks", "cites": 7809}, {"url": "https://arxiv.org/abs/1508.04025", "title": "Effective Approaches to Attention-based Neural Machine Translation", "cites": 7067}, {"url": "https://arxiv.org/abs/1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 6375}, {"url": "https://arxiv.org/abs/1508.07909", "title": "Neural Machine Translation of Rare Words with Subword Units", "cites": 6145}, {"url": "https://arxiv.org/abs/1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 6112}, {"url": "https://arxiv.org/abs/1606.05250", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "cites": 5982}, {"url": "https://arxiv.org/abs/1609.08144", "title": "Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation", "cites": 5801}, {"url": "https://arxiv.org/abs/1409.1259", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches", "cites": 5630}, {"url": "https://arxiv.org/abs/1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 5120}, {"url": "https://arxiv.org/abs/1509.01626", "title": "Character-level Convolutional Networks for Text Classification", "cites": 4666}, {"url": "https://arxiv.org/abs/1909.11942", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 4571}, {"url": "https://arxiv.org/abs/1804.07461", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding", "cites": 4504}, {"url": "https://arxiv.org/abs/1910.01108", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 4244}, {"url": "https://arxiv.org/abs/1910.03771", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 4210}, {"url": "https://arxiv.org/abs/1505.00468", "title": "VQA: Visual Question Answering", "cites": 4012}, {"url": "https://arxiv.org/abs/1607.01759", "title": "Bag of Tricks for Efficient Text Classification", "cites": 3923}, {"url": "https://arxiv.org/abs/1911.02116", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 3692}, {"url": "https://arxiv.org/abs/1308.0850", "title": "Generating Sequences With Recurrent Neural Networks", "cites": 3596}, {"url": "https://arxiv.org/abs/1603.01360", "title": "Neural Architectures for Named Entity Recognition", "cites": 3549}, {"url": "https://arxiv.org/abs/1901.08746", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 3433}, {"url": "https://arxiv.org/abs/1508.05326", "title": "A large annotated corpus for learning natural language inference", "cites": 3387}, {"url": "https://arxiv.org/abs/1404.2188", "title": "A Convolutional Neural Network for Modelling Sentences", "cites": 3357}, {"url": "https://arxiv.org/abs/1806.09055", "title": "DARTS: Differentiable Architecture Search", "cites": 3299}, {"url": "https://arxiv.org/abs/1508.01991", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "cites": 3280}, {"url": "https://arxiv.org/abs/1704.04368", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "cites": 3267}, {"url": "https://arxiv.org/abs/1704.05426", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through\n  Inference", "cites": 3190}, {"url": "https://arxiv.org/abs/1411.5726", "title": "CIDEr: Consensus-based Image Description Evaluation", "cites": 3106}, {"url": "https://arxiv.org/abs/1506.03340", "title": "Teaching Machines to Read and Comprehend", "cites": 2998}, {"url": "https://arxiv.org/abs/1803.01271", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "cites": 2992}, {"url": "https://arxiv.org/abs/1705.03122", "title": "Convolutional Sequence to Sequence Learning", "cites": 2933}, {"url": "https://arxiv.org/abs/1503.00075", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "cites": 2889}, {"url": "https://arxiv.org/abs/1705.02315", "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "cites": 2798}, {"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": 2798}, {"url": "https://arxiv.org/abs/1901.02860", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 2721}, {"url": "https://arxiv.org/abs/2203.02155", "title": "Training language models to follow instructions with human feedback", "cites": 2677}, {"url": "https://arxiv.org/abs/1512.02595", "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "cites": 2669}, {"url": "https://arxiv.org/abs/2006.11477", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 2655}, {"url": "https://arxiv.org/abs/1904.08779", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 2529}, {"url": "https://arxiv.org/abs/1708.02709", "title": "Recent Trends in Deep Learning Based Natural Language Processing", "cites": 2467}, {"url": "https://arxiv.org/abs/1509.00685", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "cites": 2457}, {"url": "https://arxiv.org/abs/1908.02265", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 2447}, {"url": "https://arxiv.org/abs/1808.06226", "title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "cites": 2415}, {"url": "https://arxiv.org/abs/1904.09675", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 2413}, {"url": "https://arxiv.org/abs/1904.01038", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 2409}, {"url": "https://arxiv.org/abs/1603.01354", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "cites": 2390}, {"url": "https://arxiv.org/abs/1802.03268", "title": "Efficient Neural Architecture Search via Parameter Sharing", "cites": 2312}, {"url": "https://arxiv.org/abs/1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge\n  Bases", "cites": 2307}, {"url": "https://arxiv.org/abs/1506.07503", "title": "Attention-Based Models for Speech Recognition", "cites": 2301}, {"url": "https://arxiv.org/abs/1607.06520", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "cites": 2272}, {"url": "https://arxiv.org/abs/1511.06709", "title": "Improving Neural Machine Translation Models with Monolingual Data", "cites": 2236}, {"url": "https://arxiv.org/abs/1506.06726", "title": "Skip-Thought Vectors", "cites": 2193}, {"url": "https://arxiv.org/abs/1901.07291", "title": "Cross-lingual Language Model Pretraining", "cites": 2148}, {"url": "https://arxiv.org/abs/2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 2065}, {"url": "https://arxiv.org/abs/1511.06349", "title": "Generating Sentences from a Continuous Space", "cites": 2064}, {"url": "https://arxiv.org/abs/1712.05884", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram\n  Predictions", "cites": 2038}, {"url": "https://arxiv.org/abs/2004.05150", "title": "Longformer: The Long-Document Transformer", "cites": 2037}, {"url": "https://arxiv.org/abs/1506.06724", "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by\n  Watching Movies and Reading Books", "cites": 2035}, {"url": "https://arxiv.org/abs/1806.03822", "title": "Know What You Don't Know: Unanswerable Questions for SQuAD", "cites": 2015}, {"url": "https://arxiv.org/abs/1602.06023", "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and\n  Beyond", "cites": 2010}, {"url": "https://arxiv.org/abs/1612.03975", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "cites": 1995}, {"url": "https://arxiv.org/abs/1611.01603", "title": "Bidirectional Attention Flow for Machine Comprehension", "cites": 1972}, {"url": "https://arxiv.org/abs/1508.01211", "title": "Listen, Attend and Spell", "cites": 1957}, {"url": "https://arxiv.org/abs/1703.04009", "title": "Automated Hate Speech Detection and the Problem of Offensive Language", "cites": 1944}, {"url": "https://arxiv.org/abs/1703.04247", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "cites": 1943}, {"url": "https://arxiv.org/abs/1308.6297", "title": "Crowdsourcing a Word-Emotion Association Lexicon", "cites": 1933}, {"url": "https://arxiv.org/abs/1608.07187", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "cites": 1881}, {"url": "https://arxiv.org/abs/1903.10676", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "cites": 1879}, {"url": "https://arxiv.org/abs/1804.08771", "title": "A Call for Clarity in Reporting BLEU Scores", "cites": 1877}, {"url": "https://arxiv.org/abs/1510.03055", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "cites": 1871}, {"url": "https://arxiv.org/abs/2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "cites": 1871}, {"url": "https://arxiv.org/abs/1703.03130", "title": "A Structured Self-attentive Sentence Embedding", "cites": 1869}, {"url": "https://arxiv.org/abs/1906.02243", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1851}, {"url": "https://arxiv.org/abs/1705.02364", "title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data", "cites": 1849}, {"url": "https://arxiv.org/abs/1412.5567", "title": "Deep Speech: Scaling up end-to-end speech recognition", "cites": 1841}, {"url": "https://arxiv.org/abs/1612.00837", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "cites": 1830}, {"url": "https://arxiv.org/abs/1904.09751", "title": "The Curious Case of Neural Text Degeneration", "cites": 1778}, {"url": "https://arxiv.org/abs/1611.04558", "title": "Google's Multilingual Neural Machine Translation System: Enabling\n  Zero-Shot Translation", "cites": 1775}, {"url": "https://arxiv.org/abs/1612.08083", "title": "Language Modeling with Gated Convolutional Networks", "cites": 1773}, {"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": 1749}, {"url": "https://arxiv.org/abs/1612.06890", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "cites": 1729}, {"url": "https://arxiv.org/abs/1504.00325", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "cites": 1724}, {"url": "https://arxiv.org/abs/1506.03099", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks", "cites": 1711}, {"url": "https://arxiv.org/abs/1511.02274", "title": "Stacked Attention Networks for Image Question Answering", "cites": 1709}, {"url": "https://arxiv.org/abs/1908.07490", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1707}, {"url": "https://arxiv.org/abs/2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 1706}, {"url": "https://arxiv.org/abs/1701.02810", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "cites": 1678}]