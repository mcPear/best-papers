[{"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 5129}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 1960}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1299}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 936}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 934}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don''t Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 814}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 789}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 787}, {"url": "http://arxiv.org/abs/1912.01973v1", "title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "cites": 700}, {"url": "http://arxiv.org/abs/1912.00741v1", "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "cites": 700}, {"url": "http://arxiv.org/abs/1912.06806v1", "title": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "cites": 683}, {"url": "http://arxiv.org/abs/2004.06165v5", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 620}, {"url": "http://arxiv.org/abs/2002.12327v3", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 615}, {"url": "http://arxiv.org/abs/2003.07082v2", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 613}, {"url": "http://arxiv.org/abs/1912.08777v3", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 603}, {"url": "http://arxiv.org/abs/2001.08210v2", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 581}, {"url": "http://arxiv.org/abs/2007.14062v2", "title": "Big Bird: Transformers for Longer Sequences", "cites": 559}, {"url": "http://arxiv.org/abs/2004.10706v4", "title": "CORD-19: The COVID-19 Open Research Dataset", "cites": 473}, {"url": "http://arxiv.org/abs/2006.03654v6", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 442}, {"url": "http://arxiv.org/abs/2009.14794v3", "title": "Rethinking Attention with Performers", "cites": 441}, {"url": "http://arxiv.org/abs/2010.11934v3", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 440}, {"url": "http://arxiv.org/abs/2002.00388v4", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": 438}, {"url": "http://arxiv.org/abs/2001.09977v3", "title": "Towards a Human-like Open-Domain Chatbot", "cites": 438}, {"url": "http://arxiv.org/abs/2003.11080v5", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization", "cites": 435}, {"url": "http://arxiv.org/abs/2003.08271v4", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": 434}, {"url": "http://arxiv.org/abs/2002.08909v1", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": 434}, {"url": "http://arxiv.org/abs/2005.04118v1", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": 427}, {"url": "http://arxiv.org/abs/2101.00190v1", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 424}, {"url": "http://arxiv.org/abs/2104.08821v4", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 420}, {"url": "http://arxiv.org/abs/2002.08155v4", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": 418}, {"url": "http://arxiv.org/abs/1911.03894v3", "title": "CamemBERT: a Tasty French Language Model", "cites": 403}, {"url": "http://arxiv.org/abs/1912.02387v1", "title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "cites": 392}, {"url": "http://arxiv.org/abs/2102.05918v2", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 383}, {"url": "http://arxiv.org/abs/2005.14050v2", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "cites": 380}, {"url": "http://arxiv.org/abs/2005.11401v4", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": 377}, {"url": "http://arxiv.org/abs/2012.15723v2", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": 359}, {"url": "http://arxiv.org/abs/2004.04696v5", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": 354}, {"url": "http://arxiv.org/abs/2004.12832v2", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "cites": 349}, {"url": "http://arxiv.org/abs/2007.00808v2", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "cites": 347}, {"url": "http://arxiv.org/abs/2104.08691v2", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 343}, {"url": "http://arxiv.org/abs/2001.07676v3", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": 340}, {"url": "http://arxiv.org/abs/2004.13637v2", "title": "Recipes for building an open-domain chatbot", "cites": 339}, {"url": "http://arxiv.org/abs/2009.06732v3", "title": "Efficient Transformers: A Survey", "cites": 336}, {"url": "http://arxiv.org/abs/2006.04558v8", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": 330}, {"url": "http://arxiv.org/abs/1912.02164v4", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "cites": 328}, {"url": "http://arxiv.org/abs/2004.02984v2", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "cites": 325}, {"url": "http://arxiv.org/abs/1912.02990v1", "title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "cites": 319}, {"url": "http://arxiv.org/abs/1912.06670v2", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "cites": 317}, {"url": "http://arxiv.org/abs/2002.08910v4", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "cites": 295}, {"url": "http://arxiv.org/abs/2003.00104v4", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding", "cites": 291}, {"url": "http://arxiv.org/abs/2012.07805v2", "title": "Extracting Training Data from Large Language Models", "cites": 289}, {"url": "http://arxiv.org/abs/1912.08226v2", "title": "Meshed-Memory Transformer for Image Captioning", "cites": 286}, {"url": "http://arxiv.org/abs/1911.12543v2", "title": "How Can We Know What Language Models Know?", "cites": 283}, {"url": "http://arxiv.org/abs/2009.07118v2", "title": "It''s Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "cites": 279}, {"url": "http://arxiv.org/abs/2006.07235v2", "title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "cites": 277}, {"url": "http://arxiv.org/abs/2007.15779v6", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": 267}, {"url": "http://arxiv.org/abs/2006.16668v1", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "cites": 260}, {"url": "http://arxiv.org/abs/2005.00700v3", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "cites": 257}, {"url": "http://arxiv.org/abs/2108.05927v1", "title": "Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive\n  Content Identification in Indo-European Languages", "cites": 249}, {"url": "http://arxiv.org/abs/1912.02315v2", "title": "12-in-1: Multi-Task Vision and Language Representation Learning", "cites": 248}, {"url": "http://arxiv.org/abs/2005.10200v2", "title": "BERTweet: A pre-trained language model for English Tweets", "cites": 242}, {"url": "http://arxiv.org/abs/2002.04745v2", "title": "On Layer Normalization in the Transformer Architecture", "cites": 241}, {"url": "http://arxiv.org/abs/1911.03429v2", "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", "cites": 240}, {"url": "http://arxiv.org/abs/2002.06305v1", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "cites": 237}, {"url": "http://arxiv.org/abs/2004.09813v2", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "cites": 236}, {"url": "http://arxiv.org/abs/2002.10957v2", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "cites": 235}, {"url": "http://arxiv.org/abs/2004.10643v1", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "cites": 231}, {"url": "http://arxiv.org/abs/2004.09095v3", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "cites": 231}, {"url": "http://arxiv.org/abs/2106.07447v1", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 230}, {"url": "http://arxiv.org/abs/1911.03584v2", "title": "On the Relationship between Self-Attention and Convolutional Layers", "cites": 222}, {"url": "http://arxiv.org/abs/2007.01282v2", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain\n  Question Answering", "cites": 220}, {"url": "http://arxiv.org/abs/2001.00459v1", "title": "Joint Robust Voicing Detection and Pitch Estimation Based on Residual\n  Harmonics", "cites": 220}, {"url": "http://arxiv.org/abs/2010.01057v1", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "cites": 219}, {"url": "http://arxiv.org/abs/1912.01734v2", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks", "cites": 217}, {"url": "http://arxiv.org/abs/2005.00661v1", "title": "On Faithfulness and Factuality in Abstractive Summarization", "cites": 215}, {"url": "http://arxiv.org/abs/2005.11882v2", "title": "Sentiment Analysis: Automatically Detecting Valence, Emotions, and Other\n  Affectual States from Text", "cites": 208}, {"url": "http://arxiv.org/abs/2005.00796v4", "title": "A Simple Language Model for Task-Oriented Dialogue", "cites": 207}, {"url": "http://arxiv.org/abs/2103.17249v1", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "cites": 206}, {"url": "http://arxiv.org/abs/2007.01852v2", "title": "Language-agnostic BERT Sentence Embedding", "cites": 205}, {"url": "http://arxiv.org/abs/2102.07662v1", "title": "Overview of the TREC 2020 deep learning track", "cites": 204}, {"url": "http://arxiv.org/abs/2003.07820v2", "title": "Overview of the TREC 2019 deep learning track", "cites": 204}, {"url": "http://arxiv.org/abs/2010.06467v3", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "cites": 197}, {"url": "http://arxiv.org/abs/1912.07840v2", "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "cites": 197}, {"url": "http://arxiv.org/abs/2001.04063v3", "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training", "cites": 196}, {"url": "http://arxiv.org/abs/2002.02562v2", "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "cites": 195}, {"url": "http://arxiv.org/abs/2004.03705v3", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "cites": 194}, {"url": "http://arxiv.org/abs/2011.04006v1", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "cites": 190}, {"url": "http://arxiv.org/abs/1912.00730v1", "title": "SemEval-2017 Task 3: Community Question Answering", "cites": 190}, {"url": "http://arxiv.org/abs/2004.09984v3", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "cites": 189}, {"url": "http://arxiv.org/abs/2002.01808v5", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "cites": 189}, {"url": "http://arxiv.org/abs/2003.05002v1", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in\n  Typologically Diverse Languages", "cites": 188}, {"url": "http://arxiv.org/abs/1912.05372v4", "title": "FlauBERT: Unsupervised Language Model Pre-training for French", "cites": 188}, {"url": "http://arxiv.org/abs/2004.02349v2", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "cites": 187}, {"url": "http://arxiv.org/abs/2002.06823v1", "title": "Incorporating BERT into Neural Machine Translation", "cites": 187}, {"url": "http://arxiv.org/abs/2004.03685v3", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "cites": 186}, {"url": "http://arxiv.org/abs/1912.01972v1", "title": "SemEval-2016 Task 3: Community Question Answering", "cites": 185}, {"url": "http://arxiv.org/abs/2002.12804v1", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model\n  Pre-Training", "cites": 183}, {"url": "http://arxiv.org/abs/1911.03814v3", "title": "Scalable Zero-shot Entity Linking with Dense Entity Retrieval", "cites": 183}, {"url": "http://arxiv.org/abs/2006.06195v2", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "cites": 181}, {"url": "http://arxiv.org/abs/1912.13318v5", "title": "LayoutLM: Pre-training of Text and Layout for Document Image\n  Understanding", "cites": 180}]