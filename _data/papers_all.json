[{"url": "http://arxiv.org/abs/1706.03762v5", "title": "Attention Is All You Need", "cites": 38269}, {"url": "http://arxiv.org/abs/1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 36575}, {"url": "http://arxiv.org/abs/1802.05365v2", "title": "Deep contextualized word representations", "cites": 8267}, {"url": "http://arxiv.org/abs/1907.11692v1", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 7963}, {"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 5129}, {"url": "http://arxiv.org/abs/1609.08144v2", "title": "Google''s Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation", "cites": 4805}, {"url": "http://arxiv.org/abs/1906.08237v2", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 4459}, {"url": "http://arxiv.org/abs/1910.10683v3", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 4156}, {"url": "http://arxiv.org/abs/1909.11942v6", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 2906}, {"url": "http://arxiv.org/abs/1910.03771v5", "title": "HuggingFace''s Transformers: State-of-the-art Natural Language Processing", "cites": 2899}, {"url": "http://arxiv.org/abs/1804.07461v3", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding", "cites": 2789}, {"url": "http://arxiv.org/abs/1910.13461v1", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 2680}, {"url": "http://arxiv.org/abs/1704.04368v2", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "cites": 2550}, {"url": "http://arxiv.org/abs/1705.03122v3", "title": "Convolutional Sequence to Sequence Learning", "cites": 2486}, {"url": "http://arxiv.org/abs/1806.09055v2", "title": "DARTS: Differentiable Architecture Search", "cites": 2319}, {"url": "http://arxiv.org/abs/1910.01108v4", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 2318}, {"url": "http://arxiv.org/abs/1704.05426v4", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through\n  Inference", "cites": 2133}, {"url": "http://arxiv.org/abs/1901.08746v4", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 2128}, {"url": "http://arxiv.org/abs/1705.02315v5", "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "cites": 1985}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 1968}, {"url": "http://arxiv.org/abs/1908.10084v1", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 1946}, {"url": "http://arxiv.org/abs/1708.02709v8", "title": "Recent Trends in Deep Learning Based Natural Language Processing", "cites": 1895}, {"url": "http://arxiv.org/abs/1803.01271v2", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "cites": 1893}, {"url": "http://arxiv.org/abs/1901.02860v3", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 1879}, {"url": "http://arxiv.org/abs/1802.03268v2", "title": "Efficient Neural Architecture Search via Parameter Sharing", "cites": 1774}, {"url": "http://arxiv.org/abs/1611.01603v6", "title": "Bidirectional Attention Flow for Machine Comprehension", "cites": 1748}, {"url": "http://arxiv.org/abs/1904.01038v1", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 1630}, {"url": "http://arxiv.org/abs/1904.08779v3", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 1611}, {"url": "http://arxiv.org/abs/1808.06226v1", "title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "cites": 1602}, {"url": "http://arxiv.org/abs/1901.07291v1", "title": "Cross-lingual Language Model Pretraining", "cites": 1583}, {"url": "http://arxiv.org/abs/1703.03130v1", "title": "A Structured Self-attentive Sentence Embedding", "cites": 1554}, {"url": "http://arxiv.org/abs/1705.02364v5", "title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data", "cites": 1552}, {"url": "http://arxiv.org/abs/1611.04558v2", "title": "Google''s Multilingual Neural Machine Translation System: Enabling\n  Zero-Shot Translation", "cites": 1473}, {"url": "http://arxiv.org/abs/1806.03822v1", "title": "Know What You Don''t Know: Unanswerable Questions for SQuAD", "cites": 1456}, {"url": "http://arxiv.org/abs/1709.03815v1", "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation", "cites": 1430}, {"url": "http://arxiv.org/abs/1701.02810v2", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "cites": 1430}, {"url": "http://arxiv.org/abs/1712.05884v2", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram\n  Predictions", "cites": 1417}, {"url": "http://arxiv.org/abs/1608.07187v4", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "cites": 1409}, {"url": "http://arxiv.org/abs/1908.02265v1", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 1406}, {"url": "http://arxiv.org/abs/1703.04009v1", "title": "Automated Hate Speech Detection and the Problem of Offensive Language", "cites": 1392}, {"url": "http://arxiv.org/abs/1612.08083v3", "title": "Language Modeling with Gated Convolutional Networks", "cites": 1336}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1299}, {"url": "http://arxiv.org/abs/1612.03975v2", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "cites": 1291}, {"url": "http://arxiv.org/abs/1703.04247v1", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "cites": 1280}, {"url": "http://arxiv.org/abs/1612.06890v1", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "cites": 1278}, {"url": "http://arxiv.org/abs/1704.00051v2", "title": "Reading Wikipedia to Answer Open-Domain Questions", "cites": 1245}, {"url": "http://arxiv.org/abs/1906.02243v1", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1233}, {"url": "http://arxiv.org/abs/1706.01427v1", "title": "A simple neural network module for relational reasoning", "cites": 1227}, {"url": "http://arxiv.org/abs/1612.00837v3", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "cites": 1220}, {"url": "http://arxiv.org/abs/1804.08771v2", "title": "A Call for Clarity in Reporting BLEU Scores", "cites": 1165}, {"url": "http://arxiv.org/abs/1710.04087v3", "title": "Word Translation Without Parallel Data", "cites": 1147}, {"url": "http://arxiv.org/abs/1904.09675v3", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 1134}, {"url": "http://arxiv.org/abs/1611.09268v3", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "cites": 1134}, {"url": "http://arxiv.org/abs/1705.04304v3", "title": "A Deep Reinforced Model for Abstractive Summarization", "cites": 1102}, {"url": "http://arxiv.org/abs/1707.07328v1", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "cites": 1100}, {"url": "http://arxiv.org/abs/1803.11175v2", "title": "Universal Sentence Encoder", "cites": 1090}, {"url": "http://arxiv.org/abs/1703.10135v2", "title": "Tacotron: Towards End-to-End Speech Synthesis", "cites": 1076}, {"url": "http://arxiv.org/abs/1609.07843v1", "title": "Pointer Sentinel Mixture Models", "cites": 1072}, {"url": "http://arxiv.org/abs/1908.07490v3", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1006}, {"url": "http://arxiv.org/abs/1904.09751v2", "title": "The Curious Case of Neural Text Degeneration", "cites": 983}, {"url": "http://arxiv.org/abs/1803.02155v2", "title": "Self-Attention with Relative Position Representations", "cites": 981}, {"url": "http://arxiv.org/abs/1708.00055v1", "title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and\n  Cross-lingual Focused Evaluation", "cites": 978}, {"url": "http://arxiv.org/abs/1907.10529v3", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 953}, {"url": "http://arxiv.org/abs/1904.12848v6", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 944}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 936}, {"url": "http://arxiv.org/abs/1802.06893v2", "title": "Learning Word Vectors for 157 Languages", "cites": 936}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 934}, {"url": "http://arxiv.org/abs/1908.03265v4", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 924}, {"url": "http://arxiv.org/abs/1705.03551v2", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for\n  Reading Comprehension", "cites": 924}, {"url": "http://arxiv.org/abs/1803.07640v2", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "cites": 923}, {"url": "http://arxiv.org/abs/1701.06538v1", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "cites": 907}, {"url": "http://arxiv.org/abs/1708.02182v1", "title": "Regularizing and Optimizing LSTM Language Models", "cites": 875}, {"url": "http://arxiv.org/abs/1905.00537v3", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 869}, {"url": "http://arxiv.org/abs/1709.07871v2", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "cites": 866}, {"url": "http://arxiv.org/abs/1712.09405v1", "title": "Advances in Pre-Training Distributed Word Representations", "cites": 860}, {"url": "http://arxiv.org/abs/1801.07883v2", "title": "Deep Learning for Sentiment Analysis : A Survey", "cites": 852}, {"url": "http://arxiv.org/abs/1706.03872v1", "title": "Six Challenges for Neural Machine Translation", "cites": 849}, {"url": "http://arxiv.org/abs/1712.01769v6", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models", "cites": 848}, {"url": "http://arxiv.org/abs/1809.05679v3", "title": "Graph Convolutional Networks for Text Classification", "cites": 832}, {"url": "http://arxiv.org/abs/1611.04230v1", "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for\n  Extractive Summarization of Documents", "cites": 829}, {"url": "http://arxiv.org/abs/1611.01734v3", "title": "Deep Biaffine Attention for Neural Dependency Parsing", "cites": 827}, {"url": "http://arxiv.org/abs/1804.09541v1", "title": "QANet: Combining Local Convolution with Global Self-Attention for\n  Reading Comprehension", "cites": 822}, {"url": "http://arxiv.org/abs/1609.06038v3", "title": "Enhanced LSTM for Natural Language Inference", "cites": 818}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don''t Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 814}, {"url": "http://arxiv.org/abs/1711.00043v2", "title": "Unsupervised Machine Translation Using Monolingual Corpora Only", "cites": 814}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 798}, {"url": "http://arxiv.org/abs/1705.00648v1", "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News\n  Detection", "cites": 796}, {"url": "http://arxiv.org/abs/1906.04341v1", "title": "What Does BERT Look At? An Analysis of BERT''s Attention", "cites": 789}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 787}, {"url": "http://arxiv.org/abs/1908.08530v4", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 783}, {"url": "http://arxiv.org/abs/1809.09600v1", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\n  Answering", "cites": 780}, {"url": "http://arxiv.org/abs/1905.03197v3", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 777}, {"url": "http://arxiv.org/abs/1801.07243v5", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "cites": 767}, {"url": "http://arxiv.org/abs/1901.11504v2", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 757}, {"url": "http://arxiv.org/abs/1708.07524v2", "title": "Supervised Speech Separation Based on Deep Learning: An Overview", "cites": 756}, {"url": "http://arxiv.org/abs/1902.10197v1", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "cites": 754}, {"url": "http://arxiv.org/abs/1909.01066v2", "title": "Language Models as Knowledge Bases?", "cites": 753}, {"url": "http://arxiv.org/abs/1701.06547v5", "title": "Adversarial Learning for Neural Dialogue Generation", "cites": 753}, {"url": "http://arxiv.org/abs/1703.00955v4", "title": "Toward Controlled Generation of Text", "cites": 747}, {"url": "http://arxiv.org/abs/1901.11196v2", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 734}]