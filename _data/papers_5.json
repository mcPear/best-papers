[{"url": "http://arxiv.org/abs/1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 36575}, {"url": "http://arxiv.org/abs/1802.05365v2", "title": "Deep contextualized word representations", "cites": 8267}, {"url": "http://arxiv.org/abs/1907.11692v1", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 7963}, {"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 5129}, {"url": "http://arxiv.org/abs/1906.08237v2", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 4459}, {"url": "http://arxiv.org/abs/1910.10683v3", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 4156}, {"url": "http://arxiv.org/abs/1909.11942v6", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 2906}, {"url": "http://arxiv.org/abs/1910.03771v5", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 2899}, {"url": "http://arxiv.org/abs/1804.07461v3", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding", "cites": 2789}, {"url": "http://arxiv.org/abs/1910.13461v1", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 2680}, {"url": "http://arxiv.org/abs/1806.09055v2", "title": "DARTS: Differentiable Architecture Search", "cites": 2319}, {"url": "http://arxiv.org/abs/1910.01108v4", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 2318}, {"url": "http://arxiv.org/abs/1901.08746v4", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 2128}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 1968}, {"url": "http://arxiv.org/abs/1908.10084v1", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 1946}, {"url": "http://arxiv.org/abs/1803.01271v2", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "cites": 1893}, {"url": "http://arxiv.org/abs/1901.02860v3", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 1879}, {"url": "http://arxiv.org/abs/1802.03268v2", "title": "Efficient Neural Architecture Search via Parameter Sharing", "cites": 1774}, {"url": "http://arxiv.org/abs/1904.01038v1", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 1630}, {"url": "http://arxiv.org/abs/1904.08779v3", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 1611}, {"url": "http://arxiv.org/abs/1808.06226v1", "title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "cites": 1602}, {"url": "http://arxiv.org/abs/1901.07291v1", "title": "Cross-lingual Language Model Pretraining", "cites": 1583}, {"url": "http://arxiv.org/abs/1806.03822v1", "title": "Know What You Don't Know: Unanswerable Questions for SQuAD", "cites": 1456}, {"url": "http://arxiv.org/abs/1908.02265v1", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 1406}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1299}, {"url": "http://arxiv.org/abs/1906.02243v1", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1233}, {"url": "http://arxiv.org/abs/1804.08771v2", "title": "A Call for Clarity in Reporting BLEU Scores", "cites": 1165}, {"url": "http://arxiv.org/abs/1904.09675v3", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 1134}, {"url": "http://arxiv.org/abs/1803.11175v2", "title": "Universal Sentence Encoder", "cites": 1090}, {"url": "http://arxiv.org/abs/1908.07490v3", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1006}, {"url": "http://arxiv.org/abs/1904.09751v2", "title": "The Curious Case of Neural Text Degeneration", "cites": 983}, {"url": "http://arxiv.org/abs/1803.02155v2", "title": "Self-Attention with Relative Position Representations", "cites": 981}, {"url": "http://arxiv.org/abs/1907.10529v3", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 953}, {"url": "http://arxiv.org/abs/1904.12848v6", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 944}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 936}, {"url": "http://arxiv.org/abs/1802.06893v2", "title": "Learning Word Vectors for 157 Languages", "cites": 936}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 934}, {"url": "http://arxiv.org/abs/1908.03265v4", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 924}, {"url": "http://arxiv.org/abs/1803.07640v2", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "cites": 923}, {"url": "http://arxiv.org/abs/1905.00537v3", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 869}, {"url": "http://arxiv.org/abs/1801.07883v2", "title": "Deep Learning for Sentiment Analysis : A Survey", "cites": 852}, {"url": "http://arxiv.org/abs/1809.05679v3", "title": "Graph Convolutional Networks for Text Classification", "cites": 832}, {"url": "http://arxiv.org/abs/1804.09541v1", "title": "QANet: Combining Local Convolution with Global Self-Attention for\n  Reading Comprehension", "cites": 822}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 814}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 798}, {"url": "http://arxiv.org/abs/1906.04341v1", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 789}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 787}, {"url": "http://arxiv.org/abs/1908.08530v4", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 783}, {"url": "http://arxiv.org/abs/1809.09600v1", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\n  Answering", "cites": 780}, {"url": "http://arxiv.org/abs/1905.03197v3", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 777}, {"url": "http://arxiv.org/abs/1801.07243v5", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "cites": 767}, {"url": "http://arxiv.org/abs/1901.11504v2", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 757}, {"url": "http://arxiv.org/abs/1902.10197v1", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "cites": 754}, {"url": "http://arxiv.org/abs/1909.01066v2", "title": "Language Models as Knowledge Bases?", "cites": 753}, {"url": "http://arxiv.org/abs/1901.11196v2", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 734}, {"url": "http://arxiv.org/abs/1904.03323v3", "title": "Publicly Available Clinical BERT Embeddings", "cites": 717}, {"url": "http://arxiv.org/abs/1905.05950v2", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 714}, {"url": "http://arxiv.org/abs/1804.00015v1", "title": "ESPnet: End-to-End Speech Processing Toolkit", "cites": 705}, {"url": "http://arxiv.org/abs/1908.03557v1", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 700}, {"url": "http://arxiv.org/abs/1803.02324v2", "title": "Annotation Artifacts in Natural Language Inference Data", "cites": 699}, {"url": "http://arxiv.org/abs/1808.09381v2", "title": "Understanding Back-Translation at Scale", "cites": 695}, {"url": "http://arxiv.org/abs/1908.08345v2", "title": "Text Summarization with Pretrained Encoders", "cites": 690}, {"url": "http://arxiv.org/abs/1909.10351v5", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 680}, {"url": "http://arxiv.org/abs/1804.03209v1", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "cites": 679}, {"url": "http://arxiv.org/abs/1906.01502v1", "title": "How multilingual is Multilingual BERT?", "cites": 678}, {"url": "http://arxiv.org/abs/1805.04833v1", "title": "Hierarchical Neural Story Generation", "cites": 671}, {"url": "http://arxiv.org/abs/1905.05583v3", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 664}, {"url": "http://arxiv.org/abs/1902.10186v3", "title": "Attention is not Explanation", "cites": 659}, {"url": "http://arxiv.org/abs/1812.04606v3", "title": "Deep Anomaly Detection with Outlier Exposure", "cites": 653}, {"url": "http://arxiv.org/abs/1811.03604v2", "title": "Federated Learning for Mobile Keyboard Prediction", "cites": 644}, {"url": "http://arxiv.org/abs/1808.07042v2", "title": "CoQA: A Conversational Question Answering Challenge", "cites": 641}, {"url": "http://arxiv.org/abs/1803.05355v3", "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "cites": 634}, {"url": "http://arxiv.org/abs/1905.07129v3", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 631}, {"url": "http://arxiv.org/abs/1905.02450v5", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 626}, {"url": "http://arxiv.org/abs/2004.06165v5", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 620}, {"url": "http://arxiv.org/abs/2002.12327v3", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 615}, {"url": "http://arxiv.org/abs/2003.07082v2", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 613}, {"url": "http://arxiv.org/abs/1902.01007v4", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 609}, {"url": "http://arxiv.org/abs/1902.00751v2", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 609}, {"url": "http://arxiv.org/abs/1912.08777v3", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 606}, {"url": "http://arxiv.org/abs/1911.00536v3", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 582}, {"url": "http://arxiv.org/abs/2001.08210v2", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 581}, {"url": "http://arxiv.org/abs/1805.12471v3", "title": "Neural Network Acceptability Judgments", "cites": 572}, {"url": "http://arxiv.org/abs/1901.04085v5", "title": "Passage Re-ranking with BERT", "cites": 568}, {"url": "http://arxiv.org/abs/1804.10959v1", "title": "Subword Regularization: Improving Neural Network Translation Models with\n  Multiple Subword Candidates", "cites": 565}, {"url": "http://arxiv.org/abs/1805.01070v2", "title": "What you can cram into a single vector: Probing sentence embeddings for\n  linguistic properties", "cites": 562}, {"url": "http://arxiv.org/abs/2007.14062v2", "title": "Big Bird: Transformers for Longer Sequences", "cites": 559}, {"url": "http://arxiv.org/abs/1904.08067v5", "title": "Text Classification Algorithms: A Survey", "cites": 555}, {"url": "http://arxiv.org/abs/1904.01201v2", "title": "Habitat: A Platform for Embodied AI Research", "cites": 552}, {"url": "http://arxiv.org/abs/1804.07755v2", "title": "Phrase-Based & Neural Unsupervised Machine Translation", "cites": 549}, {"url": "http://arxiv.org/abs/1812.10464v2", "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual\n  Transfer and Beyond", "cites": 543}, {"url": "http://arxiv.org/abs/1809.05053v1", "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "cites": 534}, {"url": "http://arxiv.org/abs/1909.08053v4", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 530}, {"url": "http://arxiv.org/abs/1909.05858v2", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 524}, {"url": "http://arxiv.org/abs/1810.00278v3", "title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\n  Task-Oriented Dialogue Modelling", "cites": 519}, {"url": "http://arxiv.org/abs/1905.06316v1", "title": "What do you learn from context? Probing for sentence structure in\n  contextualized word representations", "cites": 515}, {"url": "http://arxiv.org/abs/1804.07998v2", "title": "Generating Natural Language Adversarial Examples", "cites": 515}, {"url": "http://arxiv.org/abs/1808.08745v1", "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional\n  Neural Networks for Extreme Summarization", "cites": 514}, {"url": "http://arxiv.org/abs/1903.08983v3", "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in\n  Social Media (OffensEval)", "cites": 497}, {"url": "http://arxiv.org/abs/1904.05862v4", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 484}]