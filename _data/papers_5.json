[{"url": "http://arxiv.org/abs/1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 43750}, {"url": "http://arxiv.org/abs/1907.11692v1", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 9923}, {"url": "http://arxiv.org/abs/1802.05365v2", "title": "Deep contextualized word representations", "cites": 8915}, {"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 7025}, {"url": "http://arxiv.org/abs/1910.10683v3", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 5465}, {"url": "http://arxiv.org/abs/1906.08237v2", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 5123}, {"url": "http://arxiv.org/abs/1910.13461v1", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 3536}, {"url": "http://arxiv.org/abs/1909.11942v6", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 3451}, {"url": "http://arxiv.org/abs/1910.03771v5", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 3321}, {"url": "http://arxiv.org/abs/1804.07461v3", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding", "cites": 3262}, {"url": "http://arxiv.org/abs/1910.01108v4", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 2855}, {"url": "http://arxiv.org/abs/1806.09055v2", "title": "DARTS: Differentiable Architecture Search", "cites": 2607}, {"url": "http://arxiv.org/abs/1908.10084v1", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 2576}, {"url": "http://arxiv.org/abs/1901.08746v4", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 2497}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 2443}, {"url": "http://arxiv.org/abs/1803.01271v2", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "cites": 2247}, {"url": "http://arxiv.org/abs/1901.02860v3", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 2147}, {"url": "http://arxiv.org/abs/1802.03268v2", "title": "Efficient Neural Architecture Search via Parameter Sharing", "cites": 1978}, {"url": "http://arxiv.org/abs/1904.08779v3", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 1971}, {"url": "http://arxiv.org/abs/1904.01038v1", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 1905}, {"url": "http://arxiv.org/abs/1808.06226v1", "title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "cites": 1853}, {"url": "http://arxiv.org/abs/1901.07291v1", "title": "Cross-lingual Language Model Pretraining", "cites": 1770}, {"url": "http://arxiv.org/abs/1908.02265v1", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 1697}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1660}, {"url": "http://arxiv.org/abs/1806.03822v1", "title": "Know What You Don't Know: Unanswerable Questions for SQuAD", "cites": 1614}, {"url": "http://arxiv.org/abs/1904.09675v3", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 1464}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 1436}, {"url": "http://arxiv.org/abs/1906.02243v1", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1427}, {"url": "http://arxiv.org/abs/1804.08771v2", "title": "A Call for Clarity in Reporting BLEU Scores", "cites": 1390}, {"url": "http://arxiv.org/abs/1903.10676v3", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "cites": 1321}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 1223}, {"url": "http://arxiv.org/abs/1803.11175v2", "title": "Universal Sentence Encoder", "cites": 1216}, {"url": "http://arxiv.org/abs/1908.07490v3", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1214}, {"url": "http://arxiv.org/abs/1904.09751v2", "title": "The Curious Case of Neural Text Degeneration", "cites": 1206}, {"url": "http://arxiv.org/abs/1803.02155v2", "title": "Self-Attention with Relative Position Representations", "cites": 1165}, {"url": "http://arxiv.org/abs/1904.12848v6", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 1152}, {"url": "http://arxiv.org/abs/1907.10529v3", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 1121}, {"url": "http://arxiv.org/abs/1908.03265v4", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 1094}, {"url": "http://arxiv.org/abs/1905.00537v3", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 1016}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 1014}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 1012}, {"url": "http://arxiv.org/abs/1802.06893v2", "title": "Learning Word Vectors for 157 Languages", "cites": 1012}, {"url": "http://arxiv.org/abs/1809.05679v3", "title": "Graph Convolutional Networks for Text Classification", "cites": 999}, {"url": "http://arxiv.org/abs/1803.07640v2", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "cites": 990}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 979}, {"url": "http://arxiv.org/abs/1801.07883v2", "title": "Deep Learning for Sentiment Analysis : A Survey", "cites": 961}, {"url": "http://arxiv.org/abs/1902.10197v1", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "cites": 952}, {"url": "http://arxiv.org/abs/1909.01066v2", "title": "Language Models as Knowledge Bases?", "cites": 937}, {"url": "http://arxiv.org/abs/1908.08530v4", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 928}, {"url": "http://arxiv.org/abs/1809.09600v1", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\n  Answering", "cites": 910}, {"url": "http://arxiv.org/abs/1901.11196v2", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 907}, {"url": "http://arxiv.org/abs/1906.04341v1", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 904}, {"url": "http://arxiv.org/abs/1905.03197v3", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 896}, {"url": "http://arxiv.org/abs/1804.09541v1", "title": "QANet: Combining Local Convolution with Global Self-Attention for\n  Reading Comprehension", "cites": 883}, {"url": "http://arxiv.org/abs/1908.03557v1", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 873}, {"url": "http://arxiv.org/abs/1902.00751v2", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 860}, {"url": "http://arxiv.org/abs/1804.00015v1", "title": "ESPnet: End-to-End Speech Processing Toolkit", "cites": 852}, {"url": "http://arxiv.org/abs/1801.07243v5", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "cites": 843}, {"url": "http://arxiv.org/abs/1904.03323v3", "title": "Publicly Available Clinical BERT Embeddings", "cites": 838}, {"url": "http://arxiv.org/abs/1908.08345v2", "title": "Text Summarization with Pretrained Encoders", "cites": 830}, {"url": "http://arxiv.org/abs/1901.11504v2", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 824}, {"url": "http://arxiv.org/abs/1905.05950v2", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 823}, {"url": "http://arxiv.org/abs/1804.03209v1", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "cites": 819}, {"url": "http://arxiv.org/abs/2004.06165v5", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 818}, {"url": "http://arxiv.org/abs/1912.08777v3", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 817}, {"url": "http://arxiv.org/abs/1909.10351v5", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 813}, {"url": "http://arxiv.org/abs/1811.03604v2", "title": "Federated Learning for Mobile Keyboard Prediction", "cites": 805}, {"url": "http://arxiv.org/abs/1803.02324v2", "title": "Annotation Artifacts in Natural Language Inference Data", "cites": 789}, {"url": "http://arxiv.org/abs/1905.05583v3", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 778}, {"url": "http://arxiv.org/abs/1805.04833v1", "title": "Hierarchical Neural Story Generation", "cites": 777}, {"url": "http://arxiv.org/abs/1906.01502v1", "title": "How multilingual is Multilingual BERT?", "cites": 770}, {"url": "http://arxiv.org/abs/1808.09381v2", "title": "Understanding Back-Translation at Scale", "cites": 765}, {"url": "http://arxiv.org/abs/1812.04606v3", "title": "Deep Anomaly Detection with Outlier Exposure", "cites": 760}, {"url": "http://arxiv.org/abs/1902.10186v3", "title": "Attention is not Explanation", "cites": 751}, {"url": "http://arxiv.org/abs/1905.07129v3", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 746}, {"url": "http://arxiv.org/abs/2007.14062v2", "title": "Big Bird: Transformers for Longer Sequences", "cites": 744}, {"url": "http://arxiv.org/abs/2003.07082v2", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 744}, {"url": "http://arxiv.org/abs/1803.05355v3", "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "cites": 733}, {"url": "http://arxiv.org/abs/2001.08210v2", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 725}, {"url": "http://arxiv.org/abs/2002.12327v3", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 721}, {"url": "http://arxiv.org/abs/1902.01007v4", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 718}, {"url": "http://arxiv.org/abs/1911.00536v3", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 717}, {"url": "http://arxiv.org/abs/1808.07042v2", "title": "CoQA: A Conversational Question Answering Challenge", "cites": 710}, {"url": "http://arxiv.org/abs/1905.02450v5", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 698}, {"url": "http://arxiv.org/abs/2104.08821v4", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 697}, {"url": "http://arxiv.org/abs/2101.00190v1", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 696}, {"url": "http://arxiv.org/abs/1805.12471v3", "title": "Neural Network Acceptability Judgments", "cites": 669}, {"url": "http://arxiv.org/abs/1904.08067v5", "title": "Text Classification Algorithms: A Survey", "cites": 663}, {"url": "http://arxiv.org/abs/1904.01201v2", "title": "Habitat: A Platform for Embodied AI Research", "cites": 658}, {"url": "http://arxiv.org/abs/2102.05918v2", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 643}, {"url": "http://arxiv.org/abs/1804.10959v1", "title": "Subword Regularization: Improving Neural Network Translation Models with\n  Multiple Subword Candidates", "cites": 641}, {"url": "http://arxiv.org/abs/1808.08745v1", "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional\n  Neural Networks for Extreme Summarization", "cites": 639}, {"url": "http://arxiv.org/abs/2010.11934v3", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 638}, {"url": "http://arxiv.org/abs/1904.05862v4", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 633}, {"url": "http://arxiv.org/abs/2006.03654v6", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 630}, {"url": "http://arxiv.org/abs/1909.08053v4", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 625}, {"url": "http://arxiv.org/abs/1901.04085v5", "title": "Passage Re-ranking with BERT", "cites": 614}, {"url": "http://arxiv.org/abs/1805.01070v2", "title": "What you can cram into a single vector: Probing sentence embeddings for\n  linguistic properties", "cites": 611}, {"url": "http://arxiv.org/abs/1909.05858v2", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 610}, {"url": "http://arxiv.org/abs/1810.00278v3", "title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\n  Task-Oriented Dialogue Modelling", "cites": 610}]