[{"url": "https://arxiv.org/abs/2005.14165", "title": "Language Models are Few-Shot Learners", "cites": 15614}, {"url": "https://arxiv.org/abs/1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 14845}, {"url": "https://arxiv.org/abs/1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 9737}, {"url": "https://arxiv.org/abs/1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 6375}, {"url": "https://arxiv.org/abs/1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 6112}, {"url": "https://arxiv.org/abs/1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 5120}, {"url": "https://arxiv.org/abs/1909.11942", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 4571}, {"url": "https://arxiv.org/abs/1910.01108", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 4244}, {"url": "https://arxiv.org/abs/1910.03771", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 4210}, {"url": "https://arxiv.org/abs/1911.02116", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 3692}, {"url": "https://arxiv.org/abs/1901.08746", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 3433}, {"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": 2798}, {"url": "https://arxiv.org/abs/1901.02860", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 2721}, {"url": "https://arxiv.org/abs/2203.02155", "title": "Training language models to follow instructions with human feedback", "cites": 2677}, {"url": "https://arxiv.org/abs/2006.11477", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 2655}, {"url": "https://arxiv.org/abs/1904.08779", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 2529}, {"url": "https://arxiv.org/abs/1908.02265", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 2447}, {"url": "https://arxiv.org/abs/1904.09675", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 2413}, {"url": "https://arxiv.org/abs/1904.01038", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 2409}, {"url": "https://arxiv.org/abs/1901.07291", "title": "Cross-lingual Language Model Pretraining", "cites": 2148}, {"url": "https://arxiv.org/abs/2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 2065}, {"url": "https://arxiv.org/abs/2004.05150", "title": "Longformer: The Long-Document Transformer", "cites": 2037}, {"url": "https://arxiv.org/abs/1903.10676", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "cites": 1879}, {"url": "https://arxiv.org/abs/2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "cites": 1871}, {"url": "https://arxiv.org/abs/1906.02243", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1851}, {"url": "https://arxiv.org/abs/1904.09751", "title": "The Curious Case of Neural Text Degeneration", "cites": 1778}, {"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": 1749}, {"url": "https://arxiv.org/abs/1908.07490", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1707}, {"url": "https://arxiv.org/abs/2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 1706}, {"url": "https://arxiv.org/abs/1902.00751", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 1657}, {"url": "https://arxiv.org/abs/1904.12848", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 1604}, {"url": "https://arxiv.org/abs/2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 1598}, {"url": "https://arxiv.org/abs/2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 1536}, {"url": "https://arxiv.org/abs/2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 1515}, {"url": "https://arxiv.org/abs/1909.01066", "title": "Language Models as Knowledge Bases?", "cites": 1507}, {"url": "https://arxiv.org/abs/2004.10964", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 1491}, {"url": "https://arxiv.org/abs/2001.04451", "title": "Reformer: The Efficient Transformer", "cites": 1474}, {"url": "https://arxiv.org/abs/1907.10529", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 1448}, {"url": "https://arxiv.org/abs/2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 1448}, {"url": "https://arxiv.org/abs/1905.00537", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 1447}, {"url": "https://arxiv.org/abs/2107.13586", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "cites": 1446}, {"url": "https://arxiv.org/abs/1908.03265", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 1388}, {"url": "https://arxiv.org/abs/1902.10197", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "cites": 1349}, {"url": "https://arxiv.org/abs/2303.08774", "title": "GPT-4 Technical Report", "cites": 1308}, {"url": "https://arxiv.org/abs/1901.11196", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 1300}, {"url": "https://arxiv.org/abs/1912.08777", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 1290}, {"url": "https://arxiv.org/abs/2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 1276}, {"url": "https://arxiv.org/abs/1908.03557", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 1269}, {"url": "https://arxiv.org/abs/1908.08530", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 1252}, {"url": "https://arxiv.org/abs/1904.03323", "title": "Publicly Available Clinical BERT Embeddings", "cites": 1204}, {"url": "https://arxiv.org/abs/2006.03654", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 1204}, {"url": "https://arxiv.org/abs/2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 1194}, {"url": "https://arxiv.org/abs/2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "cites": 1191}, {"url": "https://arxiv.org/abs/2007.14062", "title": "Big Bird: Transformers for Longer Sequences", "cites": 1190}, {"url": "https://arxiv.org/abs/1909.10351", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 1184}, {"url": "https://arxiv.org/abs/1905.03197", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 1181}, {"url": "https://arxiv.org/abs/2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": 1169}, {"url": "https://arxiv.org/abs/2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": 1153}, {"url": "https://arxiv.org/abs/1906.04341", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 1109}, {"url": "https://arxiv.org/abs/2109.01652", "title": "Finetuned Language Models Are Zero-Shot Learners", "cites": 1105}, {"url": "https://arxiv.org/abs/1908.08345", "title": "Text Summarization with Pretrained Encoders", "cites": 1074}, {"url": "https://arxiv.org/abs/2001.08210", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 1070}, {"url": "https://arxiv.org/abs/1911.00536", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 1062}, {"url": "https://arxiv.org/abs/1905.05583", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 1061}, {"url": "https://arxiv.org/abs/2003.07082", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 1059}, {"url": "https://arxiv.org/abs/1905.05950", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 1042}, {"url": "https://arxiv.org/abs/2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": 1037}, {"url": "https://arxiv.org/abs/2106.07447", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 1033}, {"url": "https://arxiv.org/abs/2002.00388", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": 1005}, {"url": "https://arxiv.org/abs/1906.01502", "title": "How multilingual is Multilingual BERT?", "cites": 1000}, {"url": "https://arxiv.org/abs/1905.07129", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 997}, {"url": "https://arxiv.org/abs/1901.11504", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 996}, {"url": "https://arxiv.org/abs/2002.08909", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": 990}, {"url": "https://arxiv.org/abs/2002.12327", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 976}, {"url": "https://arxiv.org/abs/1902.10186", "title": "Attention is not Explanation", "cites": 942}, {"url": "https://arxiv.org/abs/2001.07676", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": 926}, {"url": "https://arxiv.org/abs/2003.08271", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": 926}, {"url": "https://arxiv.org/abs/1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 921}, {"url": "https://arxiv.org/abs/1902.01007", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 913}, {"url": "https://arxiv.org/abs/1904.08067", "title": "Text Classification Algorithms: A Survey", "cites": 904}, {"url": "https://arxiv.org/abs/2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": 897}, {"url": "https://arxiv.org/abs/1904.01201", "title": "Habitat: A Platform for Embodied AI Research", "cites": 893}, {"url": "https://arxiv.org/abs/1904.05862", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 891}, {"url": "https://arxiv.org/abs/2009.14794", "title": "Rethinking Attention with Performers", "cites": 888}, {"url": "https://arxiv.org/abs/2110.08207", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "cites": 865}, {"url": "https://arxiv.org/abs/1909.05858", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 839}, {"url": "https://arxiv.org/abs/1905.02450", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 819}, {"url": "https://arxiv.org/abs/1912.02990", "title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "cites": 815}, {"url": "https://arxiv.org/abs/2007.15779", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": 794}, {"url": "https://arxiv.org/abs/2006.04558", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": 783}, {"url": "https://arxiv.org/abs/2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "cites": 776}, {"url": "https://arxiv.org/abs/2201.08239", "title": "LaMDA: Language Models for Dialog Applications", "cites": 775}, {"url": "https://arxiv.org/abs/1912.06670", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "cites": 769}, {"url": "https://arxiv.org/abs/1901.04085", "title": "Passage Re-ranking with BERT", "cites": 765}, {"url": "https://arxiv.org/abs/2206.07682", "title": "Emergent Abilities of Large Language Models", "cites": 759}, {"url": "https://arxiv.org/abs/2004.04696", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": 758}, {"url": "https://arxiv.org/abs/2012.07805", "title": "Extracting Training Data from Large Language Models", "cites": 747}, {"url": "https://arxiv.org/abs/2005.04118", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": 744}, {"url": "https://arxiv.org/abs/1911.12543", "title": "How Can We Know What Language Models Know?", "cites": 736}, {"url": "https://arxiv.org/abs/2001.09977", "title": "Towards a Human-like Open-Domain Chatbot", "cites": 708}]