[{"url": "http://arxiv.org/abs/1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 43750}, {"url": "http://arxiv.org/abs/1907.11692v1", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 9923}, {"url": "http://arxiv.org/abs/1910.10683v3", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 5465}, {"url": "http://arxiv.org/abs/1906.08237v2", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 5123}, {"url": "http://arxiv.org/abs/1910.13461v1", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 3536}, {"url": "http://arxiv.org/abs/1909.11942v6", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 3451}, {"url": "http://arxiv.org/abs/1910.03771v5", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 3321}, {"url": "http://arxiv.org/abs/1910.01108v4", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 2855}, {"url": "http://arxiv.org/abs/1908.10084v1", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 2576}, {"url": "http://arxiv.org/abs/1901.08746v4", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 2497}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 2443}, {"url": "http://arxiv.org/abs/1901.02860v3", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 2147}, {"url": "http://arxiv.org/abs/1904.08779v3", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 1971}, {"url": "http://arxiv.org/abs/1904.01038v1", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 1905}, {"url": "http://arxiv.org/abs/1901.07291v1", "title": "Cross-lingual Language Model Pretraining", "cites": 1770}, {"url": "http://arxiv.org/abs/1908.02265v1", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 1697}, {"url": "http://arxiv.org/abs/1904.09675v3", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 1464}, {"url": "http://arxiv.org/abs/1906.02243v1", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1427}, {"url": "http://arxiv.org/abs/1903.10676v3", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "cites": 1321}, {"url": "http://arxiv.org/abs/1908.07490v3", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1214}, {"url": "http://arxiv.org/abs/1904.09751v2", "title": "The Curious Case of Neural Text Degeneration", "cites": 1206}, {"url": "http://arxiv.org/abs/1904.12848v6", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 1152}, {"url": "http://arxiv.org/abs/1907.10529v3", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 1121}, {"url": "http://arxiv.org/abs/1908.03265v4", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 1094}, {"url": "http://arxiv.org/abs/1905.00537v3", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 1016}, {"url": "http://arxiv.org/abs/1909.01066v2", "title": "Language Models as Knowledge Bases?", "cites": 937}, {"url": "http://arxiv.org/abs/1908.08530v4", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 928}, {"url": "http://arxiv.org/abs/1901.11196v2", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 907}, {"url": "http://arxiv.org/abs/1906.04341v1", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 904}, {"url": "http://arxiv.org/abs/1905.03197v3", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 896}, {"url": "http://arxiv.org/abs/1908.03557v1", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 873}, {"url": "http://arxiv.org/abs/1902.00751v2", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 860}, {"url": "http://arxiv.org/abs/1904.03323v3", "title": "Publicly Available Clinical BERT Embeddings", "cites": 838}, {"url": "http://arxiv.org/abs/1908.08345v2", "title": "Text Summarization with Pretrained Encoders", "cites": 830}, {"url": "http://arxiv.org/abs/1901.11504v2", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 824}, {"url": "http://arxiv.org/abs/1905.05950v2", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 823}, {"url": "http://arxiv.org/abs/1912.08777v3", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 817}, {"url": "http://arxiv.org/abs/1909.10351v5", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 813}, {"url": "http://arxiv.org/abs/1905.05583v3", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 778}, {"url": "http://arxiv.org/abs/1906.01502v1", "title": "How multilingual is Multilingual BERT?", "cites": 770}, {"url": "http://arxiv.org/abs/1902.10186v3", "title": "Attention is not Explanation", "cites": 751}, {"url": "http://arxiv.org/abs/1905.07129v3", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 746}, {"url": "http://arxiv.org/abs/1902.01007v4", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 718}, {"url": "http://arxiv.org/abs/1911.00536v3", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 717}, {"url": "http://arxiv.org/abs/1905.02450v5", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 698}, {"url": "http://arxiv.org/abs/1904.08067v5", "title": "Text Classification Algorithms: A Survey", "cites": 663}, {"url": "http://arxiv.org/abs/1904.01201v2", "title": "Habitat: A Platform for Embodied AI Research", "cites": 658}, {"url": "http://arxiv.org/abs/1904.05862v4", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 633}, {"url": "http://arxiv.org/abs/1909.08053v4", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 625}, {"url": "http://arxiv.org/abs/1901.04085v5", "title": "Passage Re-ranking with BERT", "cites": 614}, {"url": "http://arxiv.org/abs/1909.05858v2", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 610}, {"url": "http://arxiv.org/abs/1905.06316v1", "title": "What do you learn from context? Probing for sentence structure in\n  contextualized word representations", "cites": 571}, {"url": "http://arxiv.org/abs/1905.09418v2", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy\n  Lifting, the Rest Can Be Pruned", "cites": 543}, {"url": "http://arxiv.org/abs/1903.08983v3", "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in\n  Social Media (OffensEval)", "cites": 533}, {"url": "http://arxiv.org/abs/1908.04626v2", "title": "Attention is not not Explanation", "cites": 522}, {"url": "http://arxiv.org/abs/1906.00300v3", "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "cites": 520}, {"url": "http://arxiv.org/abs/1904.09223v1", "title": "ERNIE: Enhanced Representation through Knowledge Integration", "cites": 518}, {"url": "http://arxiv.org/abs/1905.09263v5", "title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "cites": 512}, {"url": "http://arxiv.org/abs/1906.05317v2", "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph\n  Construction", "cites": 500}, {"url": "http://arxiv.org/abs/1905.10650v3", "title": "Are Sixteen Heads Really Better than One?", "cites": 499}, {"url": "http://arxiv.org/abs/1811.00146v3", "title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning", "cites": 499}, {"url": "http://arxiv.org/abs/1904.00962v5", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "cites": 496}, {"url": "http://arxiv.org/abs/1903.08855v5", "title": "Linguistic Knowledge and Transferability of Contextual Representations", "cites": 495}, {"url": "http://arxiv.org/abs/1910.06711v3", "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform\n  Synthesis", "cites": 494}, {"url": "http://arxiv.org/abs/1811.00937v2", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "cites": 478}, {"url": "http://arxiv.org/abs/1905.12616v3", "title": "Defending Against Neural Fake News", "cites": 472}, {"url": "http://arxiv.org/abs/1911.03894v3", "title": "CamemBERT: a Tasty French Language Model", "cites": 470}, {"url": "http://arxiv.org/abs/1902.09666v2", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "cites": 469}, {"url": "http://arxiv.org/abs/1906.08101v3", "title": "Pre-Training with Whole Word Masking for Chinese BERT", "cites": 461}, {"url": "http://arxiv.org/abs/1901.10430v2", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "cites": 450}, {"url": "http://arxiv.org/abs/1906.03158v1", "title": "Matching the Blanks: Distributional Similarity for Relation Learning", "cites": 446}, {"url": "http://arxiv.org/abs/1908.09355v1", "title": "Patient Knowledge Distillation for BERT Model Compression", "cites": 445}, {"url": "http://arxiv.org/abs/1909.06317v2", "title": "A Comparative Study on Transformer vs RNN in Speech Applications", "cites": 443}, {"url": "http://arxiv.org/abs/1904.09077v2", "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "cites": 439}, {"url": "http://arxiv.org/abs/1906.05474v2", "title": "Transfer Learning in Biomedical Natural Language Processing: An\n  Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "cites": 432}, {"url": "http://arxiv.org/abs/1903.00161v2", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning\n  Over Paragraphs", "cites": 432}, {"url": "http://arxiv.org/abs/1907.12412v2", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "cites": 428}, {"url": "http://arxiv.org/abs/1912.06670v2", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "cites": 426}, {"url": "http://arxiv.org/abs/1910.14599v2", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "cites": 423}, {"url": "http://arxiv.org/abs/1909.04164v2", "title": "Knowledge Enhanced Contextual Word Representations", "cites": 423}, {"url": "http://arxiv.org/abs/1907.11932v6", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment", "cites": 419}, {"url": "http://arxiv.org/abs/1912.02164v4", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "cites": 406}, {"url": "http://arxiv.org/abs/1904.12584v1", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "cites": 404}, {"url": "http://arxiv.org/abs/1906.00295v1", "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences", "cites": 397}, {"url": "http://arxiv.org/abs/1910.11856v3", "title": "On the Cross-lingual Transferability of Monolingual Representations", "cites": 386}, {"url": "http://arxiv.org/abs/1908.07125v3", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "cites": 385}, {"url": "http://arxiv.org/abs/1912.08226v2", "title": "Meshed-Memory Transformer for Image Captioning", "cites": 381}, {"url": "http://arxiv.org/abs/1909.00512v1", "title": "How Contextual are Contextualized Word Representations? Comparing the\n  Geometry of BERT, ELMo, and GPT-2 Embeddings", "cites": 381}, {"url": "http://arxiv.org/abs/1903.03862v2", "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases\n  in Word Embeddings But do not Remove Them", "cites": 377}, {"url": "http://arxiv.org/abs/1911.12543v2", "title": "How Can We Know What Language Models Know?", "cites": 376}, {"url": "http://arxiv.org/abs/1907.10597v3", "title": "Green AI", "cites": 373}, {"url": "http://arxiv.org/abs/1904.02232v2", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based\n  Sentiment Analysis", "cites": 368}, {"url": "http://arxiv.org/abs/1909.07606v1", "title": "K-BERT: Enabling Language Representation with Knowledge Graph", "cites": 361}, {"url": "http://arxiv.org/abs/1901.05287v1", "title": "Assessing BERT's Syntactic Abilities", "cites": 358}, {"url": "http://arxiv.org/abs/1902.07669v3", "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language\n  Processing", "cites": 357}, {"url": "http://arxiv.org/abs/1910.05453v3", "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations", "cites": 356}, {"url": "http://arxiv.org/abs/1903.09588v1", "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing\n  Auxiliary Sentence", "cites": 346}, {"url": "http://arxiv.org/abs/1904.05342v3", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission", "cites": 344}, {"url": "http://arxiv.org/abs/1901.08149v2", "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based\n  Conversational Agents", "cites": 343}, {"url": "http://arxiv.org/abs/1909.11740v3", "title": "UNITER: UNiversal Image-TExt Representation Learning", "cites": 339}]