[{"url": "http://arxiv.org/abs/1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": 36575}, {"url": "http://arxiv.org/abs/1907.11692v1", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 7963}, {"url": "http://arxiv.org/abs/1906.08237v2", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 4459}, {"url": "http://arxiv.org/abs/1910.10683v3", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 4156}, {"url": "http://arxiv.org/abs/1909.11942v6", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 2906}, {"url": "http://arxiv.org/abs/1910.03771v5", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 2899}, {"url": "http://arxiv.org/abs/1910.13461v1", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 2680}, {"url": "http://arxiv.org/abs/1910.01108v4", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 2318}, {"url": "http://arxiv.org/abs/1901.08746v4", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 2128}, {"url": "http://arxiv.org/abs/1911.02116v2", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 1968}, {"url": "http://arxiv.org/abs/1908.10084v1", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 1946}, {"url": "http://arxiv.org/abs/1901.02860v3", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 1879}, {"url": "http://arxiv.org/abs/1904.01038v1", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 1630}, {"url": "http://arxiv.org/abs/1904.08779v3", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 1611}, {"url": "http://arxiv.org/abs/1901.07291v1", "title": "Cross-lingual Language Model Pretraining", "cites": 1583}, {"url": "http://arxiv.org/abs/1908.02265v1", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 1406}, {"url": "http://arxiv.org/abs/1906.02243v1", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1233}, {"url": "http://arxiv.org/abs/1904.09675v3", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 1134}, {"url": "http://arxiv.org/abs/1908.07490v3", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1006}, {"url": "http://arxiv.org/abs/1904.09751v2", "title": "The Curious Case of Neural Text Degeneration", "cites": 983}, {"url": "http://arxiv.org/abs/1907.10529v3", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 953}, {"url": "http://arxiv.org/abs/1904.12848v6", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 944}, {"url": "http://arxiv.org/abs/1908.03265v4", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 924}, {"url": "http://arxiv.org/abs/1905.00537v3", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 869}, {"url": "http://arxiv.org/abs/1906.04341v1", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 789}, {"url": "http://arxiv.org/abs/1908.08530v4", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 783}, {"url": "http://arxiv.org/abs/1905.03197v3", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 777}, {"url": "http://arxiv.org/abs/1901.11504v2", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 757}, {"url": "http://arxiv.org/abs/1909.01066v2", "title": "Language Models as Knowledge Bases?", "cites": 753}, {"url": "http://arxiv.org/abs/1901.11196v2", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 734}, {"url": "http://arxiv.org/abs/1904.03323v3", "title": "Publicly Available Clinical BERT Embeddings", "cites": 717}, {"url": "http://arxiv.org/abs/1905.05950v2", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 714}, {"url": "http://arxiv.org/abs/1908.03557v1", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 700}, {"url": "http://arxiv.org/abs/1908.08345v2", "title": "Text Summarization with Pretrained Encoders", "cites": 690}, {"url": "http://arxiv.org/abs/1909.10351v5", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 680}, {"url": "http://arxiv.org/abs/1906.01502v1", "title": "How multilingual is Multilingual BERT?", "cites": 678}, {"url": "http://arxiv.org/abs/1905.05583v3", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 664}, {"url": "http://arxiv.org/abs/1902.10186v3", "title": "Attention is not Explanation", "cites": 659}, {"url": "http://arxiv.org/abs/1905.07129v3", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 631}, {"url": "http://arxiv.org/abs/1905.02450v5", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 626}, {"url": "http://arxiv.org/abs/1902.01007v4", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 609}, {"url": "http://arxiv.org/abs/1902.00751v2", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 609}, {"url": "http://arxiv.org/abs/1912.08777v3", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 606}, {"url": "http://arxiv.org/abs/1911.00536v3", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 582}, {"url": "http://arxiv.org/abs/1901.04085v5", "title": "Passage Re-ranking with BERT", "cites": 568}, {"url": "http://arxiv.org/abs/1904.08067v5", "title": "Text Classification Algorithms: A Survey", "cites": 555}, {"url": "http://arxiv.org/abs/1904.01201v2", "title": "Habitat: A Platform for Embodied AI Research", "cites": 552}, {"url": "http://arxiv.org/abs/1909.08053v4", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 530}, {"url": "http://arxiv.org/abs/1909.05858v2", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 524}, {"url": "http://arxiv.org/abs/1905.06316v1", "title": "What do you learn from context? Probing for sentence structure in\n  contextualized word representations", "cites": 515}, {"url": "http://arxiv.org/abs/1903.08983v3", "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in\n  Social Media (OffensEval)", "cites": 497}, {"url": "http://arxiv.org/abs/1904.05862v4", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 484}, {"url": "http://arxiv.org/abs/1905.09418v2", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy\n  Lifting, the Rest Can Be Pruned", "cites": 460}, {"url": "http://arxiv.org/abs/1903.08855v5", "title": "Linguistic Knowledge and Transferability of Contextual Representations", "cites": 455}, {"url": "http://arxiv.org/abs/1908.04626v2", "title": "Attention is not not Explanation", "cites": 449}, {"url": "http://arxiv.org/abs/1906.00300v3", "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "cites": 448}, {"url": "http://arxiv.org/abs/1905.10650v3", "title": "Are Sixteen Heads Really Better than One?", "cites": 431}, {"url": "http://arxiv.org/abs/1905.12616v3", "title": "Defending Against Neural Fake News", "cites": 425}, {"url": "http://arxiv.org/abs/1902.09666v2", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "cites": 425}, {"url": "http://arxiv.org/abs/1904.09223v1", "title": "ERNIE: Enhanced Representation through Knowledge Integration", "cites": 420}, {"url": "http://arxiv.org/abs/1811.00146v3", "title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning", "cites": 419}, {"url": "http://arxiv.org/abs/1905.09263v5", "title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "cites": 414}, {"url": "http://arxiv.org/abs/1904.00962v5", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "cites": 413}, {"url": "http://arxiv.org/abs/1906.05317v2", "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph\n  Construction", "cites": 412}, {"url": "http://arxiv.org/abs/1901.10430v2", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "cites": 410}, {"url": "http://arxiv.org/abs/1811.00937v2", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "cites": 406}, {"url": "http://arxiv.org/abs/1911.03894v3", "title": "CamemBERT: a Tasty French Language Model", "cites": 404}, {"url": "http://arxiv.org/abs/1904.09077v2", "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "cites": 395}, {"url": "http://arxiv.org/abs/1910.06711v3", "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform\n  Synthesis", "cites": 393}, {"url": "http://arxiv.org/abs/1908.09355v1", "title": "Patient Knowledge Distillation for BERT Model Compression", "cites": 390}, {"url": "http://arxiv.org/abs/1909.06317v2", "title": "A Comparative Study on Transformer vs RNN in Speech Applications", "cites": 378}, {"url": "http://arxiv.org/abs/1906.05474v2", "title": "Transfer Learning in Biomedical Natural Language Processing: An\n  Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "cites": 371}, {"url": "http://arxiv.org/abs/1903.00161v2", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning\n  Over Paragraphs", "cites": 370}, {"url": "http://arxiv.org/abs/1909.04164v2", "title": "Knowledge Enhanced Contextual Word Representations", "cites": 368}, {"url": "http://arxiv.org/abs/1906.03158v1", "title": "Matching the Blanks: Distributional Similarity for Relation Learning", "cites": 365}, {"url": "http://arxiv.org/abs/1907.12412v2", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "cites": 361}, {"url": "http://arxiv.org/abs/1904.12584v1", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "cites": 358}, {"url": "http://arxiv.org/abs/1906.08101v3", "title": "Pre-Training with Whole Word Masking for Chinese BERT", "cites": 348}, {"url": "http://arxiv.org/abs/1910.14599v2", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "cites": 345}, {"url": "http://arxiv.org/abs/1903.03862v2", "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases\n  in Word Embeddings But do not Remove Them", "cites": 345}, {"url": "http://arxiv.org/abs/1910.11856v3", "title": "On the Cross-lingual Transferability of Monolingual Representations", "cites": 339}, {"url": "http://arxiv.org/abs/1907.11932v6", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment", "cites": 334}, {"url": "http://arxiv.org/abs/1901.05287v1", "title": "Assessing BERT's Syntactic Abilities", "cites": 329}, {"url": "http://arxiv.org/abs/1912.02164v4", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "cites": 328}, {"url": "http://arxiv.org/abs/1908.07125v3", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "cites": 321}, {"url": "http://arxiv.org/abs/1902.07669v3", "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language\n  Processing", "cites": 320}, {"url": "http://arxiv.org/abs/1912.06670v2", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "cites": 319}, {"url": "http://arxiv.org/abs/1904.02232v2", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based\n  Sentiment Analysis", "cites": 318}, {"url": "http://arxiv.org/abs/1901.08149v2", "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based\n  Conversational Agents", "cites": 316}, {"url": "http://arxiv.org/abs/1908.08593v2", "title": "Revealing the Dark Secrets of BERT", "cites": 304}, {"url": "http://arxiv.org/abs/1907.10597v3", "title": "Green AI", "cites": 300}, {"url": "http://arxiv.org/abs/1909.00512v1", "title": "How Contextual are Contextualized Word Representations? Comparing the\n  Geometry of BERT, ELMo, and GPT-2 Embeddings", "cites": 299}, {"url": "http://arxiv.org/abs/1902.01718v2", "title": "End-to-End Open-Domain Question Answering with BERTserini", "cites": 298}, {"url": "http://arxiv.org/abs/1909.11740v3", "title": "UNITER: UNiversal Image-TExt Representation Learning", "cites": 297}, {"url": "http://arxiv.org/abs/1903.09588v1", "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing\n  Auxiliary Sentence", "cites": 296}, {"url": "http://arxiv.org/abs/1909.07606v1", "title": "K-BERT: Enabling Language Representation with Knowledge Graph", "cites": 289}, {"url": "http://arxiv.org/abs/1912.08226v2", "title": "Meshed-Memory Transformer for Image Captioning", "cites": 288}, {"url": "http://arxiv.org/abs/1904.05342v3", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission", "cites": 288}, {"url": "http://arxiv.org/abs/1901.11117v4", "title": "The Evolved Transformer", "cites": 288}, {"url": "http://arxiv.org/abs/1911.12543v2", "title": "How Can We Know What Language Models Know?", "cites": 286}]