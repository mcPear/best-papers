[{"url": "https://arxiv.org/abs/1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "cites": 14845}, {"url": "https://arxiv.org/abs/1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "cites": 9737}, {"url": "https://arxiv.org/abs/1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "cites": 6375}, {"url": "https://arxiv.org/abs/1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "cites": 6112}, {"url": "https://arxiv.org/abs/1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "cites": 5120}, {"url": "https://arxiv.org/abs/1909.11942", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "cites": 4571}, {"url": "https://arxiv.org/abs/1910.01108", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\n  lighter", "cites": 4244}, {"url": "https://arxiv.org/abs/1910.03771", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "cites": 4210}, {"url": "https://arxiv.org/abs/1911.02116", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "cites": 3692}, {"url": "https://arxiv.org/abs/1901.08746", "title": "BioBERT: a pre-trained biomedical language representation model for\n  biomedical text mining", "cites": 3433}, {"url": "https://arxiv.org/abs/1901.02860", "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "cites": 2721}, {"url": "https://arxiv.org/abs/1904.08779", "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "cites": 2529}, {"url": "https://arxiv.org/abs/1908.02265", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "cites": 2447}, {"url": "https://arxiv.org/abs/1904.09675", "title": "BERTScore: Evaluating Text Generation with BERT", "cites": 2413}, {"url": "https://arxiv.org/abs/1904.01038", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "cites": 2409}, {"url": "https://arxiv.org/abs/1901.07291", "title": "Cross-lingual Language Model Pretraining", "cites": 2148}, {"url": "https://arxiv.org/abs/1903.10676", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "cites": 1879}, {"url": "https://arxiv.org/abs/1906.02243", "title": "Energy and Policy Considerations for Deep Learning in NLP", "cites": 1851}, {"url": "https://arxiv.org/abs/1904.09751", "title": "The Curious Case of Neural Text Degeneration", "cites": 1778}, {"url": "https://arxiv.org/abs/1908.07490", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "cites": 1707}, {"url": "https://arxiv.org/abs/1902.00751", "title": "Parameter-Efficient Transfer Learning for NLP", "cites": 1657}, {"url": "https://arxiv.org/abs/1904.12848", "title": "Unsupervised Data Augmentation for Consistency Training", "cites": 1604}, {"url": "https://arxiv.org/abs/1909.01066", "title": "Language Models as Knowledge Bases?", "cites": 1507}, {"url": "https://arxiv.org/abs/1907.10529", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "cites": 1448}, {"url": "https://arxiv.org/abs/1905.00537", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems", "cites": 1447}, {"url": "https://arxiv.org/abs/1908.03265", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "cites": 1388}, {"url": "https://arxiv.org/abs/1902.10197", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "cites": 1349}, {"url": "https://arxiv.org/abs/1901.11196", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks", "cites": 1300}, {"url": "https://arxiv.org/abs/1912.08777", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "cites": 1290}, {"url": "https://arxiv.org/abs/1908.03557", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "cites": 1269}, {"url": "https://arxiv.org/abs/1908.08530", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "cites": 1252}, {"url": "https://arxiv.org/abs/1904.03323", "title": "Publicly Available Clinical BERT Embeddings", "cites": 1204}, {"url": "https://arxiv.org/abs/1909.10351", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "cites": 1184}, {"url": "https://arxiv.org/abs/1905.03197", "title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "cites": 1181}, {"url": "https://arxiv.org/abs/1906.04341", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "cites": 1109}, {"url": "https://arxiv.org/abs/1908.08345", "title": "Text Summarization with Pretrained Encoders", "cites": 1074}, {"url": "https://arxiv.org/abs/1911.00536", "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational\n  Response Generation", "cites": 1062}, {"url": "https://arxiv.org/abs/1905.05583", "title": "How to Fine-Tune BERT for Text Classification?", "cites": 1061}, {"url": "https://arxiv.org/abs/1905.05950", "title": "BERT Rediscovers the Classical NLP Pipeline", "cites": 1042}, {"url": "https://arxiv.org/abs/1906.01502", "title": "How multilingual is Multilingual BERT?", "cites": 1000}, {"url": "https://arxiv.org/abs/1905.07129", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "cites": 997}, {"url": "https://arxiv.org/abs/1901.11504", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "cites": 996}, {"url": "https://arxiv.org/abs/1902.10186", "title": "Attention is not Explanation", "cites": 942}, {"url": "https://arxiv.org/abs/1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "cites": 921}, {"url": "https://arxiv.org/abs/1902.01007", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "cites": 913}, {"url": "https://arxiv.org/abs/1904.08067", "title": "Text Classification Algorithms: A Survey", "cites": 904}, {"url": "https://arxiv.org/abs/1904.01201", "title": "Habitat: A Platform for Embodied AI Research", "cites": 893}, {"url": "https://arxiv.org/abs/1904.05862", "title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "cites": 891}, {"url": "https://arxiv.org/abs/1909.05858", "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation", "cites": 839}, {"url": "https://arxiv.org/abs/1905.02450", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "cites": 819}, {"url": "https://arxiv.org/abs/1912.02990", "title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "cites": 815}, {"url": "https://arxiv.org/abs/1912.06670", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "cites": 769}, {"url": "https://arxiv.org/abs/1901.04085", "title": "Passage Re-ranking with BERT", "cites": 765}, {"url": "https://arxiv.org/abs/1911.12543", "title": "How Can We Know What Language Models Know?", "cites": 736}, {"url": "https://arxiv.org/abs/1904.00962", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "cites": 704}, {"url": "https://arxiv.org/abs/1905.09418", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy\n  Lifting, the Rest Can Be Pruned", "cites": 689}, {"url": "https://arxiv.org/abs/1906.00300", "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "cites": 683}, {"url": "https://arxiv.org/abs/1904.09223", "title": "ERNIE: Enhanced Representation through Knowledge Integration", "cites": 679}, {"url": "https://arxiv.org/abs/1905.10650", "title": "Are Sixteen Heads Really Better than One?", "cites": 673}, {"url": "https://arxiv.org/abs/1905.06316", "title": "What do you learn from context? Probing for sentence structure in\n  contextualized word representations", "cites": 659}, {"url": "https://arxiv.org/abs/1905.12616", "title": "Defending Against Neural Fake News", "cites": 654}, {"url": "https://arxiv.org/abs/1910.06711", "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform\n  Synthesis", "cites": 651}, {"url": "https://arxiv.org/abs/1906.05317", "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph\n  Construction", "cites": 649}, {"url": "https://arxiv.org/abs/1908.04626", "title": "Attention is not not Explanation", "cites": 641}, {"url": "https://arxiv.org/abs/1911.03894", "title": "CamemBERT: a Tasty French Language Model", "cites": 634}, {"url": "https://arxiv.org/abs/1906.00295", "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences", "cites": 633}, {"url": "https://arxiv.org/abs/1910.14599", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "cites": 629}, {"url": "https://arxiv.org/abs/1903.08983", "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in\n  Social Media (OffensEval)", "cites": 626}, {"url": "https://arxiv.org/abs/1907.11932", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment", "cites": 621}, {"url": "https://arxiv.org/abs/1907.10597", "title": "Green AI", "cites": 618}, {"url": "https://arxiv.org/abs/1912.01973", "title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "cites": 604}, {"url": "https://arxiv.org/abs/1912.02164", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "cites": 587}, {"url": "https://arxiv.org/abs/1908.09355", "title": "Patient Knowledge Distillation for BERT Model Compression", "cites": 586}, {"url": "https://arxiv.org/abs/1903.08855", "title": "Linguistic Knowledge and Transferability of Contextual Representations", "cites": 582}, {"url": "https://arxiv.org/abs/1907.12412", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "cites": 581}, {"url": "https://arxiv.org/abs/1912.08226", "title": "Meshed-Memory Transformer for Image Captioning", "cites": 576}, {"url": "https://arxiv.org/abs/1906.05474", "title": "Transfer Learning in Biomedical Natural Language Processing: An\n  Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "cites": 575}, {"url": "https://arxiv.org/abs/1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "cites": 569}, {"url": "https://arxiv.org/abs/1906.03158", "title": "Matching the Blanks: Distributional Similarity for Relation Learning", "cites": 566}, {"url": "https://arxiv.org/abs/1903.00161", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning\n  Over Paragraphs", "cites": 563}, {"url": "https://arxiv.org/abs/1909.06317", "title": "A Comparative Study on Transformer vs RNN in Speech Applications", "cites": 543}, {"url": "https://arxiv.org/abs/1904.09077", "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "cites": 532}, {"url": "https://arxiv.org/abs/1901.10430", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "cites": 526}, {"url": "https://arxiv.org/abs/1909.07606", "title": "K-BERT: Enabling Language Representation with Knowledge Graph", "cites": 525}, {"url": "https://arxiv.org/abs/1909.00512", "title": "How Contextual are Contextualized Word Representations? Comparing the\n  Geometry of BERT, ELMo, and GPT-2 Embeddings", "cites": 522}, {"url": "https://arxiv.org/abs/1904.12584", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "cites": 518}, {"url": "https://arxiv.org/abs/1909.04164", "title": "Knowledge Enhanced Contextual Word Representations", "cites": 518}, {"url": "https://arxiv.org/abs/1908.07125", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "cites": 515}, {"url": "https://arxiv.org/abs/1910.11856", "title": "On the Cross-lingual Transferability of Monolingual Representations", "cites": 515}, {"url": "https://arxiv.org/abs/1904.05342", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission", "cites": 514}, {"url": "https://arxiv.org/abs/1905.07830", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "cites": 511}, {"url": "https://arxiv.org/abs/1905.10044", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "cites": 502}, {"url": "https://arxiv.org/abs/1904.02232", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based\n  Sentiment Analysis", "cites": 489}, {"url": "https://arxiv.org/abs/1910.11470", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep\n  Learning models", "cites": 485}, {"url": "https://arxiv.org/abs/1912.00741", "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "cites": 484}, {"url": "https://arxiv.org/abs/1910.05453", "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations", "cites": 482}, {"url": "https://arxiv.org/abs/1906.03731", "title": "Is Attention Interpretable?", "cites": 480}, {"url": "https://arxiv.org/abs/1907.10641", "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale", "cites": 480}, {"url": "https://arxiv.org/abs/1902.07669", "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language\n  Processing", "cites": 467}, {"url": "https://arxiv.org/abs/1903.09588", "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing\n  Auxiliary Sentence", "cites": 462}]