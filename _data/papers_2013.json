[{"url": "https://arxiv.org/abs/1310.4546", "title": "Distributed Representations of Words and Phrases and their\n  Compositionality", "cites": "30 833", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.", "no": 1}, {"url": "https://arxiv.org/abs/1301.3781", "title": "Efficient Estimation of Word Representations in Vector Space", "cites": "27 844", "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.", "no": 2}, {"url": "https://arxiv.org/abs/1303.5778", "title": "Speech Recognition with Deep Recurrent Neural Networks", "cites": "7 974", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data.\nEnd-to-end training methods such as Connectionist Temporal Classification make\nit possible to train RNNs for sequence labelling problems where the\ninput-output alignment is unknown. The combination of these methods with the\nLong Short-term Memory RNN architecture has proved particularly fruitful,\ndelivering state-of-the-art results in cursive handwriting recognition. However\nRNN performance in speech recognition has so far been disappointing, with\nbetter results returned by deep feedforward networks. This paper investigates\n\\emph{deep recurrent neural networks}, which combine the multiple levels of\nrepresentation that have proved so effective in deep networks with the flexible\nuse of long range context that empowers RNNs. When trained end-to-end with\nsuitable regularisation, we find that deep Long Short-term Memory RNNs achieve\na test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to\nour knowledge is the best recorded score.", "no": 3}, {"url": "https://arxiv.org/abs/1308.0850", "title": "Generating Sequences With Recurrent Neural Networks", "cites": "3 704", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles.", "no": 4}, {"url": "https://arxiv.org/abs/1308.6297", "title": "Crowdsourcing a Word-Emotion Association Lexicon", "cites": "1 975", "abstract": "Even though considerable attention has been given to the polarity of words\n(positive and negative) and the creation of large polarity lexicons, research\nin emotion analysis has had to rely on limited and small emotion lexicons. In\nthis paper we show how the combined strength and wisdom of the crowds can be\nused to generate a large, high-quality, word-emotion and word-polarity\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\nemotion annotation in a crowdsourcing scenario and propose solutions to address\nthem. Most notably, in addition to questions about emotions associated with\nterms, we show how the inclusion of a word choice question can discourage\nmalicious data entry, help identify instances where the annotator may not be\nfamiliar with the target term (allowing us to reject such annotations), and\nhelp obtain annotations at sense level (rather than at word level). We\nconducted experiments on how to formulate the emotion-annotation questions, and\nshow that asking if a term is associated with an emotion leads to markedly\nhigher inter-annotator agreement than that obtained by asking if a term evokes\nan emotion.", "no": 5}, {"url": "https://arxiv.org/abs/1309.4168", "title": "Exploiting Similarities among Languages for Machine Translation", "cites": "1 516", "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine\ntranslation systems. This paper develops a method that can automate the process\nof generating and extending dictionaries and phrase tables. Our method can\ntranslate missing word and phrase entries by learning language structures based\non large monolingual data and mapping between languages from small bilingual\ndata. It uses distributed representation of words and learns a linear mapping\nbetween vector spaces of languages. Despite its simplicity, our method is\nsurprisingly effective: we can achieve almost 90% precision@5 for translation\nof words between English and Spanish. This method makes little assumption about\nthe languages, so it can be used to extend and refine dictionaries and\ntranslation tables for any language pairs.", "no": 6}, {"url": "https://arxiv.org/abs/1308.6242", "title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of\n  Tweets", "cites": "1 054", "abstract": "In this paper, we describe how we created two state-of-the-art SVM\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\n(message-level task) and one to detect the sentiment of a term within a\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\nin the message-level task and 88.93 in the term-level task. We implemented a\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\nhashtags, and one from tweets with emoticons. In the message-level task, the\nlexicon-based features provided a gain of 5 F-score points over all others.\nBoth of our systems can be replicated us available resources.", "no": 7}, {"url": "https://arxiv.org/abs/1312.3005", "title": "One Billion Word Benchmark for Measuring Progress in Statistical\n  Language Modeling", "cites": "1 007", "abstract": "We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models.", "no": 8}, {"url": "https://arxiv.org/abs/1308.5499", "title": "Linear models and linear mixed effects models in R with linguistic\n  applications", "cites": "549", "abstract": "This text is a conceptual introduction to mixed effects modeling with\nlinguistic applications, using the R programming environment. The reader is\nintroduced to linear modeling and assumptions, as well as to mixed\neffects/multilevel modeling, including a discussion of random intercepts,\nrandom slopes and likelihood ratio tests. The example used throughout the text\nfocuses on the phonetic analysis of voice pitch data.", "no": 9}, {"url": "https://arxiv.org/abs/1307.1662", "title": "Polyglot: Distributed Word Representations for Multilingual NLP", "cites": "459", "abstract": "Distributed word representations (word embeddings) have recently contributed\nto competitive performance in language modeling and several NLP tasks. In this\nwork, we train word embeddings for more than 100 languages using their\ncorresponding Wikipedias. We quantitatively demonstrate the utility of our word\nembeddings by using them as the sole features for training a part of speech\ntagger for a subset of these languages. We find their performance to be\ncompetitive with near state-of-art methods in English, Danish and Swedish.\nMoreover, we investigate the semantic features captured by these embeddings\nthrough the proximity of word groupings. We will release these embeddings\npublicly to help researchers in the development and enhancement of multilingual\napplications.", "no": 10}, {"url": "https://arxiv.org/abs/1306.6078", "title": "A Computational Approach to Politeness with Application to Social\n  Factors", "cites": "367", "abstract": "We propose a computational framework for identifying linguistic aspects of\npoliteness. Our starting point is a new corpus of requests annotated for\npoliteness, which we use to evaluate aspects of politeness theory and to\nuncover new interactions between politeness markers and context. These findings\nguide our construction of a classifier with domain-independent lexical and\nsyntactic features operationalizing key components of politeness theory, such\nas indirection, deference, impersonalization and modality. Our classifier\nachieves close to human performance and is effective across domains. We use our\nframework to study the relationship between politeness and social power,\nshowing that polite Wikipedia editors are more likely to achieve high status\nthrough elections, but, once elevated, they become less polite. We see a\nsimilar negative correlation between politeness and power on Stack Exchange,\nwhere users at the top of the reputation scale are less polite than those at\nthe bottom. Finally, we apply our classifier to a preliminary analysis of\npoliteness variation by gender and community.", "no": 11}, {"url": "https://arxiv.org/abs/1309.6202", "title": "Sentiment Analysis in the News", "cites": "305", "abstract": "Recent years have brought a significant growth in the volume of research in\nsentiment analysis, mostly on highly subjective text types (movie or product\nreviews). The main difference these texts have with news articles is that their\ntarget is clearly defined and unique across the text. Following different\nannotation efforts and the analysis of the issues encountered, we realised that\nnews opinion mining is different from that of other text types. We identified\nthree subtasks that need to be addressed: definition of the target; separation\nof the good and bad news content from the good and bad sentiment expressed on\nthe target; and analysis of clearly marked opinion that is expressed\nexplicitly, not needing interpretation or the use of world knowledge.\nFurthermore, we distinguish three different possible views on newspaper\narticles - author, reader and text, which have to be addressed differently at\nthe time of analysing sentiment. Given these definitions, we present work on\nmining opinions about entities in English language news, in which (a) we test\nthe relative suitability of various sentiment dictionaries and (b) we attempt\nto separate positive or negative opinion from good or bad news. In the\nexperiments described here, we tested whether or not subject domain-defining\nvocabulary should be ignored. Results showed that this idea is more appropriate\nin the context of news opinion mining and that the approaches taking this into\nconsideration produce a better performance.", "no": 12}, {"url": "https://arxiv.org/abs/1305.6143", "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model", "cites": "259", "abstract": "We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.", "no": 13}, {"url": "https://arxiv.org/abs/1306.3584", "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "cites": "251", "abstract": "The compositionality of meaning extends beyond the single sentence. Just as\nwords combine to form the meaning of sentences, so do sentences combine to form\nthe meaning of paragraphs, dialogues and general discourse. We introduce both a\nsentence model and a discourse model corresponding to the two levels of\ncompositionality. The sentence model adopts convolution as the central\noperation for composing semantic vectors and is based on a novel hierarchical\nconvolutional neural network. The discourse model extends the sentence model\nand is based on a recurrent neural network that is conditioned in a novel way\nboth on the current sentence and on the current speaker. The discourse model is\nable to capture both the sequentiality of sentences and the interaction between\ndifferent speakers. Without feature engineering or pretraining and with simple\ngreedy decoding, the discourse model coupled to the sentence model obtains\nstate of the art performance on a dialogue act classification experiment.", "no": 14}, {"url": "https://arxiv.org/abs/1301.3605", "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition\n  Tasks", "cites": "248", "abstract": "Recent studies have shown that deep neural networks (DNNs) perform\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\non large vocabulary speech recognition tasks. In this paper, we argue that the\nimproved accuracy achieved by the DNNs is the result of their ability to\nextract discriminative internal representations that are robust to the many\nsources of variability in speech signals. We show that these representations\nbecome increasingly insensitive to small perturbations in the input with\nincreasing network depth, which leads to better speech recognition performance\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\nthat are substantially different from the training examples. If the training\ndata are sufficiently representative, however, internal features learned by the\nDNN are relatively stable with respect to speaker differences, bandwidth\ndifferences, and environment distortion. This enables DNN-based recognizers to\nperform as well or better than state-of-the-art systems based on GMMs or\nshallow networks without the need for explicit model adaptation or feature\nnormalization.", "no": 15}, {"url": "https://arxiv.org/abs/1312.5542", "title": "Word Emdeddings through Hellinger PCA", "cites": "248", "abstract": "Word embeddings resulting from neural language models have been shown to be\nsuccessful for a large variety of NLP tasks. However, such architecture might\nbe difficult to train and time-consuming. Instead, we propose to drastically\nsimplify the word embeddings computation through a Hellinger PCA of the word\nco-occurence matrix. We compare those new word embeddings with some well-known\nembeddings on NER and movie review tasks and show that we can reach similar or\neven better performance. Although deep learning is not really necessary for\ngenerating good word embeddings, we show that it can provide an easy way to\nadapt embeddings to specific tasks.", "no": 16}, {"url": "https://arxiv.org/abs/1307.5336", "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts", "cites": "247", "abstract": "The use of robo-readers to analyze news texts is an emerging technology trend\nin computational finance. In recent research, a substantial effort has been\ninvested to develop sophisticated financial polarity-lexicons that can be used\nto investigate how financial sentiments relate to future company performance.\nHowever, based on experience from other fields, where sentiment analysis is\ncommonly applied, it is well-known that the overall semantic orientation of a\nsentence may differ from the prior polarity of individual words. The objective\nof this article is to investigate how semantic orientations can be better\ndetected in financial and economic news by accommodating the overall\nphrase-structure information and domain-specific use of language. Our three\nmain contributions are: (1) establishment of a human-annotated finance\nphrase-bank, which can be used as benchmark for training and evaluating\nalternative models; (2) presentation of a technique to enhance financial\nlexicons with attributes that help to identify expected direction of events\nthat affect overall sentiment; (3) development of a linearized phrase-structure\nmodel for detecting contextual semantic orientations in financial and economic\nnews texts. The relevance of the newly added lexicon features and the benefit\nof using the proposed learning-algorithm are demonstrated in a comparative\nstudy against previously used general sentiment models as well as the popular\nword frequency models used in recent financial studies. The proposed framework\nis parsimonious and avoids the explosion in feature-space caused by the use of\nconventional n-gram features.", "no": 17}, {"url": "https://arxiv.org/abs/1307.7973", "title": "Connecting Language and Knowledge Bases with Embedding Models for\n  Relation Extraction", "cites": "236", "abstract": "This paper proposes a novel approach for relation extraction from free text\nwhich is trained to jointly use information from the text and from existing\nknowledge. Our model is based on two scoring functions that operate by learning\nlow-dimensional embeddings of words and of entities and relationships from a\nknowledge base. We empirically show on New York Times articles aligned with\nFreebase relations that our approach is able to efficiently use the extra\ninformation provided by a large subset of Freebase data (4M entities, 23k\nrelationships) to improve over existing methods that rely on text features\nalone.", "no": 18}, {"url": "https://arxiv.org/abs/1309.1501", "title": "Improvements to deep convolutional neural networks for LVCSR", "cites": "218", "abstract": "Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.", "no": 19}, {"url": "https://arxiv.org/abs/1310.0201", "title": "Cross-Recurrence Quantification Analysis of Categorical and Continuous\n  Time Series: an R package", "cites": "207", "abstract": "This paper describes the R package crqa to perform cross-recurrence\nquantification analysis of two time series of either a categorical or\ncontinuous nature. Streams of behavioral information, from eye movements to\nlinguistic elements, unfold over time. When two people interact, such as in\nconversation, they often adapt to each other, leading these behavioral levels\nto exhibit recurrent states. In dialogue, for example, interlocutors adapt to\neach other by exchanging interactive cues: smiles, nods, gestures, choice of\nwords, and so on. In order for us to capture closely the goings-on of dynamic\ninteraction, and uncover the extent of coupling between two individuals, we\nneed to quantify how much recurrence is taking place at these levels. Methods\navailable in crqa would allow researchers in cognitive science to pose such\nquestions as how much are two people recurrent at some level of analysis, what\nis the characteristic lag time for one person to maximally match another, or\nwhether one person is leading another. First, we set the theoretical ground to\nunderstand the difference between 'correlation' and 'co-visitation' when\ncomparing two time series, using an aggregative or cross-recurrence approach.\nThen, we describe more formally the principles of cross-recurrence, and show\nwith the current package how to carry out analyses applying them. We end the\npaper by comparing computational efficiency, and results' consistency, of crqa\nR package, with the benchmark MATLAB toolbox crptoolbox. We show perfect\ncomparability between the two libraries on both levels.", "no": 20}, {"url": "https://arxiv.org/abs/1304.1018", "title": "Estimating Phoneme Class Conditional Probabilities from Raw Speech\n  Signal using Convolutional Neural Networks", "cites": "199", "abstract": "In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic\nspeech recognition (ASR) system, the phoneme class conditional probabilities\nare estimated by first extracting acoustic features from the speech signal\nbased on prior knowledge such as, speech perception or/and speech production\nknowledge, and, then modeling the acoustic features with an ANN. Recent\nadvances in machine learning techniques, more specifically in the field of\nimage processing and text processing, have shown that such divide and conquer\nstrategy (i.e., separating feature extraction and modeling steps) may not be\nnecessary. Motivated from these studies, in the framework of convolutional\nneural networks (CNNs), this paper investigates a novel approach, where the\ninput to the ANN is raw speech signal and the output is phoneme class\nconditional probability estimates. On TIMIT phoneme recognition task, we study\ndifferent ANN architectures to show the benefit of CNNs and compare the\nproposed approach against conventional approach where, spectral-based feature\nMFCC is extracted and modeled by a multilayer perceptron. Our studies show that\nthe proposed approach can yield comparable or better phoneme recognition\nperformance when compared to the conventional approach. It indicates that CNNs\ncan learn features relevant for phoneme classification automatically from the\nraw speech signal.", "no": 21}, {"url": "https://arxiv.org/abs/1309.6874", "title": "Integrating Document Clustering and Topic Modeling", "cites": "193", "abstract": "Document clustering and topic modeling are two closely related tasks which\ncan mutually benefit each other. Topic modeling can project documents into a\ntopic space which facilitates effective document clustering. Cluster labels\ndiscovered by document clustering can be incorporated into topic models to\nextract local topics specific to each cluster and global topics shared by all\nclusters. In this paper, we propose a multi-grain clustering topic model\n(MGCTM) which integrates document clustering and topic modeling into a unified\nframework and jointly performs the two tasks to achieve the overall best\nperformance. Our model tightly couples two components: a mixture component used\nfor discovering latent groups in document collection and a topic model\ncomponent used for mining multi-grain topics including local topics specific to\neach cluster and global topics shared across clusters.We employ variational\ninference to approximate the posterior of hidden variables and learn model\nparameters. Experiments on two datasets demonstrate the effectiveness of our\nmodel.", "no": 22}, {"url": "https://arxiv.org/abs/1309.4035", "title": "Domain and Function: A Dual-Space Model of Semantic Relations and\n  Compositions", "cites": "191", "abstract": "Given appropriate representations of the semantic relations between carpenter\nand wood and between mason and stone (for example, vectors in a vector space\nmodel), a suitable algorithm should be able to recognize that these relations\nare highly similar (carpenter is to wood as mason is to stone; the relations\nare analogous). Likewise, with representations of dog, house, and kennel, an\nalgorithm should be able to recognize that the semantic composition of dog and\nhouse, dog house, is highly similar to kennel (dog house and kennel are\nsynonymous). It seems that these two tasks, recognizing relations and\ncompositions, are closely connected. However, up to now, the best models for\nrelations are significantly different from the best models for compositions. In\nthis paper, we introduce a dual-space model that unifies these two tasks. This\nmodel matches the performance of the best previous models for relations and\ncompositions. The dual-space model consists of a space for measuring domain\nsimilarity and a space for measuring function similarity. Carpenter and wood\nshare the same domain, the domain of carpentry. Mason and stone share the same\ndomain, the domain of masonry. Carpenter and mason share the same function, the\nfunction of artisans. Wood and stone share the same function, the function of\nmaterials. In the composition dog house, kennel has some domain overlap with\nboth dog and house (the domains of pets and buildings). The function of kennel\nis similar to the function of house (the function of shelters). By combining\ndomain and function similarities in various ways, we can model relations,\ncompositions, and other aspects of semantics.", "no": 23}, {"url": "https://arxiv.org/abs/1309.5909", "title": "From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels\n  and Fairy Tales", "cites": "185", "abstract": "Today we have access to unprecedented amounts of literary texts. However,\nsearch still relies heavily on key words. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in both individual books and across very large collections. We\nintroduce the concept of emotion word density, and using the Brothers Grimm\nfairy tales as example, we show how collections of text can be organized for\nbetter search. Using the Google Books Corpus we show how to determine an\nentity's emotion associations from co-occurring words. Finally, we compare\nemotion words in fairy tales and novels, to show that fairy tales have a much\nwider range of emotion word densities than novels.", "no": 24}, {"url": "https://arxiv.org/abs/1309.6347", "title": "Tracking Sentiment in Mail: How Genders Differ on Emotional Axes", "cites": "175", "abstract": "With the widespread use of email, we now have access to unprecedented amounts\nof text that we ourselves have written. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in many types of mail. We create a large word--emotion\nassociation lexicon by crowdsourcing, and use it to compare emotions in love\nletters, hate mail, and suicide notes. We show that there are marked\ndifferences across genders in how they use emotion words in work-place email.\nFor example, women use many words from the joy--sadness axis, whereas men\nprefer terms from the fear--trust axis. Finally, we show visualizations that\ncan help people track emotions in their emails.", "no": 25}, {"url": "https://arxiv.org/abs/1312.6173", "title": "Multilingual Distributed Representations without Word Alignment", "cites": "153", "abstract": "Distributed representations of meaning are a natural way to encode covariance\nrelationships between words and phrases in NLP. By overcoming data sparsity\nproblems, as well as providing information about semantic relatedness which is\nnot available in discrete representations, distributed representations have\nproven useful in many NLP tasks. Recent work has shown how compositional\nsemantic representations can successfully be applied to a number of monolingual\napplications such as sentiment analysis. At the same time, there has been some\ninitial success in work on learning shared word-level representations across\nlanguages. We combine these two approaches by proposing a method for learning\ndistributed representations in a multilingual setup. Our model learns to assign\nsimilar embeddings to aligned sentences and dissimilar ones to sentence which\nare not aligned while not requiring word alignments. We show that our\nrepresentations are semantically informative and apply them to a cross-lingual\ndocument classification task where we outperform the previous state of the art.\nFurther, by employing parallel corpora of multiple language pairs we find that\nour model learns representations that capture semantic relationships across\nlanguages for which no parallel data was used.", "no": 26}, {"url": "https://arxiv.org/abs/1301.6939", "title": "Multi-Step Regression Learning for Compositional Distributional\n  Semantics", "cites": "142", "abstract": "We present a model for compositional distributional semantics related to the\nframework of Coecke et al. (2010), and emulating formal semantics by\nrepresenting functions as tensors and arguments as vectors. We introduce a new\nlearning method for tensors, generalising the approach of Baroni and Zamparelli\n(2010). We evaluate it on two benchmark data sets, and find it to outperform\nexisting leading methods. We argue in our analysis that the nature of this\nlearning method also renders it suitable for solving more subtle problems\ncompositional distributional models might face.", "no": 27}, {"url": "https://arxiv.org/abs/1309.5226", "title": "DGT-TM: A freely Available Translation Memory in 22 Languages", "cites": "142", "abstract": "The European Commission's (EC) Directorate General for Translation, together\nwith the EC's Joint Research Centre, is making available a large translation\nmemory (TM; i.e. sentences and their professionally produced translations)\ncovering twenty-two official European Union (EU) languages and their 231\nlanguage pairs. Such a resource is typically used by translation professionals\nin combination with TM software to improve speed and consistency of their\ntranslations. However, this resource has also many uses for translation studies\nand for language technology applications, including Statistical Machine\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\nmultilingual classification and clustering, and many more. In this reference\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\nits size, and explain how it was produced and how to use it.", "no": 28}, {"url": "https://arxiv.org/abs/1305.1145", "title": "Techniques for Feature Extraction In Speech Recognition System : A\n  Comparative Study", "cites": "126", "abstract": "The time domain waveform of a speech signal carries all of the auditory\ninformation. From the phonological point of view, it little can be said on the\nbasis of the waveform itself. However, past research in mathematics, acoustics,\nand speech technology have provided many methods for converting data that can\nbe considered as information if interpreted correctly. In order to find some\nstatistically relevant information from incoming data, it is important to have\nmechanisms for reducing the information of each segment in the audio signal\ninto a relatively small number of parameters, or features. These features\nshould describe each segment in such a characteristic way that other similar\nsegments can be grouped together by comparing their features. There are\nenormous interesting and exceptional ways to describe the speech signal in\nterms of parameters. Though, they all have their strengths and weaknesses, we\nhave presented some of the most used methods with their importance.", "no": 29}, {"url": "https://arxiv.org/abs/1312.4617", "title": "A Survey of Data Mining Techniques for Social Media Analysis", "cites": "118", "abstract": "Social network has gained remarkable attention in the last decade. Accessing\nsocial network sites such as Twitter, Facebook LinkedIn and Google+ through the\ninternet and the web 2.0 technologies has become more affordable. People are\nbecoming more interested in and relying on social network for information, news\nand opinion of other users on diverse subject matters. The heavy reliance on\nsocial network sites causes them to generate massive data characterised by\nthree computational issues namely; size, noise and dynamism. These issues often\nmake social network data very complex to analyse manually, resulting in the\npertinent use of computational means of analysing them. Data mining provides a\nwide range of techniques for detecting useful knowledge from massive datasets\nlike trends, patterns and rules [44]. Data mining techniques are used for\ninformation retrieval, statistical modelling and machine learning. These\ntechniques employ data pre-processing, data analysis, and data interpretation\nprocesses in the course of data analysis. This survey discusses different data\nmining techniques used in mining diverse aspects of the social network over\ndecades going from the historical techniques to the up-to-date models,\nincluding our novel technique named TRCM. All the techniques covered in this\nsurvey are listed in the Table.1 including the tools employed as well as names\nof their authors.", "no": 30}, {"url": "https://arxiv.org/abs/1310.8059", "title": "Description and Evaluation of Semantic Similarity Measures Approaches", "cites": "115", "abstract": "In recent years, semantic similarity measure has a great interest in Semantic\nWeb and Natural Language Processing (NLP). Several similarity measures have\nbeen developed, being given the existence of a structured knowledge\nrepresentation offered by ontologies and corpus which enable semantic\ninterpretation of terms. Semantic similarity measures compute the similarity\nbetween concepts/terms included in knowledge sources in order to perform\nestimations. This paper discusses the existing semantic similarity methods\nbased on structure, information content and feature approaches. Additionally,\nwe present a critical evaluation of several categories of semantic similarity\napproaches based on two standard benchmarks. The aim of this paper is to give\nan efficient evaluation of all these measures which help researcher and\npractitioners to select the measure that best fit for their requirements.", "no": 31}, {"url": "https://arxiv.org/abs/1302.4813", "title": "Probabilistic Frame Induction", "cites": "113", "abstract": "In natural-language discourse, related events tend to appear near each other\nto describe a larger scenario. Such structures can be formalized by the notion\nof a frame (a.k.a. template), which comprises a set of related events and\nprototypical participants and event transitions. Identifying frames is a\nprerequisite for information extraction and natural language generation, and is\nusually done manually. Methods for inducing frames have been proposed recently,\nbut they typically use ad hoc procedures and are difficult to diagnose or\nextend. In this paper, we propose the first probabilistic approach to frame\ninduction, which incorporates frames, events, participants as latent topics and\nlearns those frame and event transitions that best explain the text. The number\nof frames is inferred by a novel application of a split-merge method from\nsyntactic parsing. In end-to-end evaluations from text to induced frames and\nextracted facts, our method produced state-of-the-art results while\nsubstantially reducing engineering effort.", "no": 32}, {"url": "https://arxiv.org/abs/1308.6628", "title": "Joint Video and Text Parsing for Understanding Events and Answering\n  Queries", "cites": "113", "abstract": "We propose a framework for parsing video and text jointly for understanding\nevents and answering user queries. Our framework produces a parse graph that\nrepresents the compositional structures of spatial information (objects and\nscenes), temporal information (actions and events) and causal information\n(causalities between events and fluents) in the video and text. The knowledge\nrepresentation of our framework is based on a spatial-temporal-causal And-Or\ngraph (S/T/C-AOG), which jointly models possible hierarchical compositions of\nobjects, scenes and events as well as their interactions and mutual contexts,\nand specifies the prior probabilistic distribution of the parse graphs. We\npresent a probabilistic generative model for joint parsing that captures the\nrelations between the input video/text, their corresponding parse graphs and\nthe joint parse graph. Based on the probabilistic model, we propose a joint\nparsing system consisting of three modules: video parsing, text parsing and\njoint inference. Video parsing and text parsing produce two parse graphs from\nthe input video and text respectively. The joint inference module produces a\njoint parse graph by performing matching, deduction and revision on the video\nand text parse graphs. The proposed framework has the following objectives:\nFirstly, we aim at deep semantic parsing of video and text that goes beyond the\ntraditional bag-of-words approaches; Secondly, we perform parsing and reasoning\nacross the spatial, temporal and causal dimensions based on the joint S/T/C-AOG\nrepresentation; Thirdly, we show that deep joint parsing facilitates subsequent\napplications such as generating narrative text descriptions and answering\nqueries in the forms of who, what, when, where and why. We empirically\nevaluated our system based on comparison against ground-truth as well as\naccuracy of query answering and obtained satisfactory results.", "no": 33}, {"url": "https://arxiv.org/abs/1308.6300", "title": "Computing Lexical Contrast", "cites": "105", "abstract": "Knowing the degree of semantic contrast between words has widespread\napplication in natural language processing, including machine translation,\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\nopposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such\nas antipodals, complementaries, and gradable. However, existing lexicons often\ndo not classify opposites into the different kinds. They also do not explicitly\nlist word pairs that are not opposites but yet have some degree of contrast in\nmeaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm\nfreezing}. We propose an automatic method to identify contrasting word pairs\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\nthere exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm\ntropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm\ncold}.) We will call this the contrast hypothesis. We begin with a large\ncrowdsourcing experiment to determine the amount of human agreement on the\nconcept of oppositeness and its different kinds. In the process, we flesh out\nkey features of different kinds of opposites. We then present an automatic and\nempirical measure of lexical contrast that relies on the contrast hypothesis,\ncorpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show\nthat the proposed measure of lexical contrast obtains high precision and large\ncoverage, outperforming existing methods.", "no": 34}, {"url": "https://arxiv.org/abs/1309.5843", "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", "cites": "102", "abstract": "Assigning a positive or negative score to a word out of context (i.e. a\nword's prior polarity) is a challenging task for sentiment analysis. In the\nliterature, various approaches based on SentiWordNet have been proposed. In\nthis paper, we compare the most often used techniques together with newly\nproposed ones and incorporate all of them in a learning framework to see\nwhether blending them can further improve the estimation of prior polarity\nscores. Using two different versions of SentiWordNet and testing regression and\nclassification models across tasks and datasets, our learning approach\nconsistently outperforms the single metrics, providing a new state-of-the-art\napproach in computing words' prior polarity for sentiment analysis. We conclude\nour investigation showing interesting biases in calculated prior polarity\nscores when word Part of Speech and annotator gender are considered.", "no": 35}, {"url": "https://arxiv.org/abs/1305.5566", "title": "The most controversial topics in Wikipedia: A multilingual and\n  geographical analysis", "cites": "100", "abstract": "We present, visualize and analyse the similarities and differences between\nthe controversial topics related to \"edit wars\" identified in 10 different\nlanguage versions of Wikipedia. After a brief review of the related work we\ndescribe the methods developed to locate, measure, and categorize the\ncontroversial topics in the different languages. Visualizations of the degree\nof overlap between the top 100 lists of most controversial articles in\ndifferent languages and the content related to geographical locations will be\npresented. We discuss what the presented analysis and visualizations can tell\nus about the multicultural aspects of Wikipedia and practices of\npeer-production. Our results indicate that Wikipedia is more than just an\nencyclopaedia; it is also a window into convergent and divergent social-spatial\npriorities, interests and preferences.", "no": 36}, {"url": "https://arxiv.org/abs/1307.3336", "title": "Opinion Mining and Analysis: A survey", "cites": "99", "abstract": "The current research is focusing on the area of Opinion Mining also called as\nsentiment analysis due to sheer volume of opinion rich web resources such as\ndiscussion forums, review sites and blogs are available in digital form. One\nimportant problem in sentiment analysis of product reviews is to produce\nsummary of opinions based on product features. We have surveyed and analyzed in\nthis paper, various techniques that have been developed for the key tasks of\nopinion mining. We have provided an overall picture of what is involved in\ndeveloping a software system for opinion mining on the basis of our survey and\nanalysis.", "no": 37}, {"url": "https://arxiv.org/abs/1306.6755", "title": "Arabizi Detection and Conversion to Arabic", "cites": "98", "abstract": "Arabizi is Arabic text that is written using Latin characters. Arabizi is\nused to present both Modern Standard Arabic (MSA) or Arabic dialects. It is\ncommonly used in informal settings such as social networking sites and is often\nwith mixed with English. In this paper we address the problems of: identifying\nArabizi in text and converting it to Arabic characters. We used word and\nsequence-level features to identify Arabizi that is mixed with English. We\nachieved an identification accuracy of 98.5%. As for conversion, we used\ntransliteration mining with language modeling to generate equivalent Arabic\ntext. We achieved 88.7% conversion accuracy, with roughly a third of errors\nbeing spelling and morphological variants of the forms in ground truth.", "no": 38}, {"url": "https://arxiv.org/abs/1304.5823", "title": "Towards a Formal Distributional Semantics: Simulating Logical Calculi\n  with Tensors", "cites": "97", "abstract": "The development of compositional distributional models of semantics\nreconciling the empirical aspects of distributional semantics with the\ncompositional aspects of formal semantics is a popular topic in the\ncontemporary literature. This paper seeks to bring this reconciliation one step\nfurther by showing how the mathematical constructs commonly used in\ncompositional distributional models, such as tensors and matrices, can be used\nto simulate different aspects of predicate logic.\n  This paper discusses how the canonical isomorphism between tensors and\nmultilinear maps can be exploited to simulate a full-blown quantifier-free\npredicate calculus using tensors. It provides tensor interpretations of the set\nof logical connectives required to model propositional calculi. It suggests a\nvariant of these tensor calculi capable of modelling quantifiers, using few\nnon-linear operations. It finally discusses the relation between these\nvariants, and how this relation should constitute the subject of future work.", "no": 39}, {"url": "https://arxiv.org/abs/1309.5290", "title": "An introduction to the Europe Media Monitor family of applications", "cites": "97", "abstract": "Most large organizations have dedicated departments that monitor the media to\nkeep up-to-date with relevant developments and to keep an eye on how they are\nrepresented in the news. Part of this media monitoring work can be automated.\nIn the European Union with its 23 official languages, it is particularly\nimportant to cover media reports in many languages in order to capture the\ncomplementary news content published in the different countries. It is also\nimportant to be able to access the news content across languages and to merge\nthe extracted information. We present here the four publicly accessible systems\nof the Europe Media Monitor (EMM) family of applications, which cover between\n19 and 50 languages (see http://press.jrc.it/overview.html). We give an\noverview of their functionality and discuss some of the implications of the\nfact that they cover quite so many languages. We discuss design issues\nnecessary to be able to achieve this high multilinguality, as well as the\nbenefits of this multilinguality.", "no": 40}, {"url": "https://arxiv.org/abs/1307.0261", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised\n  Information Extraction", "cites": "96", "abstract": "We describe a open-domain information extraction method for extracting\nconcept-instance pairs from an HTML corpus. Most earlier approaches to this\nproblem rely on combining clusters of distributionally similar terms and\nconcept-instance pairs obtained with Hearst patterns. In contrast, our method\nrelies on a novel approach for clustering terms found in HTML tables, and then\nassigning concept names to these clusters using Hearst patterns. The method can\nbe efficiently applied to a large corpus, and experimental results on several\ndatasets show that our method can accurately extract large numbers of\nconcept-instance pairs.", "no": 41}, {"url": "https://arxiv.org/abs/1309.3949", "title": "Performance Investigation of Feature Selection Methods", "cites": "94", "abstract": "Sentiment analysis or opinion mining has become an open research domain after\nproliferation of Internet and Web 2.0 social media. People express their\nattitudes and opinions on social media including blogs, discussion forums,\ntweets, etc. and, sentiment analysis concerns about detecting and extracting\nsentiment or opinion from online text. Sentiment based text classification is\ndifferent from topical text classification since it involves discrimination\nbased on expressed opinion on a topic. Feature selection is significant for\nsentiment analysis as the opinionated text may have high dimensions, which can\nadversely affect the performance of sentiment analysis classifier. This paper\nexplores applicability of feature selection methods for sentiment analysis and\ninvestigates their performance for classification in term of recall, precision\nand accuracy. Five feature selection methods (Document Frequency, Information\nGain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment\nfeature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews\ncorpus with a size of 2000 documents. The experimental results show that\nInformation Gain gave consistent results and Gain Ratio performs overall best\nfor sentimental feature selection while sentiment lexicons gave poor\nperformance. Furthermore, we found that performance of the classifier depends\non appropriate number of representative feature selected from text.", "no": 42}, {"url": "https://arxiv.org/abs/1301.3226", "title": "The Expressive Power of Word Embeddings", "cites": "93", "abstract": "We seek to better understand the difference in quality of the several\npublicly released embeddings. We propose several tasks that help to distinguish\nthe characteristics of different embeddings. Our evaluation of sentiment\npolarity and synonym/antonym relations shows that embeddings are able to\ncapture surprisingly nuanced semantics even in the absence of sentence\nstructure. Moreover, benchmarking the embeddings shows great variance in\nquality and characteristics of the semantics captured by the tested embeddings.\nFinally, we show the impact of varying the number of dimensions and the\nresolution of each dimension on the effective useful features captured by the\nembedding space. Our contributions highlight the importance of embeddings for\nNLP tasks and the effect of their quality on the final results.", "no": 43}, {"url": "https://arxiv.org/abs/1302.0393", "title": "Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams\n  for Lambek Calculus", "cites": "92", "abstract": "The Distributional Compositional Categorical (DisCoCat) model is a\nmathematical framework that provides compositional semantics for meanings of\nnatural language sentences. It consists of a computational procedure for\nconstructing meanings of sentences, given their grammatical structure in terms\nof compositional type-logic, and given the empirically derived meanings of\ntheir words. For the particular case that the meaning of words is modelled\nwithin a distributional vector space model, its experimental predictions,\nderived from real large scale data, have outperformed other empirically\nvalidated methods that could build vectors for a full sentence. This success\ncan be attributed to a conceptually motivated mathematical underpinning, by\nintegrating qualitative compositional type-logic and quantitative modelling of\nmeaning within a category-theoretic mathematical framework.\n  The type-logic used in the DisCoCat model is Lambek's pregroup grammar.\nPregroup types form a posetal compact closed category, which can be passed, in\na functorial manner, on to the compact closed structure of vector spaces,\nlinear maps and tensor product. The diagrammatic versions of the equational\nreasoning in compact closed categories can be interpreted as the flow of word\nmeanings within sentences. Pregroups simplify Lambek's previous type-logic, the\nLambek calculus, which has been extensively used to formalise and reason about\nvarious linguistic phenomena. The apparent reliance of the DisCoCat on\npregroups has been seen as a shortcoming. This paper addresses this concern, by\npointing out that one may as well realise a functorial passage from the\noriginal type-logic of Lambek, a monoidal bi-closed category, to vector spaces,\nor to any other model of meaning organised within a monoidal bi-closed\ncategory. The corresponding string diagram calculus, due to Baez and Stay, now\ndepicts the flow of word meanings.", "no": 44}, {"url": "https://arxiv.org/abs/1310.5042", "title": "Distributional semantics beyond words: Supervised learning of analogy\n  and paraphrase", "cites": "89", "abstract": "There have been several efforts to extend distributional semantics beyond\nindividual words, to measure the similarity of word pairs, phrases, and\nsentences (briefly, tuples; ordered sets of words, contiguous or\nnoncontiguous). One way to extend beyond words is to compare two tuples using a\nfunction that combines pairwise similarities between the component words in the\ntuples. A strength of this approach is that it works with both relational\nsimilarity (analogy) and compositional similarity (paraphrase). However, past\nwork required hand-coding the combination function for different tasks. The\nmain contribution of this paper is that combination functions are generated by\nsupervised learning. We achieve state-of-the-art results in measuring\nrelational similarity between word pairs (SAT analogies and SemEval~2012 Task\n2) and measuring compositional similarity between noun-modifier phrases and\nunigrams (multiple-choice paraphrase questions).", "no": 45}, {"url": "https://arxiv.org/abs/1302.3831", "title": "Quantum Entanglement in Concept Combinations", "cites": "86", "abstract": "Research in the application of quantum structures to cognitive science\nconfirms that these structures quite systematically appear in the dynamics of\nconcepts and their combinations and quantum-based models faithfully represent\nexperimental data of situations where classical approaches are problematical.\nIn this paper, we analyze the data we collected in an experiment on a specific\nconceptual combination, showing that Bell's inequalities are violated in the\nexperiment. We present a new refined entanglement scheme to model these data\nwithin standard quantum theory rules, where 'entangled measurements and\nentangled evolutions' occur, in addition to the expected 'entangled states',\nand present a full quantum representation in complex Hilbert space of the data.\nThis stronger form of entanglement in measurements and evolutions might have\nrelevant applications in the foundations of quantum theory, as well as in the\ninterpretation of nonlocality tests. It could indeed explain some\nnon-negligible 'anomalies' identified in EPR-Bell experiments.", "no": 46}, {"url": "https://arxiv.org/abs/1310.1285", "title": "Semantic Measures for the Comparison of Units of Language, Concepts or\n  Instances from Text and Knowledge Base Analysis", "cites": "86", "abstract": "Semantic measures are widely used today to estimate the strength of the\nsemantic relationship between elements of various types: units of language\n(e.g., words, sentences, documents), concepts or even instances semantically\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\nmeasures play an important role to compare such elements according to semantic\nproxies: texts and knowledge representations, which support their meaning or\ndescribe their nature. Semantic measures are therefore essential for designing\nintelligent agents which will for example take advantage of semantic analysis\nto mimic human ability to compare abstract or concrete objects. This paper\nproposes a comprehensive survey of the broad notion of semantic measure for the\ncomparison of units of language, concepts or instances based on semantic proxy\nanalyses. Semantic measures generalize the well-known notions of semantic\nsimilarity, semantic relatedness and semantic distance, which have been\nextensively studied by various communities over the last decades (e.g.,\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).", "no": 47}, {"url": "https://arxiv.org/abs/1305.5753", "title": "A probabilistic framework for analysing the compositionality of\n  conceptual combinations", "cites": "78", "abstract": "Conceptual combination performs a fundamental role in creating the broad\nrange of compound phrases utilized in everyday language. This article provides\na novel probabilistic framework for assessing whether the semantics of\nconceptual combinations are compositional, and so can be considered as a\nfunction of the semantics of the constituent concepts, or not. While the\nsystematicity and productivity of language provide a strong argument in favor\nof assuming compositionality, this very assumption is still regularly\nquestioned in both cognitive science and philosophy. Additionally, the\nprinciple of semantic compositionality is underspecified, which means that\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\nRather than adjudicating between different grades of compositionality, the\nframework presented here contributes formal methods for determining a clear\ndividing line between compositional and non-compositional semantics. In\naddition, we suggest that the distinction between these is contextually\nsensitive. Utilizing formal frameworks developed for analyzing composite\nsystems in quantum theory, we present two methods that allow the semantics of\nconceptual combinations to be classified as \"compositional\" or\n\"non-compositional\". Compositionality is first formalised by factorising the\njoint probability distribution modeling the combination, where the terms in the\nfactorisation correspond to individual concepts. This leads to the necessary\nand sufficient condition for the joint probability distribution to exist. A\nfailure to meet this condition implies that the underlying concepts cannot be\nmodeled in a single probability space when considering their combination, and\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\nare demonstrated by applying them to an empirical study of twenty-four\nnon-lexicalised conceptual combinations.", "no": 48}, {"url": "https://arxiv.org/abs/1308.4941", "title": "Automatic Labeling for Entity Extraction in Cyber Security", "cites": "78", "abstract": "Timely analysis of cyber-security information necessitates automated\ninformation extraction from unstructured text. While state-of-the-art\nextraction methods produce extremely accurate results, they require ample\ntraining data, which is generally unavailable for specialized applications,\nsuch as detecting security related entities; moreover, manual annotation of\ncorpora is very costly and often not a viable solution. In response, we develop\na very precise method to automatically label text from several data sources by\nleveraging related, domain-specific, structured data and provide public access\nto a corpus annotated with cyber-security entities. Next, we implement a\nMaximum Entropy Model trained with the average perceptron on a portion of our\ncorpus ($\\sim$750,000 words) and achieve near perfect precision, recall, and\naccuracy, with training times under 17 seconds.", "no": 49}, {"url": "https://arxiv.org/abs/1301.7382", "title": "Inferring Informational Goals from Free-Text Queries: A Bayesian\n  Approach", "cites": "76", "abstract": "People using consumer software applications typically do not use technical\njargon when querying an online database of help topics. Rather, they attempt to\ncommunicate their goals with common words and phrases that describe software\nfunctionality in terms of structure and objects they understand. We describe a\nBayesian approach to modeling the relationship between words in a user's query\nfor assistance and the informational goals of the user. After reviewing the\ngeneral method, we describe several extensions that center on integrating\nadditional distinctions and structure about language usage and user goals into\nthe Bayesian models.", "no": 50}, {"url": "https://arxiv.org/abs/1303.6175", "title": "Compression as a universal principle of animal behavior", "cites": "76", "abstract": "A key aim in biology and psychology is to identify fundamental principles\nunderpinning the behavior of animals, including humans. Analyses of human\nlanguage and the behavior of a range of non-human animal species have provided\nevidence for a common pattern underlying diverse behavioral phenomena: words\nfollow Zipf's law of brevity (the tendency of more frequently used words to be\nshorter), and conformity to this general pattern has been seen in the behavior\nof a number of other animals. It has been argued that the presence of this law\nis a sign of efficient coding in the information theoretic sense. However, no\nstrong direct connection has been demonstrated between the law and compression,\nthe information theoretic principle of minimizing the expected length of a\ncode. Here we show that minimizing the expected code length implies that the\nlength of a word cannot increase as its frequency increases. Furthermore, we\nshow that the mean code length or duration is significantly small in human\nlanguage, and also in the behavior of other species in all cases where\nagreement with the law of brevity has been found. We argue that compression is\na general principle of animal behavior, that reflects selection for efficiency\nof coding.", "no": 51}, {"url": "https://arxiv.org/abs/1312.0976", "title": "Multilinguals and Wikipedia Editing", "cites": "76", "abstract": "This article analyzes one month of edits to Wikipedia in order to examine the\nrole of users editing multiple language editions (referred to as multilingual\nusers). Such multilingual users may serve an important function in diffusing\ninformation across different language editions of the encyclopedia, and prior\nwork has suggested this could reduce the level of self-focus bias in each\nedition. This study finds multilingual users are much more active than their\nsingle-edition (monolingual) counterparts. They are found in all language\neditions, but smaller-sized editions with fewer users have a higher percentage\nof multilingual users than larger-sized editions. About a quarter of\nmultilingual users always edit the same articles in multiple languages, while\njust over 40% of multilingual users edit different articles in different\nlanguages. When non-English users do edit a second language edition, that\nedition is most frequently English. Nonetheless, several regional and\nlinguistic cross-editing patterns are also present.", "no": 52}, {"url": "https://arxiv.org/abs/1308.5010", "title": "Sentiment in New York City: A High Resolution Spatial and Temporal View", "cites": "71", "abstract": "Measuring public sentiment is a key task for researchers and policymakers\nalike. The explosion of available social media data allows for a more\ntime-sensitive and geographically specific analysis than ever before. In this\npaper we analyze data from the micro-blogging site Twitter and generate a\nsentiment map of New York City. We develop a classifier specifically tuned for\n140-character Twitter messages, or tweets, using key words, phrases and\nemoticons to determine the mood of each tweet. This method, combined with\ngeotagging provided by users, enables us to gauge public sentiment on extremely\nfine-grained spatial and temporal scales. We find that public mood is generally\nhighest in public parks and lowest at transportation hubs, and locate other\nareas of strong sentiment such as cemeteries, medical centers, a jail, and a\nsewage facility. Sentiment progressively improves with proximity to Times\nSquare. Periodic patterns of sentiment fluctuate on both a daily and a weekly\nscale: more positive tweets are posted on weekends than on weekdays, with a\ndaily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon.", "no": 53}, {"url": "https://arxiv.org/abs/1306.4886", "title": "Supervised Topical Key Phrase Extraction of News Stories using\n  Crowdsourcing, Light Filtering and Co-reference Normalization", "cites": "68", "abstract": "Fast and effective automated indexing is critical for search and personalized\nservices. Key phrases that consist of one or more words and represent the main\nconcepts of the document are often used for the purpose of indexing. In this\npaper, we investigate the use of additional semantic features and\npre-processing steps to improve automatic key phrase extraction. These features\ninclude the use of signal words and freebase categories. Some of these features\nlead to significant improvements in the accuracy of the results. We also\nexperimented with 2 forms of document pre-processing that we call light\nfiltering and co-reference normalization. Light filtering removes sentences\nfrom the document, which are judged peripheral to its main content.\nCo-reference normalization unifies several written forms of the same named\nentity into a unique form. We also needed a \"Gold Standard\" - a set of labeled\ndocuments for training and evaluation. While the subjective nature of key\nphrase selection precludes a true \"Gold Standard\", we used Amazon's Mechanical\nTurk service to obtain a useful approximation. Our data indicates that the\nbiggest improvements in performance were due to shallow semantic features, news\ncategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of\ndeeper semantic features such as Freebase sub-categories was not beneficial by\nitself, but in combination with pre-processing, did cause slight improvements\nin the nDCG scores.", "no": 54}, {"url": "https://arxiv.org/abs/1309.6162", "title": "JRC-Names: A freely available, highly multilingual named entity resource", "cites": "68", "abstract": "This paper describes a new, freely available, highly multilingual named\nentity resource for person and organisation names that has been compiled over\nseven years of large-scale multilingual news analysis combined with Wikipedia\nmining, resulting in 205,000 per-son and organisation names plus about the same\nnumber of spelling variants written in over 20 different scripts and in many\nmore languages. This resource, produced as part of the Europe Media Monitor\nactivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number\nof purposes. These include improving name search in databases or on the\ninternet, seeding machine learning systems to learn named entity recognition\nrules, improve machine translation results, and more. We describe here how this\nresource was created; we give statistics on its current size; we address the\nissue of morphological inflection; and we give details regarding its\nfunctionality. Updates to this resource will be made available daily.", "no": 55}, {"url": "https://arxiv.org/abs/1301.3618", "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\n  Semantic Word Vectors", "cites": "64", "abstract": "Knowledge bases provide applications with the benefit of easily accessible,\nsystematic relational knowledge but often suffer in practice from their\nincompleteness and lack of knowledge of new entities and relations. Much work\nhas focused on building or extending them by finding patterns in large\nunannotated text corpora. In contrast, here we mainly aim to complete a\nknowledge base by predicting additional true relationships between entities,\nbased on generalizations that can be discerned in the given knowledgebase. We\nintroduce a neural tensor network (NTN) model which predicts new relationship\nentries that can be added to the database. This model can be improved by\ninitializing entity representations with word vectors learned in an\nunsupervised fashion from text, and when doing this, existing relations can\neven be queried for entities that were not present in the database. Our model\ngeneralizes and outperforms existing models for this problem, and can classify\nunseen relationships in WordNet with an accuracy of 75.8%.", "no": 56}, {"url": "https://arxiv.org/abs/1309.7340", "title": "Early Stage Influenza Detection from Twitter", "cites": "63", "abstract": "Influenza is an acute respiratory illness that occurs virtually every year\nand results in substantial disease, death and expense. Detection of Influenza\nin its earliest stage would facilitate timely action that could reduce the\nspread of the illness. Existing systems such as CDC and EISS which try to\ncollect diagnosis data, are almost entirely manual, resulting in about two-week\ndelays for clinical data acquisition. Twitter, a popular microblogging service,\nprovides us with a perfect source for early-stage flu detection due to its\nreal- time nature. For example, when a flu breaks out, people that get the flu\nmay post related tweets which enables the detection of the flu breakout\npromptly. In this paper, we investigate the real-time flu detection problem on\nTwitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal\nunsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to\nidentify the flu breakout at the earliest stage. We test our model on real\nTwitter datasets from the United States along with baselines in multiple\napplications, such as real-time flu breakout detection, future epidemic phase\nprediction, or Influenza-like illness (ILI) physician visits. Experimental\nresults show the robustness and effectiveness of our approach. We build up a\nreal time flu reporting system based on the proposed approach, and we are\nhopeful that it would help government or health organizations in identifying\nflu outbreaks and facilitating timely actions to decrease unnecessary\nmortality.", "no": 57}, {"url": "https://arxiv.org/abs/1308.3785", "title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition", "cites": "60", "abstract": "This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).", "no": 58}, {"url": "https://arxiv.org/abs/1303.0347", "title": "Probing the statistical properties of unknown texts: application to the\n  Voynich Manuscript", "cites": "59", "abstract": "While the use of statistical physics methods to analyze large corpora has\nbeen useful to unveil many patterns in texts, no comprehensive investigation\nhas been performed investigating the properties of statistical measurements\nacross different languages and texts. In this study we propose a framework that\naims at determining if a text is compatible with a natural language and which\nlanguages are closest to it, without any knowledge of the meaning of the words.\nThe approach is based on three types of statistical measurements, i.e. obtained\nfrom first-order statistics of word properties in a text, from the topology of\ncomplex networks representing text, and from intermittency concepts where text\nis treated as a time series. Comparative experiments were performed with the\nNew Testament in 15 different languages and with distinct books in English and\nPortuguese in order to quantify the dependency of the different measurements on\nthe language and on the story being told in the book. The metrics found to be\ninformative in distinguishing real texts from their shuffled versions include\nassortativity, degree and selectivity of words. As an illustration, we analyze\nan undeciphered medieval manuscript known as the Voynich Manuscript. We show\nthat it is mostly compatible with natural languages and incompatible with\nrandom texts. We also obtain candidates for key-words of the Voynich Manuscript\nwhich could be helpful in the effort of deciphering it. Because we were able to\nidentify statistical measurements that are more dependent on the syntax than on\nthe semantics, the framework may also serve for text analysis in\nlanguage-dependent applications.", "no": 59}, {"url": "https://arxiv.org/abs/1308.0661", "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical\n  Texts", "cites": "59", "abstract": "Named entity recognition (NER) is a popular domain of natural language\nprocessing. For this reason, many tools exist to perform this task. Amongst\nother points, they differ in the processing method they rely upon, the entity\ntypes they can detect, the nature of the text they can handle, and their\ninput/output formats. This makes it difficult for a user to select an\nappropriate NER tool for a specific situation. In this article, we try to\nanswer this question in the context of biographic texts. For this matter, we\nfirst constitute a new corpus by annotating Wikipedia articles. We then select\npublicly available, well known and free for research NER tools for comparison:\nStanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply\nthem to our corpus, assess their performances and compare them. When\nconsidering overall performances, a clear hierarchy emerges: Stanford has the\nbest results, followed by LingPipe, Illionois and OpenCalais. However, a more\ndetailed evaluation performed relatively to entity types and article categories\nhighlights the fact their performances are diversely influenced by those\nfactors. This complementarity opens an interesting perspective regarding the\ncombination of these individual tools in order to improve performance.", "no": 60}, {"url": "https://arxiv.org/abs/1303.0350", "title": "Structure-semantics interplay in complex networks and its effects on the\n  predictability of similarity in texts", "cites": "56", "abstract": "There are different ways to define similarity for grouping similar texts into\nclusters, as the concept of similarity may depend on the purpose of the task.\nFor instance, in topic extraction similar texts mean those within the same\nsemantic field, whereas in author recognition stylistic features should be\nconsidered. In this study, we introduce ways to classify texts employing\nconcepts of complex networks, which may be able to capture syntactic, semantic\nand even pragmatic features. The interplay between the various metrics of the\ncomplex networks is analyzed with three applications, namely identification of\nmachine translation (MT) systems, evaluation of quality of machine translated\ntexts and authorship recognition. We shall show that topological features of\nthe networks representing texts can enhance the ability to identify MT systems\nin particular cases. For evaluating the quality of MT texts, on the other hand,\nhigh correlation was obtained with methods capable of capturing the semantics.\nThis was expected because the golden standards used are themselves based on\nword co-occurrence. Notwithstanding, the Katz similarity, which involves\nsemantic and structure in the comparison of texts, achieved the highest\ncorrelation with the NIST measurement, indicating that in some cases the\ncombination of both approaches can improve the ability to quantify quality in\nMT. In authorship recognition, again the topological features were relevant in\nsome contexts, though for the books and authors analyzed good results were\nobtained with semantic features as well. Because hybrid approaches encompassing\nsemantic and topological features have not been extensively used, we believe\nthat the methodology proposed here may be useful to enhance text classification\nconsiderably, as it combines well-established strategies.", "no": 61}, {"url": "https://arxiv.org/abs/1302.1612", "title": "Arabic text summarization based on latent semantic analysis to enhance\n  arabic documents clustering", "cites": "54", "abstract": "Arabic Documents Clustering is an important task for obtaining good results\nwith the traditional Information Retrieval (IR) systems especially with the\nrapid growth of the number of online documents present in Arabic language.\nDocuments clustering aim to automatically group similar documents in one\ncluster using different similarity/distance measures. This task is often\naffected by the documents length, useful information on the documents is often\naccompanied by a large amount of noise, and therefore it is necessary to\neliminate this noise while keeping useful information to boost the performance\nof Documents clustering. In this paper, we propose to evaluate the impact of\ntext summarization using the Latent Semantic Analysis Model on Arabic Documents\nClustering in order to solve problems cited above, using five\nsimilarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard\nCoefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler\nDivergence, for two times: without and with stemming. Our experimental results\nindicate that our proposed approach effectively solves the problems of noisy\ninformation and documents length, and thus significantly improve the clustering\nperformance.", "no": 62}, {"url": "https://arxiv.org/abs/1304.3841", "title": "The risks of mixing dependency lengths from sequences of different\n  length", "cites": "51", "abstract": "Mixing dependency lengths from sequences of different length is a common\npractice in language research. However, the empirical distribution of\ndependency lengths of sentences of the same length differs from that of\nsentences of varying length and the distribution of dependency lengths depends\non sentence length for real sentences and also under the null hypothesis that\ndependencies connect vertices located in random positions of the sequence. This\nsuggests that certain results, such as the distribution of syntactic dependency\nlengths mixing dependencies from sentences of varying length, could be a mere\nconsequence of that mixing. Furthermore, differences in the global averages of\ndependency length (mixing lengths from sentences of varying length) for two\ndifferent languages do not simply imply a priori that one language optimizes\ndependency lengths better than the other because those differences could be due\nto differences in the distribution of sentence lengths and other factors.", "no": 63}, {"url": "https://arxiv.org/abs/1309.5652", "title": "LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual", "cites": "50", "abstract": "The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\nfor natural language processing (NLP) research. Among these are a number of\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\nsingle collection of annotated documents. NLP research, however, usually\nrequires multiple data sets for the purposes of training models, developing\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\ncorpora used into the required data sets (divisions). This document details a\nset of rules that have been defined to enable consistent divisions for old and\nnew Arabic treebanks (ATB) and related corpora.", "no": 64}, {"url": "https://arxiv.org/abs/1309.1939", "title": "The placement of the head that minimizes online memory: a complex\n  systems approach", "cites": "47", "abstract": "It is well known that the length of a syntactic dependency determines its\nonline memory cost. Thus, the problem of the placement of a head and its\ndependents (complements or modifiers) that minimizes online memory is\nequivalent to the problem of the minimum linear arrangement of a star tree.\nHowever, how that length is translated into cognitive cost is not known. This\nstudy shows that the online memory cost is minimized when the head is placed at\nthe center, regardless of the function that transforms length into cost,\nprovided only that this function is strictly monotonically increasing. Online\nmemory defines a quasi-convex adaptive landscape with a single central minimum\nif the number of elements is odd and two central minima if that number is even.\nWe discuss various aspects of the dynamics of word order of subject (S), verb\n(V) and object (O) from a complex systems perspective and suggest that word\norders tend to evolve by swapping adjacent constituents from an initial or\nearly SOV configuration that is attracted towards a central word order by\nonline memory minimization. We also suggest that the stability of SVO is due to\nat least two factors, the quasi-convex shape of the adaptive landscape in the\nonline memory dimension and online memory adaptations that avoid regression to\nSOV. Although OVS is also optimal for placing the verb at the center, its low\nfrequency is explained by its long distance to the seminal SOV in the\npermutation space.", "no": 65}, {"url": "https://arxiv.org/abs/1311.3175", "title": "Architecture of an Ontology-Based Domain-Specific Natural Language\n  Question Answering System", "cites": "46", "abstract": "Question answering (QA) system aims at retrieving precise information from a\nlarge collection of documents against a query. This paper describes the\narchitecture of a Natural Language Question Answering (NLQA) system for a\nspecific domain based on the ontological information, a step towards semantic\nweb question answering. The proposed architecture defines four basic modules\nsuitable for enhancing current QA capabilities with the ability of processing\ncomplex questions. The first module was the question processing, which analyses\nand classifies the question and also reformulates the user query. The second\nmodule allows the process of retrieving the relevant documents. The next module\nprocesses the retrieved documents, and the last module performs the extraction\nand generation of a response. Natural language processing techniques are used\nfor processing the question and documents and also for answer extraction.\nOntology and domain knowledge are used for reformulating queries and\nidentifying the relations. The aim of the system is to generate short and\nspecific answer to the question that is asked in the natural language in a\nspecific domain. We have achieved 94 % accuracy of natural language question\nanswering in our implementation.", "no": 66}, {"url": "https://arxiv.org/abs/1304.3879", "title": "Automatic case acquisition from texts for process-oriented case-based\n  reasoning", "cites": "45", "abstract": "This paper introduces a method for the automatic acquisition of a rich case\nrepresentation from free text for process-oriented case-based reasoning. Case\nengineering is among the most complicated and costly tasks in implementing a\ncase-based reasoning system. This is especially so for process-oriented\ncase-based reasoning, where more expressive case representations are generally\nused and, in our opinion, actually required for satisfactory case adaptation.\nIn this context, the ability to acquire cases automatically from procedural\ntexts is a major step forward in order to reason on processes. We therefore\ndetail a methodology that makes case acquisition from processes described as\nfree text possible, with special attention given to assembly instruction texts.\nThis methodology extends the techniques we used to extract actions from cooking\nrecipes. We argue that techniques taken from natural language processing are\nrequired for this task, and that they give satisfactory results. An evaluation\nbased on our implemented prototype extracting workflows from recipe texts is\nprovided.", "no": 67}, {"url": "https://arxiv.org/abs/1312.2137", "title": "End-to-end Phoneme Sequence Recognition using Convolutional Neural\n  Networks", "cites": "44", "abstract": "Most phoneme recognition state-of-the-art systems rely on a classical neural\nnetwork classifiers, fed with highly tuned features, such as MFCC or PLP\nfeatures. Recent advances in ``deep learning'' approaches questioned such\nsystems, but while some attempts were made with simpler features such as\nspectrograms, state-of-the-art systems still rely on MFCCs. This might be\nviewed as a kind of failure from deep learning approaches, which are often\nclaimed to have the ability to train with raw signals, alleviating the need of\nhand-crafted features. In this paper, we investigate a convolutional neural\nnetwork approach for raw speech signals. While convolutional architectures got\ntremendous success in computer vision or text processing, they seem to have\nbeen let down in the past recent years in the speech processing field. We show\nthat it is possible to learn an end-to-end phoneme sequence classifier system\ndirectly from raw signal, with similar performance on the TIMIT and WSJ\ndatasets than existing systems based on MFCC, questioning the need of complex\nhand-crafted features on large datasets.", "no": 68}, {"url": "https://arxiv.org/abs/1304.4520", "title": "Sentiment Analysis : A Literature Survey", "cites": "43", "abstract": "Our day-to-day life has always been influenced by what people think. Ideas\nand opinions of others have always affected our own opinions. The explosion of\nWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,\nContributing to RSS, Social Bookmarking, and Social Networking. As a result\nthere has been an eruption of interest in people to mine these vast resources\nof data for opinions. Sentiment Analysis or Opinion Mining is the computational\ntreatment of opinions, sentiments and subjectivity of text. In this report, we\ntake a look at the various challenges and applications of Sentiment Analysis.\nWe will discuss in details various approaches to perform a computational\ntreatment of sentiments and opinions. Various supervised or data-driven\ntechniques to SA like Na\\\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons\nwill be discussed and their strengths and drawbacks will be touched upon. We\nwill also see a new dimension of analyzing sentiments by Cognitive Psychology\nmainly through the work of Janyce Wiebe, where we will see ways to detect\nsubjectivity, perspective in narrative and understanding the discourse\nstructure. We will also study some specific topics in Sentiment Analysis and\nthe contemporary works in those areas.", "no": 69}, {"url": "https://arxiv.org/abs/1306.2158", "title": "\"Not not bad\" is not \"bad\": A distributional account of negation", "cites": "43", "abstract": "With the increasing empirical success of distributional models of\ncompositional semantics, it is timely to consider the types of textual logic\nthat such models are capable of capturing. In this paper, we address\nshortcomings in the ability of current models to capture logical operations\nsuch as negation. As a solution we propose a tripartite formulation for a\ncontinuous vector space representation of semantics and subsequently use this\nrepresentation to develop a formal compositional notion of negation within such\nmodels.", "no": 70}, {"url": "https://arxiv.org/abs/1309.5942", "title": "Colourful Language: Measuring Word-Colour Associations", "cites": "43", "abstract": "Since many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complimented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\nthat even though they cannot be physically visualized, they too tend to have\nstrong colour associations. Finally, we show how word-colour associations\nmanifest themselves in language, and quantify usefulness of co-occurrence and\npolarity cues in automatically detecting colour associations.", "no": 71}, {"url": "https://arxiv.org/abs/1312.6192", "title": "Can recursive neural tensor networks learn logical reasoning?", "cites": "43", "abstract": "Recursive neural network models and their accompanying vector representations\nfor words have seen success in an array of increasingly semantically\nsophisticated tasks, but almost nothing is known about their ability to\naccurately capture the aspects of linguistic meaning that are necessary for\ninterpretation or reasoning. To evaluate this, I train a recursive model on a\nnew corpus of constructed examples of logical reasoning in short sentences,\nlike the inference of \"some animal walks\" from \"some dog walks\" or \"some cat\nwalks,\" given that dogs and cats are animals. This model learns representations\nthat generalize well to new types of reasoning pattern in all but a few cases,\na result which is promising for the ability of learned representation models to\ncapture logical reasoning.", "no": 72}, {"url": "https://arxiv.org/abs/1302.4471", "title": "Word sense disambiguation via high order of learning in complex networks", "cites": "42", "abstract": "Complex networks have been employed to model many real systems and as a\nmodeling tool in a myriad of applications. In this paper, we use the framework\nof complex networks to the problem of supervised classification in the word\ndisambiguation task, which consists in deriving a function from the supervised\n(or labeled) training data of ambiguous words. Traditional supervised data\nclassification takes into account only topological or physical features of the\ninput data. On the other hand, the human (animal) brain performs both low- and\nhigh-level orders of learning and it has facility to identify patterns\naccording to the semantic meaning of the input data. In this paper, we apply a\nhybrid technique which encompasses both types of learning in the field of word\nsense disambiguation and show that the high-level order of learning can really\nimprove the accuracy rate of the model. This evidence serves to demonstrate\nthat the internal structures formed by the words do present patterns that,\ngenerally, cannot be correctly unveiled by only traditional techniques.\nFinally, we exhibit the behavior of the model for different weights of the low-\nand high-level classifiers by plotting decision boundaries. This study helps\none to better understand the effectiveness of the model.", "no": 73}, {"url": "https://arxiv.org/abs/1303.4293", "title": "A Multilingual Semantic Wiki Based on Attempto Controlled English and\n  Grammatical Framework", "cites": "42", "abstract": "We describe a semantic wiki system with an underlying controlled natural\nlanguage grammar implemented in Grammatical Framework (GF). The grammar\nrestricts the wiki content to a well-defined subset of Attempto Controlled\nEnglish (ACE), and facilitates a precise bidirectional automatic translation\nbetween ACE and language fragments of a number of other natural languages,\nmaking the wiki content accessible multilingually. Additionally, our approach\nallows for automatic translation into the Web Ontology Language (OWL), which\nenables automatic reasoning over the wiki content. The developed wiki\nenvironment thus allows users to build, query and view OWL knowledge bases via\na user-friendly multilingual natural language interface. As a further feature,\nthe underlying multilingual grammar is integrated into the wiki and can be\ncollaboratively edited to extend the vocabulary of the wiki or even customize\nits sentence structures. This work demonstrates the combination of the existing\ntechnologies of Attempto Controlled English and Grammatical Framework, and is\nimplemented as an extension of the existing semantic wiki engine AceWiki.", "no": 74}, {"url": "https://arxiv.org/abs/1309.5223", "title": "JRC EuroVoc Indexer JEX - A freely available multi-label categorisation\n  tool", "cites": "42", "abstract": "EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700\nhierarchically organised subject domains used by European Institutions and many\nauthorities in Member States of the European Union (EU) for the classification\nand retrieval of official documents. JEX is JRC-developed multi-label\nclassification software that learns from manually labelled data to\nautomatically assign EuroVoc descriptors to new documents in a profile-based\ncategory-ranking task. The JEX release consists of trained classifiers for 22\nofficial EU languages, of parallel training data in the same languages, of an\ninterface that allows viewing and amending the assignment results, and of a\nmodule that allows users to re-train the tool on their own document\ncollections. JEX allows advanced users to change the document representation so\nas to possibly improve the categorisation result through linguistic\npre-processing. JEX can be used as a tool for interactive EuroVoc descriptor\nassignment to increase speed and consistency of the human categorisation\nprocess, or it can be used fully automatically. The output of JEX is a\nlanguage-independent EuroVoc feature vector lending itself also as input to\nvarious other Language Technology tasks, including cross-lingual clustering and\nclassification, cross-lingual plagiarism detection, sentence selection and\nranking, and more.", "no": 75}, {"url": "https://arxiv.org/abs/1308.1847", "title": "The Royal Birth of 2013: Analysing and Visualising Public Sentiment in\n  the UK Using Twitter", "cites": "39", "abstract": "Analysis of information retrieved from microblogging services such as Twitter\ncan provide valuable insight into public sentiment in a geographic region. This\ninsight can be enriched by visualising information in its geographic context.\nTwo underlying approaches for sentiment analysis are dictionary-based and\nmachine learning. The former is popular for public sentiment analysis, and the\nlatter has found limited use for aggregating public sentiment from Twitter\ndata. The research presented in this paper aims to extend the machine learning\napproach for aggregating public sentiment. To this end, a framework for\nanalysing and visualising public sentiment from a Twitter corpus is developed.\nA dictionary-based approach and a machine learning approach are implemented\nwithin the framework and compared using one UK case study, namely the royal\nbirth of 2013. The case study validates the feasibility of the framework for\nanalysis and rapid visualisation. One observation is that there is good\ncorrelation between the results produced by the popular dictionary-based\napproach and the machine learning approach when large volumes of tweets are\nanalysed. However, for rapid analysis to be possible faster methods need to be\ndeveloped using big data techniques and parallel methods.", "no": 76}, {"url": "https://arxiv.org/abs/1304.4086", "title": "Hubiness, length, crossings and their relationships in dependency trees", "cites": "38", "abstract": "Here tree dependency structures are studied from three different\nperspectives: their degree variance (hubiness), the mean dependency length and\nthe number of dependency crossings. Bounds that reveal pairwise dependencies\namong these three metrics are derived. Hubiness (the variance of degrees) plays\na central role: the mean dependency length is bounded below by hubiness while\nthe number of crossings is bounded above by hubiness. Our findings suggest that\nthe online memory cost of a sentence might be determined not just by the\nordering of words but also by the hubiness of the underlying structure. The 2nd\nmoment of degree plays a crucial role that is reminiscent of its role in large\ncomplex networks.", "no": 77}, {"url": "https://arxiv.org/abs/1307.4300", "title": "Rule Based Transliteration Scheme for English to Punjabi", "cites": "38", "abstract": "Machine Transliteration has come out to be an emerging and a very important\nresearch area in the field of machine translation. Transliteration basically\naims to preserve the phonological structure of words. Proper transliteration of\nname entities plays a very significant role in improving the quality of machine\ntranslation. In this paper we are doing machine transliteration for\nEnglish-Punjabi language pair using rule based approach. We have constructed\nsome rules for syllabification. Syllabification is the process to extract or\nseparate the syllable from the words. In this we are calculating the\nprobabilities for name entities (Proper names and location). For those words\nwhich do not come under the category of name entities, separate probabilities\nare being calculated by using relative frequency through a statistical machine\ntranslation toolkit known as MOSES. Using these probabilities we are\ntransliterating our input text from English to Punjabi.", "no": 78}, {"url": "https://arxiv.org/abs/1310.0575", "title": "Development of Marathi Part of Speech Tagger Using Statistical Approach", "cites": "38", "abstract": "Part-of-speech (POS) tagging is a process of assigning the words in a text\ncorresponding to a particular part of speech. A fundamental version of POS\ntagging is the identification of words as nouns, verbs, adjectives etc. For\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\none of the simplest as well as most constant and statistical model for many NLP\napplications. POS Tagging is an initial stage of linguistics, text analysis\nlike information retrieval, machine translator, text to speech synthesis,\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\neach word in a sentence and literature. Various approaches have been proposed\nto implement POS taggers. In this paper we present a Marathi part of speech\ntagger. It is morphologically rich language. Marathi is spoken by the native\npeople of Maharashtra. The general approach used for development of tagger is\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\nidea about all the algorithms with suitable examples. It also introduces a tag\nset for Marathi which can be used for tagging Marathi text. In this paper we\nhave shown the development of the tagger as well as compared to check the\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\nrespectively.", "no": 79}, {"url": "https://arxiv.org/abs/1308.4189", "title": "Seeing What You're Told: Sentence-Guided Activity Recognition In Video", "cites": "37", "abstract": "We present a system that demonstrates how the compositional structure of\nevents, in concert with the compositional structure of language, can interplay\nwith the underlying focusing mechanisms in video action recognition, thereby\nproviding a medium, not only for top-down and bottom-up integration, but also\nfor multi-modal integration between vision and language. We show how the roles\nplayed by participants (nouns), their characteristics (adjectives), the actions\nperformed (verbs), the manner of such actions (adverbs), and changing spatial\nrelations between participants (prepositions) in the form of whole sentential\ndescriptions mediated by a grammar, guides the activity-recognition process.\nFurther, the utility and expressiveness of our framework is demonstrated by\nperforming three separate tasks in the domain of multi-activity videos:\nsentence-guided focus of attention, generation of sentential descriptions of\nvideo, and query-based video search, simply by leveraging the framework in\ndifferent manners.", "no": 80}, {"url": "https://arxiv.org/abs/1302.4465", "title": "Unveiling the relationship between complex networks metrics and word\n  senses", "cites": "36", "abstract": "The automatic disambiguation of word senses (i.e., the identification of\nwhich of the meanings is used in a given context for a word that has multiple\nmeanings) is essential for such applications as machine translation and\ninformation retrieval, and represents a key step for developing the so-called\nSemantic Web. Humans disambiguate words in a straightforward fashion, but this\ndoes not apply to computers. In this paper we address the problem of Word Sense\nDisambiguation (WSD) by treating texts as complex networks, and show that word\nsenses can be distinguished upon characterizing the local structure around\nambiguous words. Our goal was not to obtain the best possible disambiguation\nsystem, but we nevertheless found that in half of the cases our approach\noutperforms traditional shallow methods. We show that the hierarchical\nconnectivity and clustering of words are usually the most relevant features for\nWSD. The results reported here shine light on the relationship between semantic\nand structural parameters of complex networks. They also indicate that when\ncombined with traditional techniques the complex network approach may be useful\nto enhance the discrimination of senses in large texts", "no": 81}, {"url": "https://arxiv.org/abs/1401.0509", "title": "Zero-Shot Learning for Semantic Utterance Classification", "cites": "36", "abstract": "We propose a novel zero-shot learning method for semantic utterance\nclassification (SUC). It learns a classifier $f: X \\to Y$ for problems where\nnone of the semantic categories $Y$ are present in the training set. The\nframework uncovers the link between categories and utterances using a semantic\nspace. We show that this semantic space can be learned by deep neural networks\ntrained on large amounts of search engine query log data. More precisely, we\npropose a novel method that can learn discriminative semantic features without\nsupervision. It uses the zero-shot learning framework to guide the learning of\nthe semantic features. We demonstrate the effectiveness of the zero-shot\nsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).\nFurthermore, we achieve state-of-the-art results by combining the semantic\nfeatures with a supervised method.", "no": 82}, {"url": "https://arxiv.org/abs/1302.4490", "title": "Complex networks analysis of language complexity", "cites": "35", "abstract": "Methods from statistical physics, such as those involving complex networks,\nhave been increasingly used in quantitative analysis of linguistic phenomena.\nIn this paper, we represented pieces of text with different levels of\nsimplification in co-occurrence networks and found that topological regularity\ncorrelated negatively with textual complexity. Furthermore, in less complex\ntexts the distance between concepts, represented as nodes, tended to decrease.\nThe complex networks metrics were treated with multivariate pattern recognition\ntechniques, which allowed us to distinguish between original texts and their\nsimplified versions. For each original text, two simplified versions were\ngenerated manually with increasing number of simplification operations. As\nexpected, distinction was easier for the strongly simplified versions, where\nthe most relevant metrics were node strength, shortest paths and diversity.\nAlso, the discrimination of complex texts was improved with higher hierarchical\nnetwork metrics, thus pointing to the usefulness of considering wider contexts\naround the concepts. Though the accuracy rate in the distinction was not as\nhigh as in methods using deep linguistic knowledge, the complex network\napproach is still useful for a rapid screening of texts whenever assessing\ncomplexity is essential to guarantee accessibility to readers with limited\nreading ability", "no": 83}, {"url": "https://arxiv.org/abs/1304.7942", "title": "ManTIME: Temporal expression identification and normalization in the\n  TempEval-3 challenge", "cites": "35", "abstract": "This paper describes a temporal expression identification and normalization\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\nphase combines the use of conditional random fields along with a\npost-processing identification pipeline, whereas the normalization phase is\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\ninvestigate the performance variation with respect to different feature types.\nSpecifically, we show that the use of WordNet-based features in the\nidentification task negatively affects the overall performance, and that there\nis no statistically significant difference in using gazetteers, shallow parsing\nand propositional noun phrases labels on top of the morphological features. On\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\naddition to the gold annotated ones) does not improve the performance.", "no": 84}, {"url": "https://arxiv.org/abs/1308.4648", "title": "PACE: Pattern Accurate Computationally Efficient Bootstrapping for\n  Timely Discovery of Cyber-Security Concepts", "cites": "35", "abstract": "Public disclosure of important security information, such as knowledge of\nvulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and\nother online sources months before proper classification into structured\ndatabases. In order to facilitate timely discovery of such knowledge, we\npropose a novel semi-supervised learning algorithm, PACE, for identifying and\nclassifying relevant entities in text sources. The main contribution of this\npaper is an enhancement of the traditional bootstrapping method for entity\nextraction by employing a time-memory trade-off that simultaneously circumvents\na costly corpus search while strengthening pattern nomination, which should\nincrease accuracy. An implementation in the cyber-security domain is discussed\nas well as challenges to Natural Language Processing imposed by the security\ndomain.", "no": 85}, {"url": "https://arxiv.org/abs/1310.6772", "title": "Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive\n  Writing for Linking Identities", "cites": "35", "abstract": "This paper describes the corpus of sockpuppet cases we gathered from\nWikipedia. A sockpuppet is an online user account created with a fake identity\nfor the purpose of covering abusive behavior and/or subverting the editing\nregulation process. We used a semi-automated method for crawling and curating a\ndataset of real sockpuppet investigation cases. To the best of our knowledge,\nthis is the first corpus available on real-world deceptive writing. We describe\nthe process for crawling the data and some preliminary results that can be used\nas baseline for benchmarking research. The dataset will be released under a\nCreative Commons license from our project website: http://docsig.cis.uab.edu.", "no": 86}, {"url": "https://arxiv.org/abs/1309.4628", "title": "Text segmentation with character-level text embeddings", "cites": "34", "abstract": "Learning word representations has recently seen much success in computational\nlinguistics. However, assuming sequences of word tokens as input to linguistic\nanalysis is often unjustified. For many languages word segmentation is a\nnon-trivial task and naturally occurring text is sometimes a mixture of natural\nlanguage strings and other character data. We propose to learn text\nrepresentations directly from raw character sequences by training a Simple\nrecurrent Network to predict the next character in text. The network uses its\nhidden layer to evolve abstract representations of the character sequences it\nsees. To demonstrate the usefulness of the learned text embeddings, we use them\nas features in a supervised character level text segmentation and labeling\ntask: recognizing spans of text containing programming language code. By using\nthe embeddings as features we are able to substantially improve over a baseline\nwhich uses only surface character n-grams.", "no": 87}, {"url": "https://arxiv.org/abs/1302.5675", "title": "Development of Yes/No Arabic Question Answering System", "cites": "32", "abstract": "Developing Question Answering systems has been one of the important research\nissues because it requires insights from a variety of\ndisciplines,including,Artificial Intelligence,Information Retrieval,\nInformation Extraction,Natural Language Processing, and Psychology.In this\npaper we realize a formal model for a lightweight semantic based open domain\nyes/no Arabic question answering system based on paragraph retrieval with\nvariable length. We propose a constrained semantic representation. Using an\nexplicit unification framework based on semantic similarities and query\nexpansion synonyms and antonyms.This frequently improves the precision of the\nsystem. Employing the passage retrieval system achieves a better precision by\nretrieving more paragraphs that contain relevant answers to the question; It\nsignificantly reduces the amount of text to be processed by the system.", "no": 88}, {"url": "https://arxiv.org/abs/1309.6352", "title": "Using Nuances of Emotion to Identify Personality", "cites": "32", "abstract": "Past work on personality detection has shown that frequency of lexical\ncategories such as first person pronouns, past tense verbs, and sentiment words\nhave significant correlations with personality traits. In this paper, for the\nfirst time, we show that fine affect (emotion) categories such as that of\nexcitement, guilt, yearning, and admiration are significant indicators of\npersonality. Additionally, we perform experiments to show that the gains\nprovided by the fine affect categories are not obtained by using coarse affect\ncategories alone or with specificity features alone. We employ these features\nin five SVM classifiers for detecting five personality traits through essays.\nWe find that the use of fine emotion features leads to statistically\nsignificant improvement over a competitive baseline, whereas the use of coarse\naffect and specificity features does not.", "no": 89}, {"url": "https://arxiv.org/abs/1311.1539", "title": "Category-Theoretic Quantitative Compositional Distributional Models of\n  Natural Language Semantics", "cites": "32", "abstract": "This thesis is about the problem of compositionality in distributional\nsemantics. Distributional semantics presupposes that the meanings of words are\na function of their occurrences in textual contexts. It models words as\ndistributions over these contexts and represents them as vectors in high\ndimensional spaces. The problem of compositionality for such models concerns\nitself with how to produce representations for larger units of text by\ncomposing the representations of smaller units of text.\n  This thesis focuses on a particular approach to this compositionality\nproblem, namely using the categorical framework developed by Coecke, Sadrzadeh,\nand Clark, which combines syntactic analysis formalisms with distributional\nsemantic representations of meaning to produce syntactically motivated\ncomposition operations. This thesis shows how this approach can be\ntheoretically extended and practically implemented to produce concrete\ncompositional distributional models of natural language semantics. It\nfurthermore demonstrates that such models can perform on par with, or better\nthan, other competing approaches in the field of natural language processing.\n  There are three principal contributions to computational linguistics in this\nthesis. The first is to extend the DisCoCat framework on the syntactic front\nand semantic front, incorporating a number of syntactic analysis formalisms and\nproviding learning procedures allowing for the generation of concrete\ncompositional distributional models. The second contribution is to evaluate the\nmodels developed from the procedures presented here, showing that they\noutperform other compositional distributional models present in the literature.\nThe third contribution is to show how using category theory to solve linguistic\nproblems forms a sound basis for research, illustrated by examples of work on\nthis topic, that also suggest directions for future research.", "no": 90}, {"url": "https://arxiv.org/abs/1312.0482", "title": "Learning Semantic Representations for the Phrase Translation Model", "cites": "32", "abstract": "This paper presents a novel semantic-based phrase translation model. A pair\nof source and target phrases are projected into continuous-valued vector\nrepresentations in a low-dimensional latent semantic space, where their\ntranslation score is computed by the distance between the pair in this new\nspace. The projection is performed by a multi-layer neural network whose\nweights are learned on parallel training data. The learning is aimed to\ndirectly optimize the quality of end-to-end machine translation results.\nExperimental evaluation has been performed on two Europarl translation tasks,\nEnglish-French and German-English. The results show that the new semantic-based\nphrase translation model significantly improves the performance of a\nstate-of-the-art phrase-based statistical machine translation sys-tem, leading\nto a gain of 0.7-1.0 BLEU points.", "no": 91}, {"url": "https://arxiv.org/abs/1305.3107", "title": "I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in\n  Twitter", "cites": "31", "abstract": "Twitter has become a major source of data for social media researchers. One\nimportant aspect of Twitter not previously considered are {\\em deletions} --\nremoval of tweets from the stream. Deletions can be due to a multitude of\nreasons such as privacy concerns, rashness or attempts to undo public\nstatements. We show how deletions can be automatically predicted ahead of time\nand analyse which tweets are likely to be deleted and how.", "no": 92}, {"url": "https://arxiv.org/abs/1311.6063", "title": "NILE: Fast Natural Language Processing for Electronic Health Records", "cites": "31", "abstract": "Objective: Narrative text in Electronic health records (EHR) contain rich\ninformation for medical and data science studies. This paper introduces the\ndesign and performance of Narrative Information Linear Extraction (NILE), a\nnatural language processing (NLP) package for EHR analysis that we share with\nthe medical informatics community. Methods: NILE uses a modified prefix-tree\nsearch algorithm for named entity recognition, which can detect prefix and\nsuffix sharing. The semantic analyses are implemented as rule-based finite\nstate machines. Analyses include negation, location, modification, family\nhistory, and ignoring. Result: The processing speed of NILE is hundreds to\nthousands times faster than existing NLP software for medical text. The\naccuracy of presence analysis of NILE is on par with the best performing models\non the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and\nbeing able to operate via API make NILE a valuable addition to the NLP software\nfor medical informatics and data science.", "no": 93}, {"url": "https://arxiv.org/abs/1303.1441", "title": "A Hybrid Approach to Extract Keyphrases from Medical Documents", "cites": "30", "abstract": "Keyphrases are the phrases, consisting of one or more words, representing the\nimportant concepts in the articles. Keyphrases are useful for a variety of\ntasks such as text summarization, automatic indexing,\nclustering/classification, text mining etc. This paper presents a hybrid\napproach to keyphrase extraction from medical documents. The keyphrase\nextraction approach presented in this paper is an amalgamation of two methods:\nthe first one assigns weights to candidate keyphrases based on an effective\ncombination of features such as position, term frequency, inverse document\nfrequency and the second one assign weights to candidate keyphrases using some\nknowledge about their similarities to the structure and characteristics of\nkeyphrases available in the memory (stored list of keyphrases). An efficient\ncandidate keyphrase identification method as the first component of the\nproposed keyphrase extraction system has also been introduced in this paper.\nThe experimental results show that the proposed hybrid approach performs better\nthan some state-of-the art keyphrase extraction approaches.", "no": 94}, {"url": "https://arxiv.org/abs/1312.0493", "title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure", "cites": "30", "abstract": "Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach.", "no": 95}, {"url": "https://arxiv.org/abs/1305.0556", "title": "A quantum teleportation inspired algorithm produces sentence meaning\n  from word meaning and grammatical structure", "cites": "29", "abstract": "We discuss an algorithm which produces the meaning of a sentence given\nmeanings of its words, and its resemblance to quantum teleportation. In fact,\nthis protocol was the main source of inspiration for this algorithm which has\nmany applications in the area of Natural Language Processing.", "no": 96}, {"url": "https://arxiv.org/abs/1309.5391", "title": "Even the Abstract have Colour: Consensus in Word-Colour Associations", "cites": "29", "abstract": "Colour is a key component in the successful dissemination of information.\nSince many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complemented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. A word-choice question was used to obtain sense-level\nannotations and to ensure data quality. We focus especially on abstract\nconcepts and emotions to show that even they tend to have strong colour\nassociations. Thus, using the right colours can not only improve semantic\ncoherence, but also inspire the desired emotional response.", "no": 97}, {"url": "https://arxiv.org/abs/1311.3011", "title": "Cornell SPF: Cornell Semantic Parsing Framework", "cites": "29", "abstract": "The Cornell Semantic Parsing Framework (SPF) is a learning and inference\nframework for mapping natural language to formal representation of its meaning.", "no": 98}, {"url": "https://arxiv.org/abs/1301.4432", "title": "Language learning from positive evidence, reconsidered: A\n  simplicity-based approach", "cites": "28", "abstract": "Children learn their native language by exposure to their linguistic and\ncommunicative environment, but apparently without requiring that their mistakes\nare corrected. Such learning from positive evidence has been viewed as raising\nlogical problems for language acquisition. In particular, without correction,\nhow is the child to recover from conjecturing an over-general grammar, which\nwill be consistent with any sentence that the child hears? There have been many\nproposals concerning how this logical problem can be dissolved. Here, we review\nrecent formal results showing that the learner has sufficient data to learn\nsuccessfully from positive evidence, if it favours the simplest encoding of the\nlinguistic input. Results include the ability to learn a linguistic prediction,\ngrammaticality judgements, language production, and form-meaning mappings. The\nsimplicity approach can also be scaled-down to analyse the ability to learn a\nspecific linguistic constructions, and is amenable to empirical test as a\nframework for describing human language acquisition.", "no": 99}, {"url": "https://arxiv.org/abs/1306.3692", "title": "An open diachronic corpus of historical Spanish: annotation criteria and\n  automatic modernisation of spelling", "cites": "28", "abstract": "The IMPACT-es diachronic corpus of historical Spanish compiles over one\nhundred books --containing approximately 8 million words-- in addition to a\ncomplementary lexicon which links more than 10 thousand lemmas with\nattestations of the different variants found in the documents. This textual\ncorpus and the accompanying lexicon have been released under an open license\n(Creative Commons by-nc-sa) in order to permit their intensive exploitation in\nlinguistic research. Approximately 7% of the words in the corpus (a selection\naimed at enhancing the coverage of the most frequent word forms) have been\nannotated with their lemma, part of speech, and modern equivalent. This paper\ndescribes the annotation criteria followed and the standards, based on the Text\nEncoding Initiative recommendations, used to the represent the texts in digital\nform. As an illustration of the possible synergies between diachronic textual\nresources and linguistic research, we describe the application of statistical\nmachine translation techniques to infer probabilistic context-sensitive rules\nfor the automatic modernisation of spelling. The automatic modernisation with\nthis type of statistical methods leads to very low character error rates when\nthe output is compared with the supervised modern version of the text.", "no": 100}]