[{"url": "https://arxiv.org/abs/1409.0473", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "cites": "24 294", "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.", "no": 1}, {"url": "https://arxiv.org/abs/1406.1078", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "cites": "19 831", "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.", "no": 2}, {"url": "https://arxiv.org/abs/1409.3215", "title": "Sequence to Sequence Learning with Neural Networks", "cites": "18 223", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.", "no": 3}, {"url": "https://arxiv.org/abs/1408.5882", "title": "Convolutional Neural Networks for Sentence Classification", "cites": "12 271", "abstract": "We report on a series of experiments with convolutional neural networks (CNN)\ntrained on top of pre-trained word vectors for sentence-level classification\ntasks. We show that a simple CNN with little hyperparameter tuning and static\nvectors achieves excellent results on multiple benchmarks. Learning\ntask-specific vectors through fine-tuning offers further gains in performance.\nWe additionally propose a simple modification to the architecture to allow for\nthe use of both task-specific and static vectors. The CNN models discussed\nherein improve upon the state of the art on 4 out of 7 tasks, which include\nsentiment analysis and question classification.", "no": 4}, {"url": "https://arxiv.org/abs/1405.4053", "title": "Distributed Representations of Sentences and Documents", "cites": "8 494", "abstract": "Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.", "no": 5}, {"url": "https://arxiv.org/abs/1409.1259", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches", "cites": "5 868", "abstract": "Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically.", "no": 6}, {"url": "https://arxiv.org/abs/1404.2188", "title": "A Convolutional Neural Network for Modelling Sentences", "cites": "3 402", "abstract": "The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.", "no": 7}, {"url": "https://arxiv.org/abs/1411.5726", "title": "CIDEr: Consensus-based Image Description Evaluation", "cites": "3 315", "abstract": "Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.", "no": 8}, {"url": "https://arxiv.org/abs/1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge\n  Bases", "cites": "2 445", "abstract": "We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning.", "no": 9}, {"url": "https://arxiv.org/abs/1412.5567", "title": "Deep Speech: Scaling up end-to-end speech recognition", "cites": "1 894", "abstract": "We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.", "no": 10}, {"url": "https://arxiv.org/abs/1409.2944", "title": "Collaborative Deep Learning for Recommender Systems", "cites": "1 503", "abstract": "Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.", "no": 11}, {"url": "https://arxiv.org/abs/1402.3722", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method", "cites": "1 496", "abstract": "The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.", "no": 12}, {"url": "https://arxiv.org/abs/1410.3916", "title": "Memory Networks", "cites": "1 491", "abstract": "We describe a new class of learning models called memory networks. Memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. The long-term memory can be\nread and written to, with the goal of using it for prediction. We investigate\nthese models in the context of question answering (QA) where the long-term\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\nmore complex, toy task generated from a simulated world. In the latter, we show\nthe reasoning power of such models by chaining multiple supporting sentences to\nanswer questions that require understanding the intension of verbs.", "no": 13}, {"url": "https://arxiv.org/abs/1411.2539", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models", "cites": "1 261", "abstract": "Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison.", "no": 14}, {"url": "https://arxiv.org/abs/1411.4952", "title": "From Captions to Visual Concepts and Back", "cites": "1 242", "abstract": "This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime.", "no": 15}, {"url": "https://arxiv.org/abs/1408.3456", "title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity\n  Estimation", "cites": "1 229", "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional\nsemantic models that improves on existing resources in several important ways.\nFirst, in contrast to gold standards such as WordSim-353 and MEN, it explicitly\nquantifies similarity rather than association or relatedness, so that pairs of\nentities that are associated but not actually similar [Freud, psychology] have\na low rating. We show that, via this focus on similarity, SimLex-999\nincentivizes the development of models with a different, and arguably wider\nrange of applications than those which reflect conceptual association. Second,\nSimLex-999 contains a range of concrete and abstract adjective, noun and verb\npairs, together with an independent rating of concreteness and (free)\nassociation strength for each pair. This diversity enables fine-grained\nanalyses of the performance of models on concepts of different types, and\nconsequently greater insight into how architectures can be improved. Further,\nunlike existing gold standard evaluations, for which automatic approaches have\nreached or surpassed the inter-annotator agreement ceiling, state-of-the-art\nmodels perform well below this ceiling on SimLex-999. There is therefore plenty\nof scope for SimLex-999 to quantify future improvements to distributional\nsemantic models, guiding the development of the next generation of\nrepresentation-learning architectures.", "no": 16}, {"url": "https://arxiv.org/abs/1412.6632", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "cites": "1 164", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .", "no": 17}, {"url": "https://arxiv.org/abs/1412.2007", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "cites": "959", "abstract": "Neural machine translation, a recently proposed approach to machine\ntranslation based purely on neural networks, has shown promising results\ncompared to the existing approaches such as phrase-based statistical machine\ntranslation. Despite its recent success, neural machine translation has its\nlimitation in handling a larger vocabulary, as training complexity as well as\ndecoding complexity increase proportionally to the number of target words. In\nthis paper, we propose a method that allows us to use a very large target\nvocabulary without increasing training complexity, based on importance\nsampling. We show that decoding can be efficiently done even with the model\nhaving a very large target vocabulary by selecting only a small subset of the\nwhole target vocabulary. The models trained by the proposed approach are\nempirically found to outperform the baseline models with a small vocabulary as\nwell as the LSTM-based neural machine translation models. Furthermore, when we\nuse the ensemble of a few models with very large target vocabularies, we\nachieve the state-of-the-art translation performance (measured by BLEU) on the\nEnglish->German translation and almost as high performance as state-of-the-art\nEnglish->French translation system.", "no": 18}, {"url": "https://arxiv.org/abs/1402.1128", "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "cites": "909", "abstract": "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.", "no": 19}, {"url": "https://arxiv.org/abs/1412.7449", "title": "Grammar as a Foreign Language", "cites": "895", "abstract": "Syntactic constituency parsing is a fundamental problem in natural language\nprocessing and has been the subject of intensive research and engineering for\ndecades. As a result, the most accurate parsers are domain specific, complex,\nand inefficient. In this paper we show that the domain agnostic\nattention-enhanced sequence-to-sequence model achieves state-of-the-art results\non the most widely used syntactic constituency parsing dataset, when trained on\na large synthetic corpus that was annotated using existing parsers. It also\nmatches the performance of standard parsers when trained only on a small\nhuman-annotated dataset, which shows that this model is highly data-efficient,\nin contrast to sequence-to-sequence models without the attention mechanism. Our\nparser is also fast, processing over a hundred sentences per second with an\nunoptimized CPU implementation.", "no": 20}, {"url": "https://arxiv.org/abs/1412.4729", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural\n  Networks", "cites": "886", "abstract": "Solving the visual symbol grounding problem has long been a goal of\nartificial intelligence. The field appears to be advancing closer to this goal\nwith recent breakthroughs in deep learning for natural language grounding in\nstatic images. In this paper, we propose to translate videos directly to\nsentences using a unified deep neural network with both convolutional and\nrecurrent structure. Described video datasets are scarce, and most existing\nmethods have been applied to toy domains with a small vocabulary of possible\nwords. By transferring knowledge from 1.2M+ images with category labels and\n100,000+ images with captions, our method is able to create sentence\ndescriptions of open-domain videos with large vocabularies. We compare our\napproach with recent work using language generation metrics, subject, verb, and\nobject prediction accuracy, and a human evaluation.", "no": 21}, {"url": "https://arxiv.org/abs/1406.5679", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "cites": "872", "abstract": "We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit.", "no": 22}, {"url": "https://arxiv.org/abs/1412.1058", "title": "Effective Use of Word Order for Text Categorization with Convolutional\n  Neural Networks", "cites": "859", "abstract": "Convolutional neural network (CNN) is a neural network that can make use of\nthe internal structure of data such as the 2D structure of image data. This\npaper studies CNN on text categorization to exploit the 1D structure (namely,\nword order) of text data for accurate prediction. Instead of using\nlow-dimensional word vectors as input as is often done, we directly apply CNN\nto high-dimensional text data, which leads to directly learning embedding of\nsmall text regions for use in classification. In addition to a straightforward\nadaptation of CNN from image to text, a simple but new variation which employs\nbag-of-word conversion in the convolution layer is proposed. An extension to\ncombine multiple convolution layers is also explored for higher accuracy. The\nexperiments demonstrate the effectiveness of our approach in comparison with\nstate-of-the-art methods.", "no": 23}, {"url": "https://arxiv.org/abs/1410.8206", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "cites": "738", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task.", "no": 24}, {"url": "https://arxiv.org/abs/1411.2738", "title": "word2vec Parameter Learning Explained", "cites": "720", "abstract": "The word2vec model and application by Mikolov et al. have attracted a great\namount of attention in recent two years. The vector representations of words\nlearned by word2vec models have been shown to carry semantic meanings and are\nuseful in various NLP tasks. As an increasing number of researchers would like\nto experiment with word2vec or similar techniques, I notice that there lacks a\nmaterial that comprehensively explains the parameter learning process of word\nembedding models in details, thus preventing researchers that are non-experts\nin neural networks from understanding the working mechanism of such models.\n  This note provides detailed derivations and explanations of the parameter\nupdate equations of the word2vec models, including the original continuous\nbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization\ntechniques, including hierarchical softmax and negative sampling. Intuitive\ninterpretations of the gradient equations are also provided alongside\nmathematical derivations.\n  In the appendix, a review on the basics of neuron networks and\nbackpropagation is provided. I also created an interactive demo, wevi, to\nfacilitate the intuitive understanding of the model.", "no": 25}, {"url": "https://arxiv.org/abs/1406.3676", "title": "Question Answering with Subgraph Embeddings", "cites": "674", "abstract": "This paper presents a system which learns to answer questions on a broad\nrange of topics from a knowledge base using few hand-crafted features. Our\nmodel learns low-dimensional embeddings of words and knowledge base\nconstituents; these representations are used to score natural language\nquestions against candidate answers. Training our system using pairs of\nquestions and structured representations of their answers, and pairs of\nquestion paraphrases, yields competitive results on a competitive benchmark of\nthe literature.", "no": 26}, {"url": "https://arxiv.org/abs/1410.0210", "title": "A Multi-World Approach to Question Answering about Real-World Scenes\n  based on Uncertain Input", "cites": "638", "abstract": "We propose a method for automatically answering questions about images by\nbringing together recent advances from natural language processing and computer\nvision. We combine discrete reasoning with uncertain predictions by a\nmulti-world approach that represents uncertainty about the perceived world in a\nbayesian framework. Our approach can handle human questions of high complexity\nabout realistic scenes and replies with range of answer like counts, object\nclasses, instances and lists of them. The system is directly trained from\nquestion-answer pairs. We establish a first benchmark for this task that can be\nseen as a modern attempt at a visual turing test.", "no": 27}, {"url": "https://arxiv.org/abs/1411.3315", "title": "Statistically Significant Detection of Linguistic Change", "cites": "435", "abstract": "We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium.", "no": 28}, {"url": "https://arxiv.org/abs/1401.5697", "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "cites": "420", "abstract": "Adequate representation of natural language semantics requires access to vast\namounts of common sense and domain-specific world knowledge. Prior work in the\nfield was based on purely statistical techniques that did not make use of\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\nor on huge manual efforts such as the CYC project. Here we propose a novel\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\ninterpretation of unrestricted natural language texts. Our method represents\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\nlargest encyclopedia in existence. We explicitly represent the meaning of any\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\nmethod on text categorization and on computing the degree of semantic\nrelatedness between fragments of natural language text. Using ESA results in\nsignificant improvements over the previous state of the art in both tasks.\nImportantly, due to the use of natural concepts, the ESA model is easy to\nexplain to human users.", "no": 29}, {"url": "https://arxiv.org/abs/1412.1632", "title": "Deep Learning for Answer Sentence Selection", "cites": "402", "abstract": "Answer sentence selection is the task of identifying sentences that contain\nthe answer to a given question. This is an important problem in its own right\nas well as in the larger context of open domain question answering. We propose\na novel approach to solving this task via means of distributed representations,\nand learn to match questions with answers by considering their semantic\nencoding. This contrasts prior work on this task, which typically relies on\nclassifiers with large numbers of hand-crafted syntactic and semantic features\nand various external resources. Our approach does not require any feature\nengineering nor does it involve specialist linguistic data, making this model\neasily applicable to a wide range of domains and languages. Experimental\nresults on a standard benchmark dataset from TREC demonstrate that---despite\nits simplicity---our model matches state of the art performance on the answer\nsentence selection task.", "no": 30}, {"url": "https://arxiv.org/abs/1401.4994", "title": "A Review of Verbal and Non-Verbal Human-Robot Interactive Communication", "cites": "394", "abstract": "In this paper, an overview of human-robot interactive communication is\npresented, covering verbal as well as non-verbal aspects of human-robot\ninteraction. Following a historical introduction, and motivation towards fluid\nhuman-robot communication, ten desiderata are proposed, which provide an\norganizational axis both of recent as well as of future research on human-robot\ncommunication. Then, the ten desiderata are examined in detail, culminating to\na unifying discussion, and a forward-looking conclusion.", "no": 31}, {"url": "https://arxiv.org/abs/1406.0032", "title": "Comparing and Combining Sentiment Analysis Methods", "cites": "390", "abstract": "Several messages express opinions about events, products, and services,\npolitical views or even their author's emotional state and mood. Sentiment\nanalysis has been used in several applications including analysis of the\nrepercussions of events in social networks, analysis of opinions about products\nand services, and simply to better understand aspects of social communication\nin Online Social Networks (OSNs). There are multiple methods for measuring\nsentiments, including lexical-based approaches and supervised machine learning\nmethods. Despite the wide use and popularity of some methods, it is unclear\nwhich method is better for identifying the polarity (i.e., positive or\nnegative) of a message as the current literature does not provide a method of\ncomparison among existing methods. Such a comparison is crucial for\nunderstanding the potential limitations, advantages, and disadvantages of\npopular methods in analyzing the content of OSNs messages. Our study aims at\nfilling this gap by presenting comparisons of eight popular sentiment analysis\nmethods in terms of coverage (i.e., the fraction of messages whose sentiment is\nidentified) and agreement (i.e., the fraction of identified sentiments that are\nin tune with ground truth). We develop a new method that combines existing\napproaches, providing the best coverage results and competitive agreement. We\nalso present a free Web service called iFeel, which provides an open API for\naccessing and comparing results across different sentiment methods for a given\ntext.", "no": 32}, {"url": "https://arxiv.org/abs/1410.2455", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word\n  Alignments", "cites": "390", "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data. This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data.", "no": 33}, {"url": "https://arxiv.org/abs/1410.1090", "title": "Explain Images with Multimodal Recurrent Neural Networks", "cites": "364", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel sentence descriptions to explain the content of images. It\ndirectly models the probability distribution of generating a word given\nprevious words and the image. Image descriptions are generated by sampling from\nthis distribution. The model consists of two sub-networks: a deep recurrent\nneural network for sentences and a deep convolutional network for images. These\ntwo sub-networks interact with each other in a multimodal layer to form the\nwhole m-RNN model. The effectiveness of our model is validated on three\nbenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model\noutperforms the state-of-the-art generative method. In addition, the m-RNN\nmodel can be applied to retrieval tasks for retrieving images or sentences, and\nachieves significant performance improvement over the state-of-the-art methods\nwhich directly optimize the ranking objective function for retrieval.", "no": 34}, {"url": "https://arxiv.org/abs/1412.6568", "title": "Improving zero-shot learning by mitigating the hubness problem", "cites": "362", "abstract": "The zero-shot paradigm exploits vector-based word representations extracted\nfrom text corpora with unsupervised methods to learn general mapping functions\nfrom other feature spaces onto word space, where the words associated to the\nnearest neighbours of the mapped vectors are used as their linguistic labels.\nWe show that the neighbourhoods of the mapped elements are strongly polluted by\nhubs, vectors that tend to be near a high proportion of items, pushing their\ncorrect labels down the neighbour list. After illustrating the problem\nempirically, we propose a simple method to correct it by taking the proximity\ndistribution of potential neighbours across many mapped vectors into account.\nWe show that this correction leads to consistent improvements in realistic\nzero-shot experiments in the cross-lingual, image labeling and image retrieval\ndomains.", "no": 35}, {"url": "https://arxiv.org/abs/1410.7182", "title": "Analysis of Named Entity Recognition and Linking for Tweets", "cites": "356", "abstract": "Applying natural language processing for mining and intelligent information\naccess to tweets (a form of microblog) is a challenging, emerging research\narea. Unlike carefully authored news text and other longer content, tweets pose\na number of new challenges, due to their short, noisy, context-dependent, and\ndynamic nature. Information extraction from tweets is typically performed in a\npipeline, comprising consecutive stages of language identification,\ntokenisation, part-of-speech tagging, named entity recognition and entity\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\nnamed entity recognition and disambiguation, investigating how robust a number\nof state-of-the-art systems are on such noisy texts, what the main sources of\nerror are, and which problems should be further investigated to improve the\nstate of the art.", "no": 36}, {"url": "https://arxiv.org/abs/1412.6623", "title": "Word Representations via Gaussian Embedding", "cites": "347", "abstract": "Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.", "no": 37}, {"url": "https://arxiv.org/abs/1402.1454", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "cites": "334", "abstract": "Cross-language learning allows us to use training data from one language to\nbuild models for a different language. Many approaches to bilingual learning\nrequire that we have word-level alignment of sentences from parallel corpora.\nIn this work we explore the use of autoencoder-based methods for cross-language\nlearning of vectorial word representations that are aligned between two\nlanguages, while not relying on word-level alignments. We show that by simply\nlearning to reconstruct the bag-of-words representations of aligned sentences,\nwithin and between languages, we can in fact learn high-quality representations\nand do without word alignments. Since training autoencoders on word\nobservations presents certain computational issues, we propose and compare\ndifferent variations adapted to this setting. We also propose an explicit\ncorrelation maximizing regularizer that leads to significant improvement in the\nperformance. We empirically investigate the success of our approach on the\nproblem of cross-language test classification, where a classifier trained on a\ngiven language (e.g., English) must learn to generalize to a different language\n(e.g., German). These experiments demonstrate that our approaches are\ncompetitive with the state-of-the-art, achieving up to 10-14 percentage point\nimprovements over the best reported results on this task.", "no": 38}, {"url": "https://arxiv.org/abs/1404.4326", "title": "Open Question Answering with Weakly Supervised Embedding Models", "cites": "321", "abstract": "Building computers able to answer questions on any subject is a long standing\ngoal of artificial intelligence. Promising progress has recently been achieved\nby methods that learn to map questions to logical forms or database queries.\nSuch approaches can be effective but at the cost of either large amounts of\nhuman-labeled data or by defining lexicons and grammars tailored by\npractitioners. In this paper, we instead take the radical approach of learning\nto map questions to vectorial feature representations. By mapping answers into\nthe same space one can query any knowledge base independent of its schema,\nwithout requiring any grammar or lexicon. Our method is trained with a new\noptimization procedure combining stochastic gradient descent followed by a\nfine-tuning step using the weak supervision provided by blending automatically\nand collaboratively generated resources. We empirically demonstrate that our\nmodel can capture meaningful signals from its noisy supervision leading to\nmajor improvements over paralex, the only existing method able to be trained on\nsimilar weakly labeled data.", "no": 39}, {"url": "https://arxiv.org/abs/1404.4641", "title": "Multilingual Models for Compositional Distributed Semantics", "cites": "317", "abstract": "We present a novel technique for learning semantic representations, which\nextends the distributional hypothesis to multilingual data and joint-space\nembeddings. Our models leverage parallel data and learn to strongly align the\nembeddings of semantically equivalent sentences, while maintaining sufficient\ndistance between those of dissimilar sentences. The models do not rely on word\nalignments or any syntactic information and are successfully applied to a\nnumber of diverse languages. We extend our approach to learn semantic\nrepresentations at the document level, too. We evaluate these models on two\ncross-lingual document classification tasks, outperforming the prior state of\nthe art. Through qualitative analysis and the study of pivoting effects we\ndemonstrate that our representations are semantically plausible and can capture\nsemantic relationships across languages without parallel data.", "no": 40}, {"url": "https://arxiv.org/abs/1405.3515", "title": "Temporal Analysis of Language through Neural Language Models", "cites": "317", "abstract": "We provide a method for automatically detecting change in language across\ntime through a chronologically trained neural language model. We train the\nmodel on the Google Books Ngram corpus to obtain word vector representations\nspecific to each year, and identify words that have changed significantly from\n1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having\nchanged during that time period. The model simultaneously identifies the\nspecific years during which such words underwent change.", "no": 41}, {"url": "https://arxiv.org/abs/1404.5367", "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "cites": "307", "abstract": "Most state-of-the-art approaches for named-entity recognition (NER) use semi\nsupervised information in the form of word clusters and lexicons. Recently\nneural network-based language models have been explored, as they as a byproduct\ngenerate highly informative vector representations for words, known as word\nembeddings. In this paper we present two contributions: a new form of learning\nword embeddings that can leverage information from relevant lexicons to improve\nthe representations, and the first system to use neural word embeddings to\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\nCoNLL 2003---significantly better than any previous system trained on public\ndata, and matching a system employing massive private industrial query-log\ndata.", "no": 42}, {"url": "https://arxiv.org/abs/1406.3855", "title": "Human language reveals a universal positivity bias", "cites": "302", "abstract": "Using human evaluation of 100,000 words spread across 24 corpora in 10\nlanguages diverse in origin and culture, we present evidence of a deep imprint\nof human sociality in language, observing that (1) the words of natural human\nlanguage possess a universal positivity bias; (2) the estimated emotional\ncontent of words is consistent between languages under translation; and (3)\nthis positivity bias is strongly independent of frequency of word usage.\nAlongside these general regularities, we describe inter-language variations in\nthe emotional spectrum of languages which allow us to rank corpora. We also\nshow how our word evaluations can be used to construct physical-like\ninstruments for both real-time and offline measurement of the emotional content\nof large-scale texts.", "no": 43}, {"url": "https://arxiv.org/abs/1410.4281", "title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks\n  for Large Vocabulary Speech Recognition", "cites": "290", "abstract": "Long short-term memory (LSTM) based acoustic modeling methods have recently\nbeen shown to give state-of-the-art performance on some speech recognition\ntasks. To achieve a further performance improvement, in this research, deep\nextensions on LSTM are investigated considering that deep hierarchical model\nhas turned out to be more efficient than a shallow one. Motivated by previous\nresearch on constructing deep recurrent neural networks (RNNs), alternative\ndeep LSTM architectures are proposed and empirically evaluated on a large\nvocabulary conversational telephone speech recognition task. Meanwhile,\nregarding to multi-GPU devices, the training process for LSTM networks is\nintroduced and discussed. Experimental results demonstrate that the deep LSTM\nnetworks benefit from the depth and yield the state-of-the-art performance on\nthis task.", "no": 44}, {"url": "https://arxiv.org/abs/1408.6988", "title": "An Information Retrieval Approach to Short Text Conversation", "cites": "243", "abstract": "Human computer conversation is regarded as one of the most difficult problems\nin artificial intelligence. In this paper, we address one of its key\nsub-problems, referred to as short text conversation, in which given a message\nfrom human, the computer returns a reasonable response to the message. We\nleverage the vast amount of short conversation data available on social media\nto study the issue. We propose formalizing short text conversation as a search\nproblem at the first step, and employing state-of-the-art information retrieval\n(IR) techniques to carry out the task. We investigate the significance as well\nas the limitation of the IR approach. Our experiments demonstrate that the\nretrieval-based model can make the system behave rather \"intelligently\", when\ncombined with a huge repository of conversation data from social media.", "no": 45}, {"url": "https://arxiv.org/abs/1405.4273", "title": "Compositional Morphology for Word Representations and Language Modelling", "cites": "236", "abstract": "This paper presents a scalable method for integrating compositional\nmorphological representations into a vector-based probabilistic language model.\nOur approach is evaluated in the context of log-bilinear language models,\nrendered suitably efficient for implementation inside a machine translation\ndecoder by factoring the vocabulary. We perform both intrinsic and extrinsic\nevaluations, presenting results on a range of languages which demonstrate that\nour model learns morphological representations that both perform well on word\nsimilarity tasks and lead to substantial reductions in perplexity. When used\nfor translation into morphologically rich languages with large vocabularies,\nour models obtain improvements of up to 1.2 BLEU points relative to a baseline\nsystem using back-off n-gram models.", "no": 46}, {"url": "https://arxiv.org/abs/1411.6718", "title": "LABR: A Large Scale Arabic Sentiment Analysis Benchmark", "cites": "227", "abstract": "We introduce LABR, the largest sentiment analysis dataset to-date for the\nArabic language. It consists of over 63,000 book reviews, each rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset, and present its\nstatistics. We explore using the dataset for two tasks: (1) sentiment polarity\nclassification; and (2) ratings classification. Moreover, we provide standard\nsplits of the dataset into training, validation and testing, for both polarity\nand ratings classification, in both balanced and unbalanced settings. We extend\nour previous work by performing a comprehensive analysis on the dataset. In\nparticular, we perform an extended survey of the different classifiers\ntypically used for the sentiment polarity classification problem. We also\nconstruct a sentiment lexicon from the dataset that contains both single and\ncompound sentiment words and we explore its effectiveness. We make the dataset\nand experimental details publicly available.", "no": 47}, {"url": "https://arxiv.org/abs/1403.2345", "title": "Home Location Identification of Twitter Users", "cites": "206", "abstract": "We present a new algorithm for inferring the home location of Twitter users\nat different granularities, including city, state, time zone or geographic\nregion, using the content of users tweets and their tweeting behavior. Unlike\nexisting approaches, our algorithm uses an ensemble of statistical and\nheuristic classifiers to predict locations and makes use of a geographic\ngazetteer dictionary to identify place-name entities. We find that a\nhierarchical classification approach, where time zone, state or geographic\nregion is predicted first and city is predicted next, can improve prediction\naccuracy. We have also analyzed movement variations of Twitter users, built a\nclassifier to predict whether a user was travelling in a certain period of time\nand use that to further improve the location detection accuracy. Experimental\nevidence suggests that our algorithm works well in practice and outperforms the\nbest existing algorithms for predicting the home location of Twitter users.", "no": 48}, {"url": "https://arxiv.org/abs/1409.2450", "title": "Exploiting Social Network Structure for Person-to-Person Sentiment\n  Analysis", "cites": "201", "abstract": "Person-to-person evaluations are prevalent in all kinds of discourse and\nimportant for establishing reputations, building social bonds, and shaping\npublic opinion. Such evaluations can be analyzed separately using signed social\nnetworks and textual sentiment analysis, but this misses the rich interactions\nbetween language and social context. To capture such interactions, we develop a\nmodel that predicts individual A's opinion of individual B by synthesizing\ninformation from the signed social network in which A and B are embedded with\nsentiment analysis of the evaluative texts relating A to B. We prove that this\nproblem is NP-hard but can be relaxed to an efficiently solvable hinge-loss\nMarkov random field, and we show that this implementation outperforms text-only\nand network-only versions in two very different datasets involving\ncommunity-level decision-making: the Wikipedia Requests for Adminship corpus\nand the Convote U.S. Congressional speech corpus.", "no": 49}, {"url": "https://arxiv.org/abs/1401.5694", "title": "Cross-lingual Annotation Projection for Semantic Roles", "cites": "200", "abstract": "This article considers the task of automatically inducing role-semantic\nannotations in the FrameNet paradigm for new languages. We propose a general\nframework that is based on annotation projection, phrased as a graph\noptimization problem. It is relatively inexpensive and has the potential to\nreduce the human effort involved in creating role-semantic resources. Within\nthis framework, we present projection models that exploit lexical and syntactic\ninformation. We provide an experimental evaluation on an English-German\nparallel corpus which demonstrates the feasibility of inducing high-precision\nGerman semantic role annotation both for manually and automatically annotated\nEnglish data.", "no": 50}, {"url": "https://arxiv.org/abs/1406.6312", "title": "Scalable Topical Phrase Mining from Text Corpora", "cites": "200", "abstract": "While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles.", "no": 51}, {"url": "https://arxiv.org/abs/1403.6173", "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail", "cites": "199", "abstract": "Humans can easily describe what they see in a coherent way and at varying\nlevel of detail. However, existing approaches for automatic video description\nare mainly focused on single sentence generation and produce descriptions at a\nfixed level of detail. In this paper, we address both of these limitations: for\na variable level of detail we produce coherent multi-sentence descriptions of\ncomplex videos. We follow a two-step approach where we first learn to predict a\nsemantic representation (SR) from video and then generate natural language\ndescriptions from the SR. To produce consistent multi-sentence descriptions, we\nmodel across-sentence consistency at the level of the SR by enforcing a\nconsistent topic. We also contribute both to the visual recognition of objects\nproposing a hand-centric approach as well as to the robust generation of\nsentences using a word lattice. Human judges rate our multi-sentence\ndescriptions as more readable, correct, and relevant than related work. To\nunderstand the difference between more detailed and shorter descriptions, we\ncollect and analyze a video description corpus of three levels of detail.", "no": 52}, {"url": "https://arxiv.org/abs/1405.1438", "title": "The effect of wording on message propagation: Topic- and\n  author-controlled natural experiments on Twitter", "cites": "189", "abstract": "Consider a person trying to spread an important message on a social network.\nHe/she can spend hours trying to craft the message. Does it actually matter?\nWhile there has been extensive prior work looking into predicting popularity of\nsocial-media content, the effect of wording per se has rarely been studied\nsince it is often confounded with the popularity of the author and the topic.\nTo control for these confounding factors, we take advantage of the surprising\nfact that there are many pairs of tweets containing the same url and written by\nthe same user but employing different wording. Given such pairs, we ask: which\nversion attracts more retweets? This turns out to be a more difficult task than\npredicting popular topics. Still, humans can answer this question better than\nchance (but far from perfectly), and the computational methods we develop can\ndo better than both an average human and a strong competing method trained on\nnon-controlled data.", "no": 53}, {"url": "https://arxiv.org/abs/1411.5654", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "cites": "186", "abstract": "In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features.", "no": 54}, {"url": "https://arxiv.org/abs/1401.5390", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "cites": "180", "abstract": "Domain knowledge is crucial for effective performance in autonomous control\nsystems. Typically, human effort is required to encode this knowledge into a\ncontrol algorithm. In this paper, we present an approach to language grounding\nwhich automatically interprets text in the context of a complex control\napplication, such as a game, and uses domain knowledge extracted from the text\nto improve control performance. Both text analysis and control strategies are\nlearned jointly using only a feedback signal inherent to the application. To\neffectively leverage textual information, our method automatically extracts the\ntext segment most relevant to the current game state, and labels it with a\ntask-centric predicate structure. This labeled text is then used to bias an\naction selection policy for the game, guiding it towards promising regions of\nthe action space. We encode our model for text analysis and game playing in a\nmulti-layer neural network, representing linguistic decisions via latent\nvariables in the hidden layers, and game action quality via the output layer.\nOperating within the Monte-Carlo Search framework, we estimate model parameters\nusing feedback from simulated games. We apply our approach to the complex\nstrategy game Civilization II using the official game manual as the text guide.\nOur results show that a linguistically-informed game-playing agent\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\nabsolute improvement and winning over 65% of games when playing against the\nbuilt-in AI of Civilization.", "no": 55}, {"url": "https://arxiv.org/abs/1405.1605", "title": "DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News", "cites": "179", "abstract": "While many lexica annotated with words polarity are available for sentiment\nanalysis, very few tackle the harder task of emotion analysis and are usually\nquite limited in coverage. In this paper, we present a novel approach for\nextracting - in a totally automated way - a high-coverage and high-precision\nlexicon of roughly 37 thousand terms annotated with emotion scores, called\nDepecheMood. Our approach exploits in an original way 'crowd-sourced' affective\nannotation implicitly provided by readers of news articles from rappler.com. By\nproviding new state-of-the-art performances in unsupervised settings for\nregression and classification tasks, even using a na\\\"{\\i}ve approach, our\nexperiments show the beneficial impact of harvesting social media data for\naffective lexicon building.", "no": 56}, {"url": "https://arxiv.org/abs/1404.4606", "title": "How Many Topics? Stability Analysis for Topic Models", "cites": "175", "abstract": "Topic modeling refers to the task of discovering the underlying thematic\nstructure in a text corpus, where the output is commonly presented as a report\nof the top terms appearing in each topic. Despite the diversity of topic\nmodeling algorithms that have been proposed, a common challenge in successfully\napplying these techniques is the selection of an appropriate number of topics\nfor a given corpus. Choosing too few topics will produce results that are\noverly broad, while choosing too many will result in the \"over-clustering\" of a\ncorpus into many small, highly-similar topics. In this paper, we propose a\nterm-centric stability analysis strategy to address this issue, the idea being\nthat a model with an appropriate number of topics will be more robust to\nperturbations in the data. Using a topic modeling approach based on matrix\nfactorization, evaluations performed on a range of corpora show that this\nstrategy can successfully guide the model selection process.", "no": 57}, {"url": "https://arxiv.org/abs/1401.5693", "title": "Sentence Compression as Tree Transduction", "cites": "174", "abstract": "This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model.", "no": 58}, {"url": "https://arxiv.org/abs/1401.5699", "title": "Text Relatedness Based on a Word Thesaurus", "cites": "167", "abstract": "The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches.", "no": 59}, {"url": "https://arxiv.org/abs/1412.5404", "title": "Word Network Topic Model: A Simple but General Solution for Short and\n  Imbalanced Texts", "cites": "167", "abstract": "The short text has been the prevalent format for information of Internet in\nrecent decades, especially with the development of online social media, whose\nmillions of users generate a vast number of short messages everyday. Although\nsophisticated signals delivered by the short text make it a promising source\nfor topic modeling, its extreme sparsity and imbalance brings unprecedented\nchallenges to conventional topic models like LDA and its variants. Aiming at\npresenting a simple but general solution for topic modeling in short texts, we\npresent a word co-occurrence network based model named WNTM to tackle the\nsparsity and imbalance simultaneously. Different from previous approaches, WNTM\nmodels the distribution over topics for each word instead of learning topics\nfor each document, which successfully enhance the semantic density of data\nspace without importing too much time or space complexity. Meanwhile, the rich\ncontextual information preserved in the word-word space also guarantees its\nsensitivity in identifying rare topics with convincing quality. Furthermore,\nemploying the same Gibbs sampling with LDA makes WNTM easily to be extended to\nvarious application scenarios. Extensive validations on both short and normal\ntexts testify the outperformance of WNTM as compared to baseline methods. And\nfinally we also demonstrate its potential in precisely discovering newly\nemerging topics or unexpected events in Weibo at pretty early stages.", "no": 60}, {"url": "https://arxiv.org/abs/1408.2873", "title": "First-Pass Large Vocabulary Continuous Speech Recognition using\n  Bi-Directional Recurrent DNNs", "cites": "161", "abstract": "We present a method to perform first-pass large vocabulary continuous speech\nrecognition using only a neural network and language model. Deep neural network\nacoustic models are now commonplace in HMM-based speech recognition systems,\nbut building such systems is a complex, domain-specific task. Recent work\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\nby directly predicting transcript text from audio. This paper extends this\napproach in two ways. First, we demonstrate that a straightforward recurrent\nneural network architecture can achieve a high level of accuracy. Second, we\npropose and evaluate a modified prefix-search decoding algorithm. This approach\nto decoding enables first-pass speech recognition with a language model,\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\nword error rates, and the importance of bi-directional network recurrence.", "no": 61}, {"url": "https://arxiv.org/abs/1410.3791", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "cites": "154", "abstract": "The increasing diversity of languages used on the web introduces a new level\nof complexity to Information Retrieval (IR) systems. We can no longer assume\nthat textual content is written in one language or even the same language\nfamily. In this paper, we demonstrate how to build massive multilingual\nannotators with minimal human expertise and intervention. We describe a system\nthat builds Named Entity Recognition (NER) annotators for 40 major languages\nusing Wikipedia and Freebase. Our approach does not require NER human annotated\ndatasets or language specific resources like treebanks, parallel corpora, and\northographic rules. The novelty of approach lies therein - using only language\nagnostic techniques, while achieving competitive performance.\n  Our method learns distributed word representations (word embeddings) which\nencode semantic and syntactic features of words in each language. Then, we\nautomatically generate datasets from Wikipedia link structure and Freebase\nattributes. Finally, we apply two preprocessing stages (oversampling and exact\nsurface form matching) which do not require any linguistic expertise.\n  Our evaluation is two fold: First, we demonstrate the system performance on\nhuman annotated datasets. Second, for languages where no gold-standard\nbenchmarks are available, we propose a new method, distant evaluation, based on\nstatistical machine translation.", "no": 62}, {"url": "https://arxiv.org/abs/1403.1451", "title": "Real-Time Classification of Twitter Trends", "cites": "146", "abstract": "Social media users give rise to social trends as they share about common\ninterests, which can be triggered by different reasons. In this work, we\nexplore the types of triggers that spark trends on Twitter, introducing a\ntypology with following four types: 'news', 'ongoing events', 'memes', and\n'commemoratives'. While previous research has analyzed trending topics in a\nlong term, we look at the earliest tweets that produce a trend, with the aim of\ncategorizing trends early on. This would allow to provide a filtered subset of\ntrends to end users. We analyze and experiment with a set of straightforward\nlanguage-independent features based on the social spread of trends to\ncategorize them into the introduced typology. Our method provides an efficient\nway to accurately categorize trending topics without need of external data,\nenabling news organizations to discover breaking news in real-time, or to\nquickly identify viral memes that might enrich marketing decisions, among\nothers. The analysis of social features also reveals patterns associated with\neach type of trend, such as tweets about ongoing events being shorter as many\nwere likely sent from mobile devices, or memes having more retweets originating\nfrom a few trend-setters.", "no": 63}, {"url": "https://arxiv.org/abs/1408.6418", "title": "Video In Sentences Out", "cites": "142", "abstract": "We present a system that produces sentential descriptions of video: who did\nwhat to whom, and where and how they did it. Action class is rendered as a\nverb, participant objects as noun phrases, properties of those objects as\nadjectival modifiers in those noun phrases, spatial relations between those\nparticipants as prepositional phrases, and characteristics of the event as\nprepositional-phrase adjuncts and adverbial modifiers. Extracting the\ninformation needed to render these linguistic entities requires an approach to\nevent recognition that recovers object tracks, the trackto-role assignments,\nand changing body posture.", "no": 64}, {"url": "https://arxiv.org/abs/1411.6699", "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for\n  Discourse Relations", "cites": "138", "abstract": "Discourse relations bind smaller linguistic units into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked arguments. A more subtle\nchallenge is that it is not enough to represent the meaning of each argument of\na discourse relation, because the relation may depend on links between\nlower-level components, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted from the\ndistributional representations of the arguments, and also of their coreferent\nentity mentions. The resulting system obtains substantial improvements over the\nprevious state-of-the-art in predicting implicit discourse relations in the\nPenn Discourse Treebank.", "no": 65}, {"url": "https://arxiv.org/abs/1412.5335", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "cites": "137", "abstract": "Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.", "no": 66}, {"url": "https://arxiv.org/abs/1412.1820", "title": "Context-Dependent Fine-Grained Entity Type Tagging", "cites": "130", "abstract": "Entity type tagging is the task of assigning category labels to each mention\nof an entity in a document. While standard systems focus on a small set of\ntypes, recent work (Ling and Weld, 2012) suggests that using a large\nfine-grained label set can lead to dramatic improvements in downstream tasks.\nIn the absence of labeled training data, existing fine-grained tagging systems\nobtain examples automatically, using resolved entities and their types\nextracted from a knowledge base. However, since the appropriate type often\ndepends on context (e.g. Washington could be tagged either as city or\ngovernment), this procedure can result in spurious labels, leading to poorer\ngeneralization. We propose the task of context-dependent fine type tagging,\nwhere the set of acceptable labels for a mention is restricted to only those\ndeducible from the local context (e.g. sentence or document). We introduce new\nresources for this task: 12,017 mentions annotated with their context-dependent\nfine types, and we provide baseline experimental results on this data.", "no": 67}, {"url": "https://arxiv.org/abs/1401.3457", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "cites": "126", "abstract": "This paper presents a new method for inferring the semantic properties of\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\nbecoming increasingly abundant due to the recent dramatic growth in\nsemi-structured, user-generated online content. One especially relevant domain\nis product reviews, which are often annotated by their authors with pros/cons\nkeyphrases such as a real bargain or good value. These annotations are\nrepresentative of the underlying semantic properties; however, unlike expert\nannotations, they are noisy: lay authors may use different labels to denote the\nsame property, and some labels may be missing. To learn using such noisy\nannotations, we find a hidden paraphrase structure which clusters the\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\nreview texts, enabling the system to predict the properties of unannotated\ndocuments and to effectively aggregate the semantic properties of multiple\nreviews. Our approach is implemented as a hierarchical Bayesian model with\njoint inference. We find that joint inference increases the robustness of the\nkeyphrase clustering and encourages the latent topics to correlate with\nsemantically meaningful properties. Multiple evaluations demonstrate that our\nmodel substantially outperforms alternative approaches for summarizing single\nand multiple documents into a set of semantically salient keyphrases.", "no": 68}, {"url": "https://arxiv.org/abs/1401.5696", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the\n  Web", "cites": "126", "abstract": "The task of identifying synonymous relations and objects, or synonym\nresolution, is critical for high-quality information extraction. This paper\ninvestigates synonym resolution in the context of unsupervised information\nextraction, where neither hand-tagged training examples nor domain knowledge is\navailable. The paper presents a scalable, fully-implemented system that runs in\nO(KN log N) time in the number of extractions, N, and the maximum number of\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\nrelational model for predicting whether two strings are co-referential based on\nthe similarity of the assertions containing them. On a set of two million\nassertions extracted from the Web, Resolver resolves objects with 78% precision\nand 68% recall, and resolves relations with 90% precision and 35% recall.\nSeveral variations of resolvers probabilistic model are explored, and\nexperiments demonstrate that under appropriate conditions these variations can\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\npolysemous names with 97% precision and 95% recall on a data set from the TREC\ncorpus.", "no": 69}, {"url": "https://arxiv.org/abs/1401.6876", "title": "Improving Statistical Machine Translation for a Resource-Poor Language\n  Using Related Resource-Rich Languages", "cites": "123", "abstract": "We propose a novel language-independent approach for improving machine\ntranslation for resource-poor languages by exploiting their similarity to\nresource-rich ones. More precisely, we improve the translation from a\nresource-poor source language X_1 into a resource-rich language Y given a\nbi-text containing a limited number of parallel sentences for X_1-Y and a\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\nrelated to X_1. This is achieved by taking advantage of the opportunities that\nvocabulary overlap and similarities between the languages X_1 and X_2 in\nspelling, word order, and syntax offer: (1) we improve the word alignments for\nthe resource-poor language, (2) we further augment it with additional\ntranslation options, and (3) we take care of potential spelling differences\nthrough appropriate transliteration. The evaluation for Indonesian- >English\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\nrespectively, which is an improvement over the best rivaling approaches, while\nusing much less additional data. Overall, our method cuts the amount of\nnecessary \"real training data by a factor of 2--5.", "no": 70}, {"url": "https://arxiv.org/abs/1405.6164", "title": "Generating Natural Language Descriptions from OWL Ontologies: the\n  NaturalOWL System", "cites": "123", "abstract": "We present NaturalOWL, a natural language generation system that produces\ntexts describing individuals or classes of OWL ontologies. Unlike simpler OWL\nverbalizers, which typically express a single axiom at a time in controlled,\noften not entirely fluent natural language primarily for the benefit of domain\nexperts, we aim to generate fluent and coherent multi-sentence texts for\nend-users. With a system like NaturalOWL, one can publish information in OWL on\nthe Web, along with automatically produced corresponding texts in multiple\nlanguages, making the information accessible not only to computer programs and\ndomain experts, but also end-users. We discuss the processing stages of\nNaturalOWL, the optional domain-dependent linguistic resources that the system\ncan use at each stage, and why they are useful. We also present trials showing\nthat when the domain-dependent llinguistic resources are available, NaturalOWL\nproduces significantly better texts compared to a simpler verbalizer, and that\nthe resources can be created with relatively light effort.", "no": 71}, {"url": "https://arxiv.org/abs/1405.5208", "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference\n  in Natural Language Processing", "cites": "118", "abstract": "Dual decomposition, and more generally Lagrangian relaxation, is a classical\nmethod for combinatorial optimization; it has recently been applied to several\ninference problems in natural language processing (NLP). This tutorial gives an\noverview of the technique. We describe example algorithms, describe formal\nguarantees for the method, and describe practical issues in implementing the\nalgorithms. While our examples are predominantly drawn from the NLP literature,\nthe material should be of general relevance to inference problems in machine\nlearning. A central theme of this tutorial is that Lagrangian relaxation is\nnaturally applied in conjunction with a broad class of combinatorial\nalgorithms, allowing inference in models that go significantly beyond previous\nwork on Lagrangian relaxation for inference in graphical models.", "no": 72}, {"url": "https://arxiv.org/abs/1408.6179", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional\n  Settings", "cites": "116", "abstract": "We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks.", "no": 73}, {"url": "https://arxiv.org/abs/1412.6815", "title": "Extraction of Salient Sentences from Labelled Documents", "cites": "115", "abstract": "We present a hierarchical convolutional document model with an architecture\ndesigned to support introspection of the document structure. Using this model,\nwe show how to use visualisation techniques from the computer vision literature\nto identify and extract topic-relevant sentences.\n  We also introduce a new scalable evaluation technique for automatic sentence\nextraction systems that avoids the need for time consuming human annotation of\nvalidation data.", "no": 74}, {"url": "https://arxiv.org/abs/1405.3282", "title": "How to Ask for a Favor: A Case Study on the Success of Altruistic\n  Requests", "cites": "114", "abstract": "Requests are at the core of many social media systems such as question &\nanswer sites and online philanthropy communities. While the success of such\nrequests is critical to the success of the community, the factors that lead\ncommunity members to satisfy a request are largely unknown. Success of a\nrequest depends on factors like who is asking, how they are asking, when are\nthey asking, and most critically what is being requested, ranging from small\nfavors to substantial monetary donations. We present a case study of altruistic\nrequests in an online community where all requests ask for the very same\ncontribution and do not offer anything tangible in return, allowing us to\ndisentangle what is requested from textual and social factors. Drawing from\nsocial psychology literature, we extract high-level social features from text\nthat operationalize social relations between recipient and donor and\ndemonstrate that these extracted relations are predictive of success. More\nspecifically, we find that clearly communicating need through the narrative is\nessential and that that linguistic indications of gratitude, evidentiality, and\ngeneralized reciprocity, as well as high status of the asker further increase\nthe likelihood of success. Building on this understanding, we develop a model\nthat can predict the success of unseen requests, significantly improving over\nseveral baselines. We link these findings to research in psychology on helping\nbehavior, providing a basis for further analysis of success in social media\nsystems.", "no": 75}, {"url": "https://arxiv.org/abs/1405.7711", "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn\n  Language", "cites": "113", "abstract": "We present a novel framework for learning to interpret and generate language\nusing only perceptual context as supervision. We demonstrate its capabilities\nby developing a system that learns to sportscast simulated robot soccer games\nin both English and Korean without any language-specific prior knowledge.\nTraining employs only ambiguous supervision consisting of a stream of\ndescriptive textual comments and a sequence of events extracted from the\nsimulation trace. The system simultaneously establishes correspondences between\nindividual comments and the events that they describe while building a\ntranslation model that supports both parsing and generation. We also present a\nnovel algorithm for learning which events are worth describing. Human\nevaluations of the generated commentaries indicate they are of reasonable\nquality and in some cases even on par with those produced by humans for our\nlimited domain.", "no": 76}, {"url": "https://arxiv.org/abs/1404.4714", "title": "Radical-Enhanced Chinese Character Embedding", "cites": "111", "abstract": "We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks.", "no": 77}, {"url": "https://arxiv.org/abs/1402.6792", "title": "Information Evolution in Social Networks", "cites": "110", "abstract": "Social networks readily transmit information, albeit with less than perfect\nfidelity. We present a large-scale measurement of this imperfect information\ncopying mechanism by examining the dissemination and evolution of thousands of\nmemes, collectively replicated hundreds of millions of times in the online\nsocial network Facebook. The information undergoes an evolutionary process that\nexhibits several regularities. A meme's mutation rate characterizes the\npopulation distribution of its variants, in accordance with the Yule process.\nVariants further apart in the diffusion cascade have greater edit distance, as\nwould be expected in an iterative, imperfect replication process. Some text\nsequences can confer a replicative advantage; these sequences are abundant and\ntransfer \"laterally\" between different memes. Subpopulations of the social\nnetwork can preferentially transmit a specific variant of a meme if the variant\nmatches their beliefs or culture. Understanding the mechanism driving change in\ndiffusing information has important implications for how we interpret and\nharness the information that reaches us through our social networks.", "no": 78}, {"url": "https://arxiv.org/abs/1406.7806", "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "cites": "103", "abstract": "Deep neural networks (DNNs) are now a central component of nearly all\nstate-of-the-art speech recognition systems. Building neural network acoustic\nmodels requires several design decisions including network architecture, size,\nand training loss function. This paper offers an empirical investigation on\nwhich aspects of DNN acoustic model design are most important for speech\nrecognition system performance. We report DNN classifier performance and final\nspeech recognizer word error rates, and compare DNNs using several metrics to\nquantify factors influencing differences in task performance. Our first set of\nexperiments use the standard Switchboard benchmark corpus, which contains\napproximately 300 hours of conversational telephone speech. We compare standard\nDNNs to convolutional networks, and present the first experiments using\nlocally-connected, untied neural networks for acoustic modeling. We\nadditionally build systems on a corpus of 2,100 hours of training data by\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\nmore thoroughly examine performance of large DNN models -- with up to ten times\nmore parameters than those typically used in speech recognition systems. Our\nresults suggest that a relatively simple DNN architecture and optimization\ntechnique produces strong results. These findings, along with previous work,\nhelp establish a set of best practices for building DNN hybrid speech\nrecognition systems with maximum likelihood training. Our experiments in DNN\noptimization additionally serve as a case study for training DNNs with\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\ngenerally.", "no": 79}, {"url": "https://arxiv.org/abs/1404.5278", "title": "The Frobenius anatomy of word meanings I: subject and object relative\n  pronouns", "cites": "100", "abstract": "This paper develops a compositional vector-based semantics of subject and\nobject relative pronouns within a categorical framework. Frobenius algebras are\nused to formalise the operations required to model the semantics of relative\npronouns, including passing information between the relative clause and the\nmodified noun phrase, as well as copying, combining, and discarding parts of\nthe relative clause. We develop two instantiations of the abstract semantics,\none based on a truth-theoretic approach and one based on corpus statistics.", "no": 80}, {"url": "https://arxiv.org/abs/1406.3830", "title": "Modelling, Visualising and Summarising Documents with a Single\n  Convolutional Neural Network", "cites": "100", "abstract": "Capturing the compositional process which maps the meaning of words to that\nof documents is a central challenge for researchers in Natural Language\nProcessing and Information Retrieval. We introduce a model that is able to\nrepresent the meaning of documents by embedding them in a low dimensional\nvector space, while preserving distinctions of word and sentence order crucial\nfor capturing nuanced semantics. Our model is based on an extended Dynamic\nConvolution Neural Network, which learns convolution filters at both the\nsentence and document level, hierarchically learning to capture and compose low\nlevel lexical features into high level semantic concepts. We demonstrate the\neffectiveness of this model on a range of document modelling tasks, achieving\nstrong results with no feature engineering and with a more compact model.\nInspired by recent advances in visualising deep convolution networks for\ncomputer vision, we present a novel visualisation technique for our document\nnetworks which not only provides insight into their learning process, but also\ncan be interpreted to produce a compelling automatic summarisation system for\ntexts.", "no": 81}, {"url": "https://arxiv.org/abs/1411.4925", "title": "Linguistic Descriptions for Automatic Generation of Textual Short-Term\n  Weather Forecasts on Real Prediction Data", "cites": "99", "abstract": "We present in this paper an application which automatically generates textual\nshort-term weather forecasts for every municipality in Galicia (NW Spain),\nusing the real data provided by the Galician Meteorology Agency (MeteoGalicia).\nThis solution combines in an innovative way computing with perceptions\ntechniques and strategies for linguistic description of data together with a\nnatural language generation (NLG) system. The application, named GALiWeather,\nextracts relevant information from weather forecast input data and encodes it\ninto intermediate descriptions using linguistic variables and temporal\nreferences. These descriptions are later translated into natural language texts\nby the natural language generation system. The obtained forecast results have\nbeen thoroughly validated by an expert meteorologist from MeteoGalicia using a\nquality assessment methodology which covers two key dimensions of a text: the\naccuracy of its content and the correctness of its form. Following this\nvalidation GALiWeather will be released as a real service offering custom\nforecasts for a wide public.", "no": 82}, {"url": "https://arxiv.org/abs/1405.4392", "title": "That's sick dude!: Automatic identification of word sense change across\n  different timescales", "cites": "93", "abstract": "In this paper, we propose an unsupervised method to identify noun sense\nchanges based on rigorous analysis of time-varying text data available in the\nform of millions of digitized books. We construct distributional thesauri based\nnetworks from data at different time points and cluster each of them separately\nto obtain word-centric sense clusters corresponding to the different time\npoints. Subsequently, we compare these sense clusters of two different time\npoints to find if (i) there is birth of a new sense or (ii) if an older sense\nhas got split into more than one sense or (iii) if a newer sense has been\nformed from the joining of older senses or (iv) if a particular sense has died.\nWe conduct a thorough evaluation of the proposed methodology both manually as\nwell as through comparison with WordNet. Manual evaluation indicates that the\nalgorithm could correctly identify 60.4% birth cases from a set of 48 randomly\npicked samples and 57% split/join cases from a set of 21 randomly picked\nsamples. Remarkably, in 44% cases the birth of a novel sense is attested by\nWordNet, while in 46% cases and 43% cases split and join are respectively\nconfirmed by WordNet. Our approach can be applied for lexicography, as well as\nfor applications like word sense disambiguation or semantic search.", "no": 83}, {"url": "https://arxiv.org/abs/1402.0556", "title": "Generating Extractive Summaries of Scientific Paradigms", "cites": "91", "abstract": "Researchers and scientists increasingly find themselves in the position of\nhaving to quickly understand large amounts of technical material. Our goal is\nto effectively serve this need by using bibliometric text mining and\nsummarization techniques to generate summaries of scientific literature. We\nshow how we can use citations to produce automatically generated, readily\nconsumable, technical extractive summaries. We first propose C-LexRank, a model\nfor summarizing single scientific articles based on citations, which employs\ncommunity detection and extracts salient information-rich sentences. Next, we\nfurther extend our experiments to summarize a set of papers, which cover the\nsame scientific topic. We generate extractive summaries of a set of Question\nAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and their\ncitation sentences and show that citations have unique information amenable to\ncreating a summary.", "no": 84}, {"url": "https://arxiv.org/abs/1403.3142", "title": "ARSENAL: Automatic Requirements Specification Extraction from Natural\n  Language", "cites": "89", "abstract": "Requirements are informal and semi-formal descriptions of the expected\nbehavior of a complex system from the viewpoints of its stakeholders\n(customers, users, operators, designers, and engineers). However, for the\npurpose of design, testing, and verification for critical systems, we can\ntransform requirements into formal models that can be analyzed automatically.\nARSENAL is a framework and methodology for systematically transforming natural\nlanguage (NL) requirements into analyzable formal models and logic\nspecifications. These models can be analyzed for consistency and\nimplementability. The ARSENAL methodology is specialized to individual domains,\nbut the approach is general enough to be adapted to new domains.", "no": 85}, {"url": "https://arxiv.org/abs/1407.7094", "title": "Crowdsourcing Dialect Characterization through Twitter", "cites": "88", "abstract": "We perform a large-scale analysis of language diatopic variation using\ngeotagged microblogging datasets. By collecting all Twitter messages written in\nSpanish over more than two years, we build a corpus from which a carefully\nselected list of concepts allows us to characterize Spanish varieties on a\nglobal scale. A cluster analysis proves the existence of well defined\nmacroregions sharing common lexical properties. Remarkably enough, we find that\nSpanish language is split into two superdialects, namely, an urban speech used\nacross major American and Spanish citites and a diverse form that encompasses\nrural areas and small towns. The latter can be further clustered into smaller\nvarieties with a stronger regional character.", "no": 86}, {"url": "https://arxiv.org/abs/1409.5165", "title": "A Method for Stopping Active Learning Based on Stabilizing Predictions\n  and the Need for User-Adjustable Stopping", "cites": "86", "abstract": "A survey of existing methods for stopping active learning (AL) reveals the\nneeds for methods that are: more widely applicable; more aggressive in saving\nannotations; and more stable across changing datasets. A new method for\nstopping AL based on stabilizing predictions is presented that addresses these\nneeds. Furthermore, stopping methods are required to handle a broad range of\ndifferent annotation/performance tradeoff valuations. Despite this, the\nexisting body of work is dominated by conservative methods with little (if any)\nattention paid to providing users with control over the behavior of stopping\nmethods. The proposed method is shown to fill a gap in the level of\naggressiveness available for stopping AL and supports providing users with\ncontrol over stopping behavior.", "no": 87}, {"url": "https://arxiv.org/abs/1401.6984", "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN", "cites": "84", "abstract": "The Kaldi toolkit is becoming popular for constructing automated speech\nrecognition (ASR) systems. Meanwhile, in recent years, deep neural networks\n(DNNs) have shown state-of-the-art performance on various ASR tasks. This\ndocument describes our open-source recipes to implement fully-fledged DNN\nacoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning\ntoolkit developed under the Theano environment. Using these recipes, we can\nbuild up multiple systems including DNN hybrid systems, convolutional neural\nnetwork (CNN) systems and bottleneck feature systems. These recipes are\ndirectly based on the Kaldi Switchboard 110-hour setup. However, adapting them\nto new datasets is easy to achieve.", "no": 88}, {"url": "https://arxiv.org/abs/1405.0947", "title": "Learning Bilingual Word Representations by Marginalizing Alignments", "cites": "84", "abstract": "We present a probabilistic model that simultaneously learns alignments and\ndistributed representations for bilingual data. By marginalizing over word\nalignments the model captures a larger semantic context than prior work relying\non hard alignments. The advantage of this approach is demonstrated in a\ncross-lingual classification task, where we outperform the prior published\nstate of the art.", "no": 89}, {"url": "https://arxiv.org/abs/1409.2195", "title": "Analyzing the Language of Food on Social Media", "cites": "84", "abstract": "We investigate the predictive power behind the language of food on social\nmedia. We collect a corpus of over three million food-related posts from\nTwitter and demonstrate that many latent population characteristics can be\ndirectly predicted from this data: overweight rate, diabetes rate, political\nleaning, and home geographical location of authors. For all tasks, our\nlanguage-based models significantly outperform the majority-class baselines.\nPerformance is further improved with more complex natural language processing,\nsuch as topic modeling. We analyze which textual features have most predictive\npower for these datasets, providing insight into the connections between the\nlanguage of food, geographic locale, and community characteristics. Lastly, we\ndesign and implement an online system for real-time query and visualization of\nthe dataset. Visualization tools, such as geo-referenced heatmaps,\nsemantics-preserving wordclouds and temporal histograms, allow us to discover\nmore complex, global patterns mirrored in the language of food.", "no": 90}, {"url": "https://arxiv.org/abs/1410.2045", "title": "Supervised learning Methods for Bangla Web Document Categorization", "cites": "82", "abstract": "This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.", "no": 91}, {"url": "https://arxiv.org/abs/1411.1147", "title": "Conditional Random Field Autoencoders for Unsupervised Structured\n  Prediction", "cites": "78", "abstract": "We introduce a framework for unsupervised learning of structured predictors\nwith overlapping, global features. Each input's latent representation is\npredicted conditional on the observable data using a feature-rich conditional\nrandom field. Then a reconstruction of the input is (re)generated, conditional\non the latent structure, using models for which maximum likelihood estimation\nhas a closed-form. Our autoencoder formulation enables efficient learning\nwithout making unrealistic independence assumptions or restricting the kinds of\nfeatures that can be used. We illustrate insightful connections to traditional\nautoencoders, posterior regularization and multi-view learning. We show\ncompetitive results with instantiations of the model for two canonical NLP\ntasks: part-of-speech induction and bitext word alignment, and show that\ntraining our model can be substantially more efficient than comparable\nfeature-rich baselines.", "no": 92}, {"url": "https://arxiv.org/abs/1409.8152", "title": "Controversy and Sentiment in Online News", "cites": "77", "abstract": "How do news sources tackle controversial issues? In this work, we take a\ndata-driven approach to understand how controversy interplays with emotional\nexpression and biased language in the news. We begin by introducing a new\ndataset of controversial and non-controversial terms collected using\ncrowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare\nmillions of articles discussing controversial and non-controversial issues over\na span of 7 months. We find that in general, when it comes to controversial\nissues, the use of negative affect and biased language is prevalent, while the\nuse of strong emotion is tempered. We also observe many differences across news\nsources. Using these findings, we show that we can indicate to what extent an\nissue is controversial, by comparing it with other issues in terms of how they\nare portrayed across different media.", "no": 93}, {"url": "https://arxiv.org/abs/1409.1257", "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation\n  using Automatic Segmentation", "cites": "76", "abstract": "The authors of (Cho et al., 2014a) have shown that the recently introduced\nneural network translation systems suffer from a significant drop in\ntranslation quality when translating long sentences, unlike existing\nphrase-based translation systems. In this paper, we propose a way to address\nthis issue by automatically segmenting an input sentence into phrases that can\nbe easily translated by the neural network translation model. Once each segment\nhas been independently translated by the neural machine translation model, the\ntranslated clauses are concatenated to form a final translation. Empirical\nresults show a significant improvement in translation quality for long\nsentences.", "no": 94}, {"url": "https://arxiv.org/abs/1405.5202", "title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference\n  Resolution", "cites": "75", "abstract": "Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions.", "no": 95}, {"url": "https://arxiv.org/abs/1411.0861", "title": "Using Linguistic Features to Estimate Suicide Probability of Chinese\n  Microblog Users", "cites": "75", "abstract": "If people with high risk of suicide can be identified through social media\nlike microblog, it is possible to implement an active intervention system to\nsave their lives. Based on this motivation, the current study administered the\nSuicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a\nleading microblog service provider in China. Two NLP (Natural Language\nProcessing) methods, the Chinese edition of Linguistic Inquiry and Word Count\n(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract\nlinguistic features from the Sina Weibo data. We trained predicting models by\nmachine learning algorithm based on these two types of features, to estimate\nsuicide probability based on linguistic features. The experiment results\nindicate that LDA can find topics that relate to suicide probability, and\nimprove the performance of prediction. Our study adds value in prediction of\nsuicidal probability of social network users with their behaviors.", "no": 96}, {"url": "https://arxiv.org/abs/1410.6903", "title": "Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech", "cites": "74", "abstract": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\nspeech features in most speech and speaker recognition applications. In this\npaper, we study the effect of resampling a speech signal on these speech\nfeatures. We first derive a relationship between the MFCC param- eters of the\nresampled speech and the MFCC parameters of the original speech. We propose six\nmethods of calculating the MFCC parameters of downsampled speech by\ntransforming the Mel filter bank used to com- pute MFCC of the original speech.\nWe then experimentally compute the MFCC parameters of the down sampled speech\nusing the proposed meth- ods and compute the Pearson coefficient between the\nMFCC parameters of the downsampled speech and that of the original speech to\nidentify the most effective choice of Mel-filter band that enables the computed\nMFCC of the resampled speech to be as close as possible to the original speech\nsample MFCC.", "no": 97}, {"url": "https://arxiv.org/abs/1401.0569", "title": "Natural Language Processing in Biomedicine: A Unified System\n  Architecture Overview", "cites": "72", "abstract": "In modern electronic medical records (EMR) much of the clinically important\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\nprovided in structured data fields, but rather are encoded in clinician\ngenerated narrative text. Natural language processing (NLP) provides a means of\n\"unlocking\" this important data source for applications in clinical decision\nsupport, quality assurance, and public health. This chapter provides an\noverview of representative NLP systems in biomedicine based on a unified\narchitectural view. A general architecture in an NLP system consists of two\nmain components: background knowledge that includes biomedical knowledge\nresources and a framework that integrates NLP tools to process text. Systems\ndiffer in both components, which we will review briefly. Additionally,\nchallenges facing current research efforts in biomedical NLP include the\npaucity of large, publicly available annotated corpora, although initiatives\nthat facilitate data sharing, system evaluation, and collaborative work between\nresearchers in clinical NLP are starting to emerge.", "no": 98}, {"url": "https://arxiv.org/abs/1402.6010", "title": "Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social\n  Media", "cites": "72", "abstract": "The growing popularity of social media (e.g, Twitter) allows users to easily\nshare information with each other and influence others by expressing their own\nsentiments on various subjects. In this work, we propose an unsupervised\n\\emph{tri-clustering} framework, which analyzes both user-level and tweet-level\nsentiments through co-clustering of a tripartite graph. A compelling feature of\nthe proposed framework is that the quality of sentiment clustering of tweets,\nusers, and features can be mutually improved by joint clustering. We further\ninvestigate the evolution of user-level sentiments and latent feature vectors\nin an online framework and devise an efficient online algorithm to sequentially\nupdate the clustering of tweets, users and features with newly arrived data.\nThe online framework not only provides better quality of both dynamic\nuser-level and tweet-level sentiment analysis, but also improves the\ncomputational and storage efficiency. We verified the effectiveness and\nefficiency of the proposed approaches on the November 2012 California ballot\nTwitter data.", "no": 99}, {"url": "https://arxiv.org/abs/1410.5877", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for\n  Statistical Machine Translation", "cites": "72", "abstract": "We explore how to improve machine translation systems by adding more\ntranslation data in situations where we already have substantial resources. The\nmain challenge is how to buck the trend of diminishing returns that is commonly\nencountered. We present an active learning-style data solicitation algorithm to\nmeet this challenge. We test it, gathering annotations via Amazon Mechanical\nTurk, and find that we get an order of magnitude increase in performance rates\nof improvement.", "no": 100}]