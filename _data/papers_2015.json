[{"url": "https://arxiv.org/abs/1508.04025", "title": "Effective Approaches to Attention-based Neural Machine Translation", "cites": "7 229", "abstract": "An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.", "no": 1}, {"url": "https://arxiv.org/abs/1508.07909", "title": "Neural Machine Translation of Rare Words with Subword Units", "cites": "6 413", "abstract": "Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary. In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively.", "no": 2}, {"url": "https://arxiv.org/abs/1509.01626", "title": "Character-level Convolutional Networks for Text Classification", "cites": "4 869", "abstract": "This article offers an empirical exploration on the use of character-level\nconvolutional networks (ConvNets) for text classification. We constructed\nseveral large-scale datasets to show that character-level convolutional\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\noffered against traditional models such as bag of words, n-grams and their\nTFIDF variants, and deep learning models such as word-based ConvNets and\nrecurrent neural networks.", "no": 3}, {"url": "https://arxiv.org/abs/1505.00468", "title": "VQA: Visual Question Answering", "cites": "4 229", "abstract": "We propose the task of free-form and open-ended Visual Question Answering\n(VQA). Given an image and a natural language question about the image, the task\nis to provide an accurate natural language answer. Mirroring real-world\nscenarios, such as helping the visually impaired, both the questions and\nanswers are open-ended. Visual questions selectively target different areas of\nan image, including background details and underlying context. As a result, a\nsystem that succeeds at VQA typically needs a more detailed understanding of\nthe image and complex reasoning than a system producing generic image captions.\nMoreover, VQA is amenable to automatic evaluation, since many open-ended\nanswers contain only a few words or a closed set of answers that can be\nprovided in a multiple-choice format. We provide a dataset containing ~0.25M\nimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the\ninformation it provides. Numerous baselines and methods for VQA are provided\nand compared with human performance. Our VQA demo is available on CloudCV\n(http://cloudcv.org/vqa).", "no": 4}, {"url": "https://arxiv.org/abs/1508.05326", "title": "A large annotated corpus for learning natural language inference", "cites": "3 557", "abstract": "Understanding entailment and contradiction is fundamental to understanding\nnatural language, and inference about entailment and contradiction is a\nvaluable testing ground for the development of semantic representations.\nHowever, machine learning research in this area has been dramatically limited\nby the lack of large-scale resources. To address this, we introduce the\nStanford Natural Language Inference corpus, a new, freely available collection\nof labeled sentence pairs, written by humans doing a novel grounded task based\non image captioning. At 570K pairs, it is two orders of magnitude larger than\nall other resources of its type. This increase in scale allows lexicalized\nclassifiers to outperform some sophisticated existing entailment models, and it\nallows a neural network-based model to perform competitively on natural\nlanguage inference benchmarks for the first time.", "no": 5}, {"url": "https://arxiv.org/abs/1508.01991", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "cites": "3 410", "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.", "no": 6}, {"url": "https://arxiv.org/abs/1506.03340", "title": "Teaching Machines to Read and Comprehend", "cites": "3 094", "abstract": "Teaching machines to read natural language documents remains an elusive\nchallenge. Machine reading systems can be tested on their ability to answer\nquestions posed on the contents of documents that they have seen, but until now\nlarge scale training and test datasets have been missing for this type of\nevaluation. In this work we define a new methodology that resolves this\nbottleneck and provides large scale supervised reading comprehension data. This\nallows us to develop a class of attention based deep neural networks that learn\nto read real documents and answer complex questions with minimal prior\nknowledge of language structure.", "no": 7}, {"url": "https://arxiv.org/abs/1503.00075", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "cites": "2 949", "abstract": "Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).", "no": 8}, {"url": "https://arxiv.org/abs/1512.02595", "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "cites": "2 726", "abstract": "We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages.\nBecause it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages. Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale.", "no": 9}, {"url": "https://arxiv.org/abs/1509.00685", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "cites": "2 510", "abstract": "Summarization based on text extraction is inherently limited, but\ngeneration-style abstractive methods have proven challenging to build. In this\nwork, we propose a fully data-driven approach to abstractive sentence\nsummarization. Our method utilizes a local attention-based model that generates\neach word of the summary conditioned on the input sentence. While the model is\nstructurally simple, it can easily be trained end-to-end and scales to a large\namount of training data. The model shows significant performance gains on the\nDUC-2004 shared task compared with several strong baselines.", "no": 10}, {"url": "https://arxiv.org/abs/1506.07503", "title": "Attention-Based Models for Speech Recognition", "cites": "2 370", "abstract": "Recurrent sequence generators conditioned on input data through an attention\nmechanism have recently shown very good performance on a range of tasks in-\ncluding machine translation, handwriting synthesis and image caption gen-\neration. We extend the attention-mechanism with features needed for speech\nrecognition. We show that while an adaptation of the model used for machine\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\nTIMIT phoneme recognition task, it can only be applied to utterances which are\nroughly as long as the ones it was trained on. We offer a qualitative\nexplanation of this failure and propose a novel and generic method of adding\nlocation-awareness to the attention mechanism to alleviate this issue. The new\nmethod yields a model that is robust to long inputs and achieves 18% PER in\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\npropose a change to the at- tention mechanism that prevents it from\nconcentrating too much on single frames, which further reduces PER to 17.6%\nlevel.", "no": 11}, {"url": "https://arxiv.org/abs/1511.06709", "title": "Improving Neural Machine Translation Models with Monolingual Data", "cites": "2 325", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance\nfor several language pairs, while only using parallel data for training.\nTarget-side monolingual data plays an important role in boosting fluency for\nphrase-based statistical machine translation, and we investigate the use of\nmonolingual data for NMT. In contrast to previous work, which combines NMT\nmodels with separately trained language models, we note that encoder-decoder\nNMT architectures already have the capacity to learn the same information as a\nlanguage model, and we explore strategies to train with monolingual data\nwithout changing the neural network architecture. By pairing monolingual\ntraining data with an automatic back-translation, we can treat it as additional\nparallel training data, and we obtain substantial improvements on the WMT 15\ntask English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task\nTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We\nalso show that fine-tuning on in-domain monolingual and parallel data gives\nsubstantial improvements for the IWSLT 15 task English->German.", "no": 12}, {"url": "https://arxiv.org/abs/1506.06726", "title": "Skip-Thought Vectors", "cites": "2 227", "abstract": "We describe an approach for unsupervised learning of a generic, distributed\nsentence encoder. Using the continuity of text from books, we train an\nencoder-decoder model that tries to reconstruct the surrounding sentences of an\nencoded passage. Sentences that share semantic and syntactic properties are\nthus mapped to similar vector representations. We next introduce a simple\nvocabulary expansion method to encode words that were not seen as part of\ntraining, allowing us to expand our vocabulary to a million words. After\ntraining our model, we extract and evaluate our vectors with linear models on 8\ntasks: semantic relatedness, paraphrase detection, image-sentence ranking,\nquestion-type classification and 4 benchmark sentiment and subjectivity\ndatasets. The end result is an off-the-shelf encoder that can produce highly\ngeneric sentence representations that are robust and perform well in practice.\nWe will make our encoder publicly available.", "no": 13}, {"url": "https://arxiv.org/abs/1511.06349", "title": "Generating Sentences from a Continuous Space", "cites": "2 127", "abstract": "The standard recurrent neural network language model (RNNLM) generates\nsentences one word at a time and does not work from an explicit global sentence\nrepresentation. In this work, we introduce and study an RNN-based variational\nautoencoder generative model that incorporates distributed latent\nrepresentations of entire sentences. This factorization allows it to explicitly\nmodel holistic properties of sentences such as style, topic, and high-level\nsyntactic features. Samples from the prior over these sentence representations\nremarkably produce diverse and well-formed sentences through simple\ndeterministic decoding. By examining paths through this latent space, we are\nable to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate its effectiveness in imputing missing\nwords, explore many interesting properties of the model's latent sentence\nspace, and present negative results on the use of the model in language\nmodeling.", "no": 14}, {"url": "https://arxiv.org/abs/1506.06724", "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by\n  Watching Movies and Reading Books", "cites": "2 121", "abstract": "Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for.", "no": 15}, {"url": "https://arxiv.org/abs/1508.01211", "title": "Listen, Attend and Spell", "cites": "2 019", "abstract": "We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.", "no": 16}, {"url": "https://arxiv.org/abs/1510.03055", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "cites": "1 955", "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.", "no": 17}, {"url": "https://arxiv.org/abs/1504.00325", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "cites": "1 839", "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation\nserver. When completed, the dataset will contain over one and a half million\ncaptions describing over 330,000 images. For the training and validation\nimages, five independent human generated captions will be provided. To ensure\nconsistency in evaluation of automatic caption generation algorithms, an\nevaluation server is used. The evaluation server receives candidate captions\nand scores them using several popular metrics, including BLEU, METEOR, ROUGE\nand CIDEr. Instructions for using the evaluation server are provided.", "no": 18}, {"url": "https://arxiv.org/abs/1506.03099", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks", "cites": "1 784", "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given\nsome input, as exemplified by recent results in machine translation and image\ncaptioning. The current approach to training them consists of maximizing the\nlikelihood of each token in the sequence given the current (recurrent) state\nand the previous token. At inference, the unknown previous token is then\nreplaced by a token generated by the model itself. This discrepancy between\ntraining and inference can yield errors that can accumulate quickly along the\ngenerated sequence. We propose a curriculum learning strategy to gently change\nthe training process from a fully guided scheme using the true previous token,\ntowards a less guided scheme which mostly uses the generated token instead.\nExperiments on several sequence prediction tasks show that this approach yields\nsignificant improvements. Moreover, it was used successfully in our winning\nentry to the MSCOCO image captioning challenge, 2015.", "no": 19}, {"url": "https://arxiv.org/abs/1511.02274", "title": "Stacked Attention Networks for Image Question Answering", "cites": "1 739", "abstract": "This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer.", "no": 20}, {"url": "https://arxiv.org/abs/1511.08308", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "cites": "1 686", "abstract": "Named entity recognition is a challenging task that has traditionally\nrequired large amounts of knowledge in the form of feature engineering and\nlexicons to achieve high performance. In this paper, we present a novel neural\nnetwork architecture that automatically detects word- and character-level\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\nthe need for most feature engineering. We also propose a novel method of\nencoding partial lexicon matches in neural networks and compare it to existing\napproaches. Extensive evaluation shows that, given only tokenized text and\npublicly available word embeddings, our system is competitive on the CoNLL-2003\ndataset and surpasses the previously reported state of the art performance on\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\nfrom publicly-available sources, we establish new state of the art performance\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\nentity linking information.", "no": 21}, {"url": "https://arxiv.org/abs/1506.05869", "title": "A Neural Conversational Model", "cites": "1 677", "abstract": "Conversational modeling is an important task in natural language\nunderstanding and machine intelligence. Although previous approaches exist,\nthey are often restricted to specific domains (e.g., booking an airline ticket)\nand require hand-crafted rules. In this paper, we present a simple approach for\nthis task which uses the recently proposed sequence to sequence framework. Our\nmodel converses by predicting the next sentence given the previous sentence or\nsentences in a conversation. The strength of our model is that it can be\ntrained end-to-end and thus requires much fewer hand-crafted rules. We find\nthat this straightforward model can generate simple conversations given a large\nconversational training dataset. Our preliminary results suggest that, despite\noptimizing the wrong objective function, the model is able to converse well. It\nis able extract knowledge from both a domain specific dataset, and from a\nlarge, noisy, and general domain dataset of movie subtitles. On a\ndomain-specific IT helpdesk dataset, the model can find a solution to a\ntechnical problem via conversations. On a noisy open-domain movie transcript\ndataset, the model can perform simple forms of common sense reasoning. As\nexpected, we also find that the lack of consistency is a common failure mode of\nour model.", "no": 22}, {"url": "https://arxiv.org/abs/1507.04808", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models", "cites": "1 662", "abstract": "We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.", "no": 23}, {"url": "https://arxiv.org/abs/1508.06615", "title": "Character-Aware Neural Language Models", "cites": "1 574", "abstract": "We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information.", "no": 24}, {"url": "https://arxiv.org/abs/1511.06732", "title": "Sequence Level Training with Recurrent Neural Networks", "cites": "1 488", "abstract": "Many natural language processing applications use language models to generate\ntext. These models are typically trained to predict the next word in a\nsequence, given the previous words and some context such as an image. However,\nat test time the model is expected to generate the entire sequence from\nscratch. This discrepancy makes generation brittle, as errors may accumulate\nalong the way. We address this issue by proposing a novel sequence level\ntraining algorithm that directly optimizes the metric used at test time, such\nas BLEU or ROUGE. On three different tasks, our approach outperforms several\nstrong baselines for greedy generation. The method is also competitive when\nthese baselines employ beam search, while being several times faster.", "no": 25}, {"url": "https://arxiv.org/abs/1505.04870", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for\n  Richer Image-to-Sentence Models", "cites": "1 422", "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based\nimage description. This paper presents Flickr30k Entities, which augments the\n158k captions from Flickr30k with 244k coreference chains, linking mentions of\nthe same entities across different captions for the same image, and associating\nthem with 276k manually annotated bounding boxes. Such annotations are\nessential for continued progress in automatic image description and grounded\nlanguage understanding. They enable us to define a new benchmark for\nlocalization of textual entity mentions in an image. We present a strong\nbaseline for this task that combines an image-text embedding, detectors for\ncommon objects, a color classifier, and a bias towards selecting larger\nobjects. While our baseline rivals in accuracy more complex state-of-the-art\nmodels, we show that its gains cannot be easily parlayed into improvements on\nsuch tasks as image-sentence retrieval, thus underlining the limitations of\ncurrent methods and the need for further research.", "no": 26}, {"url": "https://arxiv.org/abs/1503.03244", "title": "Convolutional Neural Network Architectures for Matching Natural Language\n  Sentences", "cites": "1 253", "abstract": "Semantic matching is of central importance to many natural language tasks\n\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to\nadequately model the internal structures of language objects and the\ninteraction between them. As a step toward this goal, we propose convolutional\nneural network models for matching two sentences, by adapting the convolutional\nstrategy in vision and speech. The proposed models not only nicely represent\nthe hierarchical structures of sentences with their layer-by-layer composition\nand pooling, but also capture the rich matching patterns at different levels.\nOur models are rather generic, requiring no prior knowledge on language, and\ncan hence be applied to matching tasks of different nature and in different\nlanguages. The empirical study on a variety of matching tasks demonstrates the\nefficacy of the proposed model on a variety of matching tasks and its\nsuperiority to competitor models.", "no": 27}, {"url": "https://arxiv.org/abs/1506.07285", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "cites": "1 140", "abstract": "Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook's bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets.", "no": 28}, {"url": "https://arxiv.org/abs/1503.02364", "title": "Neural Responding Machine for Short-Text Conversation", "cites": "1 113", "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models.", "no": 29}, {"url": "https://arxiv.org/abs/1511.01432", "title": "Semi-supervised Sequence Learning", "cites": "1 102", "abstract": "We present two approaches that use unlabeled data to improve sequence\nlearning with recurrent networks. The first approach is to predict what comes\nnext in a sequence, which is a conventional language model in natural language\nprocessing. The second approach is to use a sequence autoencoder, which reads\nthe input sequence into a vector and predicts the input sequence again. These\ntwo algorithms can be used as a \"pretraining\" step for a later supervised\nsequence learning algorithm. In other words, the parameters obtained from the\nunsupervised step can be used as a starting point for other supervised training\nmodels. In our experiments, we find that long short term memory recurrent\nnetworks after being pretrained with the two approaches are more stable and\ngeneralize better. With pretraining, we are able to train long short term\nmemory recurrent networks up to a few hundred timesteps, thereby achieving\nstrong performance in many text classification tasks, such as IMDB, DBpedia and\n20 Newsgroups.", "no": 30}, {"url": "https://arxiv.org/abs/1510.03820", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\n  Neural Networks for Sentence Classification", "cites": "1 090", "abstract": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong\nperformance on the practically important task of sentence classification (kim\n2014, kalchbrenner 2014, johnson 2014). However, these models require\npractitioners to specify an exact model architecture and set accompanying\nhyperparameters, including the filter region size, regularization parameters,\nand so on. It is currently unknown how sensitive model performance is to\nchanges in these configurations for the task of sentence classification. We\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\narchitecture components on model performance; our aim is to distinguish between\nimportant and comparatively inconsequential design decisions for sentence\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\nmodels) due to their comparative simplicity and strong empirical performance,\nwhich makes it a modern standard baseline method akin to Support Vector Machine\n(SVMs) and logistic regression. We derive practical advice from our extensive\nempirical results for those interested in getting the most out of CNNs for\nsentence classification in real world settings.", "no": 31}, {"url": "https://arxiv.org/abs/1502.05698", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "cites": "1 055", "abstract": "One long-term goal of machine learning research is to produce methods that\nare applicable to reasoning and natural language, in particular building an\nintelligent dialogue agent. To measure progress towards that goal, we argue for\nthe usefulness of a set of proxy tasks that evaluate reading comprehension via\nquestion answering. Our tasks measure understanding in several ways: whether a\nsystem is able to answer questions via chaining facts, simple induction,\ndeduction and many more. The tasks are designed to be prerequisites for any\nsystem that aims to be capable of conversing with a human. We believe many\nexisting learning systems can currently not solve them, and hence our aim is to\nclassify these tasks into skill sets, so that researchers can identify (and\nthen rectify) the failings of their systems. We also extend and improve the\nrecently introduced Memory Networks model, and show it is able to solve some,\nbut not all, of the tasks.", "no": 32}, {"url": "https://arxiv.org/abs/1508.04395", "title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "cites": "1 055", "abstract": "Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.", "no": 33}, {"url": "https://arxiv.org/abs/1506.02078", "title": "Visualizing and Understanding Recurrent Networks", "cites": "1 020", "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.", "no": 34}, {"url": "https://arxiv.org/abs/1502.08029", "title": "Describing Videos by Exploiting Temporal Structure", "cites": "1 015", "abstract": "Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.", "no": 35}, {"url": "https://arxiv.org/abs/1510.00726", "title": "A Primer on Neural Network Models for Natural Language Processing", "cites": "1 006", "abstract": "Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.", "no": 36}, {"url": "https://arxiv.org/abs/1511.02799", "title": "Neural Module Networks", "cites": "945", "abstract": "Visual question answering is fundamentally compositional in nature---a\nquestion like \"where is the dog?\" shares substructure with questions like \"what\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\nexploit the representational capacity of deep networks and the compositional\nlinguistic structure of questions. We describe a procedure for constructing and\nlearning *neural module networks*, which compose collections of jointly-trained\nneural \"modules\" into deep networks for question answering. Our approach\ndecomposes questions into their linguistic substructures, and uses these\nstructures to dynamically instantiate modular networks (with reusable\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\ncompound networks are jointly trained. We evaluate our approach on two\nchallenging datasets for visual question answering, achieving state-of-the-art\nresults on both the VQA natural image dataset and a new dataset of complex\nquestions about abstract shapes.", "no": 37}, {"url": "https://arxiv.org/abs/1512.05193", "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling\n  Sentence Pairs", "cites": "897", "abstract": "How to model a pair of sentences is a critical issue in many NLP tasks such\nas answer selection (AS), paraphrase identification (PI) and textual entailment\n(TE). Most prior work (i) deals with one individual task by fine-tuning a\nspecific system; (ii) models each sentence's representation separately, rarely\nconsidering the impact of the other sentence; or (iii) relies fully on manually\ndesigned, task-specific linguistic features. This work presents a general\nAttention Based Convolutional Neural Network (ABCNN) for modeling a pair of\nsentences. We make three contributions. (i) ABCNN can be applied to a wide\nvariety of tasks that require modeling of sentence pairs. (ii) We propose three\nattention schemes that integrate mutual influence between sentences into CNN;\nthus, the representation of each sentence takes into consideration its\ncounterpart. These interdependent sentence pair representations are more\npowerful than isolated sentence representations. (iii) ABCNN achieves\nstate-of-the-art performance on AS, PI and TE tasks.", "no": 38}, {"url": "https://arxiv.org/abs/1511.02283", "title": "Generation and Comprehension of Unambiguous Object Descriptions", "cites": "895", "abstract": "We propose a method that can generate an unambiguous description (known as a\nreferring expression) of a specific object or region in an image, and which can\nalso comprehend or interpret such an expression to infer which object is being\ndescribed. We show that our method outperforms previous methods that generate\ndescriptions of objects without taking into account other potentially ambiguous\nobjects in the scene. Our model is inspired by recent successes of deep\nlearning methods for image captioning, but while image captioning is difficult\nto evaluate, our task allows for easy objective evaluation. We also present a\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\nreleased the dataset and a toolbox for visualization and evaluation, see\nhttps://github.com/mjhucla/Google_Refexp_toolbox", "no": 39}, {"url": "https://arxiv.org/abs/1508.01745", "title": "Semantically Conditioned LSTM-based Natural Language Generation for\n  Spoken Dialogue Systems", "cites": "893", "abstract": "Natural language generation (NLG) is a critical component of spoken dialogue\nand it has a significant impact both on usability and perceived quality. Most\nNLG systems in common use employ rules and heuristics and tend to generate\nrigid and stylised responses without the natural variation of human language.\nThey are also not easily scaled to systems covering multiple domains and\nlanguages. This paper presents a statistical language generator based on a\nsemantically controlled Long Short-term Memory (LSTM) structure. The LSTM\ngenerator can learn from unaligned data by jointly optimising sentence planning\nand surface realisation using a simple cross entropy training criterion, and\nlanguage variation can be easily achieved by sampling from output candidates.\nWith fewer heuristics, an objective evaluation in two differing test domains\nshowed the proposed method improved performance compared to previous methods.\nHuman judges scored the LSTM system higher on informativeness and naturalness\nand overall preferred it to the other systems.", "no": 40}, {"url": "https://arxiv.org/abs/1506.06714", "title": "A Neural Network Approach to Context-Sensitive Generation of\n  Conversational Responses", "cites": "892", "abstract": "We present a novel response generation system that can be trained end to end\non large quantities of unstructured Twitter conversations. A neural network\narchitecture is used to address sparsity issues that arise when integrating\ncontextual information into classic statistical models, allowing the system to\ntake into account previous dialog utterances. Our dynamic-context generative\nmodels show consistent gains over both context-sensitive and\nnon-context-sensitive Machine Translation and Information Retrieval baselines.", "no": 41}, {"url": "https://arxiv.org/abs/1506.08909", "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems", "cites": "884", "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response.", "no": 42}, {"url": "https://arxiv.org/abs/1511.06391", "title": "Order Matters: Sequence to sequence for sets", "cites": "840", "abstract": "Sequences have become first class citizens in supervised learning thanks to\nthe resurgence of recurrent neural networks. Many complex tasks that require\nmapping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to\nefficiently represent the joint probability of sequences. In many cases,\nhowever, variable sized inputs and/or outputs might not be naturally expressed\nas sequences. For instance, it is not clear how to input a set of numbers into\na model where the task is to sort them; similarly, we do not know how to\norganize outputs when they correspond to random variables and the task is to\nmodel their unknown joint probability. In this paper, we first show using\nvarious examples that the order in which we organize input and/or output data\nmatters significantly when learning an underlying model. We then discuss an\nextension of the seq2seq framework that goes beyond sequences and handles input\nsets in a principled way. In addition, we propose a loss which, by searching\nover possible orders during training, deals with the lack of structure of\noutput sets. We show empirical evidence of our claims regarding ordering, and\non the modifications to the seq2seq framework on benchmark language modeling\nand parsing tasks, as well as two artificial tasks -- sorting numbers and\nestimating the joint probability of unknown graphical models.", "no": 43}, {"url": "https://arxiv.org/abs/1511.08630", "title": "A C-LSTM Neural Network for Text Classification", "cites": "776", "abstract": "Neural network models have been demonstrated to be capable of achieving\nremarkable performance in sentence and document modeling. Convolutional neural\nnetwork (CNN) and recurrent neural network (RNN) are two mainstream\narchitectures for such modeling tasks, which adopt totally different ways of\nunderstanding natural languages. In this work, we combine the strengths of both\narchitectures and propose a novel and unified model called C-LSTM for sentence\nrepresentation and text classification. C-LSTM utilizes CNN to extract a\nsequence of higher-level phrase representations, and are fed into a long\nshort-term memory recurrent neural network (LSTM) to obtain the sentence\nrepresentation. C-LSTM is able to capture both local features of phrases as\nwell as global and temporal sentence semantics. We evaluate the proposed\narchitecture on sentiment classification and question classification tasks. The\nexperimental results show that the C-LSTM outperforms both CNN and LSTM and can\nachieve excellent performance on these tasks.", "no": 44}, {"url": "https://arxiv.org/abs/1505.08075", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "cites": "775", "abstract": "We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.", "no": 45}, {"url": "https://arxiv.org/abs/1502.06922", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval", "cites": "770", "abstract": "This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.", "no": 46}, {"url": "https://arxiv.org/abs/1511.06114", "title": "Multi-task Sequence to Sequence Learning", "cites": "759", "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in\nsupervised learning. To date, most of its applications focused on only one task\nand not much work explored this framework for multiple tasks. This paper\nexamines three multi-task learning (MTL) settings for sequence to sequence\nmodels: (a) the oneto-many setting - where the encoder is shared between\nseveral tasks such as machine translation and syntactic parsing, (b) the\nmany-to-one setting - useful when only the decoder can be shared, as in the\ncase of translation and image caption generation, and (c) the many-to-many\nsetting - where multiple encoders and decoders are shared, which is the case\nwith unsupervised objectives and translation. Our results show that training on\na small amount of parsing and image caption data can improve the translation\nquality between English and German by up to 1.5 BLEU points over strong\nsingle-task baselines on the WMT benchmarks. Furthermore, we have established a\nnew state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we\nreveal interesting properties of the two unsupervised learning objectives,\nautoencoder and skip-thought, in the MTL context: autoencoder helps less in\nterms of perplexities but more on BLEU scores compared to skip-thought.", "no": 47}, {"url": "https://arxiv.org/abs/1508.00200", "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks", "cites": "734", "abstract": "Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.", "no": 48}, {"url": "https://arxiv.org/abs/1509.06664", "title": "Reasoning about Entailment with Neural Attention", "cites": "729", "abstract": "While most approaches to automatically recognizing entailment relations have\nused classifiers employing hand engineered features derived from complex\nnatural language processing pipelines, in practice their performance has been\nonly slightly better than bag-of-word pair classifiers using only lexical\nsimilarity. The only attempt so far to build an end-to-end differentiable\nneural network for entailment failed to outperform such a simple similarity\nclassifier. In this paper, we propose a neural model that reads two sentences\nto determine entailment using long short-term memory units. We extend this\nmodel with a word-by-word neural attention mechanism that encourages reasoning\nover entailments of pairs of words and phrases. Furthermore, we present a\nqualitative analysis of attention weights produced by this model, demonstrating\nsuch reasoning capabilities. On a large entailment dataset this model\noutperforms the previous best neural model and a classifier with engineered\nfeatures by a substantial margin. It is the first generic end-to-end\ndifferentiable system that achieves state-of-the-art accuracy on a textual\nentailment dataset.", "no": 49}, {"url": "https://arxiv.org/abs/1511.05234", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "cites": "725", "abstract": "We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].", "no": 50}, {"url": "https://arxiv.org/abs/1509.07761", "title": "Sentiment of Emojis", "cites": "719", "abstract": "There is a new generation of emoticons, called emojis, that is increasingly\nbeing used in mobile communications and social media. In the past two years,\nover ten billion emojis were used on Twitter. Emojis are Unicode graphic\nsymbols, used as a shorthand to express concepts and ideas. In contrast to the\nsmall number of well-known emoticons that carry clear emotional contents, there\nare hundreds of emojis. But what are their emotional contents? We provide the\nfirst emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a\nsentiment map of the 751 most frequently used emojis. The sentiment of the\nemojis is computed from the sentiment of the tweets in which they occur. We\nengaged 83 human annotators to label over 1.6 million tweets in 13 European\nlanguages by the sentiment polarity (negative, neutral, or positive). About 4%\nof the annotated tweets contain emojis. The sentiment analysis of the emojis\nallows us to draw several interesting conclusions. It turns out that most of\nthe emojis are positive, especially the most popular ones. The sentiment\ndistribution of the tweets with and without emojis is significantly different.\nThe inter-annotator agreement on the tweets with emojis is higher. Emojis tend\nto occur at the end of the tweets, and their sentiment polarity increases with\nthe distance. We observe no significant differences in the emoji rankings\nbetween the 13 languages and the Emoji Sentiment Ranking. Consequently, we\npropose our Emoji Sentiment Ranking as a European language-independent resource\nfor automated sentiment analysis. Finally, the paper provides a formalization\nof sentiment and a novel visualization in the form of a sentiment bar.", "no": 51}, {"url": "https://arxiv.org/abs/1507.08240", "title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and\n  WFST-based Decoding", "cites": "716", "abstract": "The performance of automatic speech recognition (ASR) has improved\ntremendously due to the application of deep neural networks (DNNs). Despite\nthis progress, building a new ASR system remains a challenging task, requiring\nvarious resources, multiple training stages and significant expertise. This\npaper presents our Eesen framework which drastically simplifies the existing\npipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen\ninvolves learning a single recurrent neural network (RNN) predicting\ncontext-independent targets (phonemes or characters). To remove the need for\npre-generated frame labels, we adopt the connectionist temporal classification\n(CTC) objective function to infer the alignments between speech and label\nsequences. A distinctive feature of Eesen is a generalized decoding approach\nbased on weighted finite-state transducers (WFSTs), which enables the efficient\nincorporation of lexicons and language models into CTC decoding. Experiments\nshow that compared with the standard hybrid DNN systems, Eesen achieves\ncomparable word error rates (WERs), while at the same time speeding up decoding\nsignificantly.", "no": 52}, {"url": "https://arxiv.org/abs/1512.01100", "title": "Effective LSTMs for Target-Dependent Sentiment Classification", "cites": "716", "abstract": "Target-dependent sentiment classification remains a challenge: modeling the\nsemantic relatedness of a target with its context words in a sentence.\nDifferent context words have different influences on determining the sentiment\npolarity of a sentence towards the target. Therefore, it is desirable to\nintegrate the connections between target word and context words when building a\nlearning system. In this paper, we develop two target dependent long short-term\nmemory (LSTM) models, where target information is automatically taken into\naccount. We evaluate our methods on a benchmark dataset from Twitter. Empirical\nresults show that modeling sentence representation with standard LSTM does not\nperform well. Incorporating target information into LSTM can significantly\nboost the classification accuracy. The target-dependent LSTM models achieve\nstate-of-the-art performances without using syntactic parser or external\nsentiment lexicons.", "no": 53}, {"url": "https://arxiv.org/abs/1511.06078", "title": "Learning Deep Structure-Preserving Image-Text Embeddings", "cites": "712", "abstract": "This paper proposes a method for learning joint embeddings of images and text\nusing a two-branch neural network with multiple layers of linear projections\nfollowed by nonlinearities. The network is trained using a large margin\nobjective that combines cross-view ranking constraints with within-view\nneighborhood structure preservation constraints inspired by metric learning\nliterature. Extensive experiments show that our approach gains significant\nimprovements in accuracy for image-to-text and text-to-image retrieval. Our\nmethod achieves new state-of-the-art results on the Flickr30K and MSCOCO\nimage-sentence datasets and shows promise on the new task of phrase\nlocalization on the Flickr30K Entities dataset.", "no": 54}, {"url": "https://arxiv.org/abs/1506.02075", "title": "Large-scale Simple Question Answering with Memory Networks", "cites": "646", "abstract": "Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions. To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance.", "no": 55}, {"url": "https://arxiv.org/abs/1505.02074", "title": "Exploring Models and Data for Image Question Answering", "cites": "637", "abstract": "This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented.", "no": 56}, {"url": "https://arxiv.org/abs/1508.02096", "title": "Finding Function in Form: Compositional Character Models for Open\n  Vocabulary Word Representation", "cites": "627", "abstract": "We introduce a model for constructing vector representations of words by\ncomposing characters using bidirectional LSTMs. Relative to traditional word\nrepresentation models that have independent vectors for each word type, our\nmodel requires only a single vector per character type and a fixed set of\nparameters for the compositional model. Despite the compactness of this model\nand, more importantly, the arbitrary nature of the form-function relationship\nin language, our \"composed\" word representations yield state-of-the-art results\nin language modeling and part-of-speech tagging. Benefits over traditional\nbaselines are particularly pronounced in morphologically rich languages (e.g.,\nTurkish).", "no": 57}, {"url": "https://arxiv.org/abs/1512.02902", "title": "MovieQA: Understanding Stories in Movies through Question-Answering", "cites": "627", "abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story\ncomprehension from both video and text. The dataset consists of 14,944\nquestions about 408 movies with high semantic diversity. The questions range\nfrom simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events\noccurred. Each question comes with a set of five possible answers; a correct\none and four deceiving answers provided by human annotators. Our dataset is\nunique in that it contains multiple sources of information -- video clips,\nplots, subtitles, scripts, and DVS. We analyze our data through various\nstatistics and methods. We further extend existing QA techniques to show that\nquestion-answering with such open-ended semantics is hard. We make this data\nset public along with an evaluation benchmark to encourage inspiring work in\nthis challenging domain.", "no": 58}, {"url": "https://arxiv.org/abs/1506.01066", "title": "Visualizing and Understanding Neural Models in NLP", "cites": "621", "abstract": "While neural networks have been successfully applied to many NLP tasks the\nresulting vector-based models are very difficult to interpret. For example it's\nnot clear how they achieve {\\em compositionality}, building sentence meaning\nfrom the meanings of words and phrases. In this paper we describe four\nstrategies for visualizing compositionality in neural models for NLP, inspired\nby similar work in computer vision. We first plot unit values to visualize\ncompositionality of negation, intensification, and concessive clauses, allow us\nto see well-known markedness asymmetries in negation. We then introduce three\nsimple and straightforward methods for visualizing a unit's {\\em salience}, the\namount it contributes to the final composed meaning: (1) gradient\nback-propagation, (2) the variance of a token from the average word node, (3)\nLSTM-style gates that measure information flow. We test our methods on\nsentiment using simple recurrent nets and LSTMs. Our general-purpose methods\nmay have wide applications for understanding compositionality and other\nsemantic properties of deep networks , and also shed light on why LSTMs\noutperform simple recurrent nets,", "no": 59}, {"url": "https://arxiv.org/abs/1508.03720", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest\n  Dependency Path", "cites": "614", "abstract": "Relation classification is an important research arena in the field of\nnatural language processing (NLP). In this paper, we present SDP-LSTM, a novel\nneural network to classify the relation of two entities in a sentence. Our\nneural architecture leverages the shortest dependency path (SDP) between two\nentities; multichannel recurrent neural networks, with long short term memory\n(LSTM) units, pick up heterogeneous information along the SDP. Our proposed\nmodel has several distinct features: (1) The shortest dependency paths retain\nmost relevant information (to relation classification), while eliminating\nirrelevant words in the sentence. (2) The multichannel LSTM networks allow\neffective information integration from heterogeneous sources over the\ndependency paths. (3) A customized dropout strategy regularizes the neural\nnetwork to alleviate overfitting. We test our model on the SemEval 2010\nrelation classification task, and achieve an $F_1$-score of 83.7\\%, higher than\ncompeting methods in the literature.", "no": 60}, {"url": "https://arxiv.org/abs/1506.01057", "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "cites": "593", "abstract": "Natural language generation of coherent long texts like paragraphs or longer\ndocuments is a challenging problem for recurrent networks models. In this\npaper, we explore an important step toward this generation task: training an\nLSTM (Long-short term memory) auto-encoder to preserve and reconstruct\nmulti-sentence paragraphs. We introduce an LSTM model that hierarchically\nbuilds an embedding for a paragraph from embeddings for sentences and words,\nthen decodes this embedding to reconstruct the original paragraph. We evaluate\nthe reconstructed paragraph using standard metrics like ROUGE and Entity Grid,\nshowing that neural models are able to encode texts in a way that preserve\nsyntactic, semantic, and discourse coherence. While only a first step toward\ngenerating coherent text units from neural models, our work has the potential\nto significantly impact natural language generation and\nsummarization\\footnote{Code for the three models described in this paper can be\nfound at www.stanford.edu/~jiweil/ .", "no": 61}, {"url": "https://arxiv.org/abs/1511.02301", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory\n  Representations", "cites": "582", "abstract": "We introduce a new test of how well language models capture meaning in\nchildren's books. Unlike standard language modelling benchmarks, it\ndistinguishes the task of predicting syntactic function words from that of\npredicting lower-frequency words, which carry greater semantic content. We\ncompare a range of state-of-the-art models, each with a different way of\nencoding what has been previously read. We show that models which store\nexplicit representations of long-term contexts outperform state-of-the-art\nneural language models at predicting semantic content words, although this\nadvantage is not observed for syntactic function words. Interestingly, we find\nthat the amount of text encoded in a single memory representation is highly\ninfluential to the performance: there is a sweet-spot, not too big and not too\nsmall, between single words and full sentences that allows the most meaningful\ninformation in a text to be effectively retained and recalled. Further, the\nattention over such window-based memories can be trained effectively through\nself-supervision. We then assess the generality of this principle by applying\nit to the CNN QA benchmark, which involves identifying named entities in\nparaphrased summaries of news articles, and achieve state-of-the-art\nperformance.", "no": 62}, {"url": "https://arxiv.org/abs/1505.01121", "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about\n  Images", "cites": "569", "abstract": "We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Neural-Image-QA, an end-to-end\nformulation to this problem for which all parts are trained jointly. In\ncontrast to previous efforts, we are facing a multi-modal problem where the\nlanguage output (answer) is conditioned on visual and natural language input\n(image and question). Our approach Neural-Image-QA doubles the performance of\nthe previous best approach on this problem. We provide additional insights into\nthe problem by analyzing how much information is contained only in the language\npart for which we provide a new human baseline. To study human consensus, which\nis related to the ambiguities inherent in this challenging task, we propose two\nnovel metrics and collect additional answers which extends the original DAQUAR\ndataset to DAQUAR-Consensus.", "no": 63}, {"url": "https://arxiv.org/abs/1506.00379", "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "cites": "566", "abstract": "Representation learning of knowledge bases (KBs) aims to embed both entities\nand relations into a low-dimensional space. Most existing methods only consider\ndirect relations in representation learning. We argue that multiple-step\nrelation paths also contain rich inference patterns between entities, and\npropose a path-based representation learning model. This model considers\nrelation paths as translations between entities for representation learning,\nand addresses two key challenges: (1) Since not all relation paths are\nreliable, we design a path-constraint resource allocation algorithm to measure\nthe reliability of relation paths. (2) We represent relation paths via semantic\ncomposition of relation embeddings. Experimental results on real-world datasets\nshow that, as compared with baselines, our model achieves significant and\nconsistent improvements on knowledge base completion and relation extraction\nfrom text.", "no": 64}, {"url": "https://arxiv.org/abs/1511.06038", "title": "Neural Variational Inference for Text Processing", "cites": "550", "abstract": "Recent advances in neural variational inference have spawned a renaissance in\ndeep latent variable models. In this paper we introduce a generic variational\ninference framework for generative and conditional models of text. While\ntraditional variational methods derive an analytic approximation for the\nintractable distributions over latent variables, here we construct an inference\nnetwork conditioned on the discrete text input to provide the variational\ndistribution. We validate this framework on two very different text modelling\napplications, generative document modelling and supervised question answering.\nOur neural variational document model combines a continuous stochastic document\nrepresentation with a bag-of-words generative model and achieves the lowest\nreported perplexities on two standard test corpora. The neural answer selection\nmodel employs a stochastic representation layer within an attention mechanism\nto extract the semantics between a question and answer pair. On two question\nanswering benchmarks this model exceeds all previous published benchmarks.", "no": 65}, {"url": "https://arxiv.org/abs/1504.06580", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "cites": "549", "abstract": "Relation classification is an important semantic processing task for which\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\nwe tackle the relation classification task using a convolutional neural network\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\nranking loss function that makes it easy to reduce the impact of artificial\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\nwhich is designed for the task of classifying the relationship between two\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\nfeatures. Additionally, our experimental results show that: (1) our approach is\nmore effective than CNN followed by a softmax classifier; (2) omitting the\nrepresentation of the artificial class Other improves both precision and\nrecall; and (3) using only word embeddings as input features is enough to\nachieve state-of-the-art results if we consider only the text between the two\ntarget nominals.", "no": 66}, {"url": "https://arxiv.org/abs/1511.08198", "title": "Towards Universal Paraphrastic Sentence Embeddings", "cites": "534", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings based on supervision from the Paraphrase Database (Ganitkevitch et\nal., 2013). We compare six compositional architectures, evaluating them on\nannotated textual similarity datasets drawn both from the same distribution as\nthe training data and from a wide range of other domains. We find that the most\ncomplex architectures, such as long short-term memory (LSTM) recurrent neural\nnetworks, perform best on the in-domain data. However, in out-of-domain\nscenarios, simple architectures such as word averaging vastly outperform LSTMs.\nOur simplest averaging model is even competitive with systems tuned for the\nparticular tasks while also being extremely efficient and easy to use.\n  In order to better understand how these architectures compare, we conduct\nfurther experiments on three supervised NLP tasks: sentence similarity,\nentailment, and sentiment classification. We again find that the word averaging\nmodels perform well for sentence similarity and entailment, outperforming\nLSTMs. However, on sentiment classification, we find that the LSTM performs\nvery strongly-even recording new state-of-the-art performance on the Stanford\nSentiment Treebank.\n  We then demonstrate how to combine our pretrained sentence embeddings with\nthese supervised tasks, using them both as a prior and as a black box feature\nextractor. This leads to performance rivaling the state of the art on the SICK\nsimilarity and entailment tasks. We release all of our resources to the\nresearch community with the hope that they can serve as the new baseline for\nfurther work on universal sentence embeddings.", "no": 67}, {"url": "https://arxiv.org/abs/1503.03535", "title": "On Using Monolingual Corpora in Neural Machine Translation", "cites": "532", "abstract": "Recent work on end-to-end neural network-based architectures for machine\ntranslation has shown promising results for En-Fr and En-De translation.\nArguably, one of the major factors behind this success has been the\navailability of high quality parallel corpora. In this work, we investigate how\nto leverage abundant monolingual corpora for neural machine translation.\nCompared to a phrase-based and hierarchical baseline, we obtain up to $1.96$\nBLEU improvement on the low-resource language pair Turkish-English, and $1.59$\nBLEU on the focused domain task of Chinese-English chat messages. While our\nmethod was initially targeted toward such tasks with less parallel data, we\nshow that it also extends to high resource languages such as Cs-En and De-En\nwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural\nmachine translation baselines, respectively.", "no": 68}, {"url": "https://arxiv.org/abs/1508.00305", "title": "Compositional Semantic Parsing on Semi-Structured Tables", "cites": "529", "abstract": "Two important aspects of semantic parsing for question answering are the\nbreadth of the knowledge source and the depth of logical compositionality.\nWhile existing work trades off one aspect for another, this paper\nsimultaneously makes progress on both fronts through a new task: answering\ncomplex questions on semi-structured tables using question-answer pairs as\nsupervision. The central challenge arises from two compounding factors: the\nbroader domain results in an open-ended set of relations, and the deeper\ncompositionality results in a combinatorial explosion in the space of logical\nforms. We propose a logical-form driven parsing algorithm guided by strong\ntyping constraints and show that it obtains significant improvements over\nnatural baselines. For evaluation, we created a new dataset of 22,033 complex\nquestions on Wikipedia tables, which is made publicly available.", "no": 69}, {"url": "https://arxiv.org/abs/1502.01710", "title": "Text Understanding from Scratch", "cites": "520", "abstract": "This article demontrates that we can apply deep learning to text\nunderstanding from character-level inputs all the way up to abstract text\nconcepts, using temporal convolutional networks (ConvNets). We apply ConvNets\nto various large-scale datasets, including ontology classification, sentiment\nanalysis, and text categorization. We show that temporal ConvNets can achieve\nastonishing performance without the knowledge of words, phrases, sentences and\nany other syntactic or semantic structures with regards to a human language.\nEvidence shows that our models can work for both English and Chinese.", "no": 70}, {"url": "https://arxiv.org/abs/1511.04164", "title": "Natural Language Object Retrieval", "cites": "515", "abstract": "In this paper, we address the task of natural language object retrieval, to\nlocalize a target object within a given image based on a natural language query\nof the object. Natural language object retrieval differs from text-based image\nretrieval task as it involves spatial information about objects within the\nscene and global scene context. To address this issue, we propose a novel\nSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate\nboxes for object retrieval, integrating spatial configurations and global\nscene-level contextual information into the network. Our model processes query\ntext, local image descriptors, spatial configurations and global context\nfeatures through a recurrent network, outputs the probability of the query text\nconditioned on each candidate box as a score for the box, and can transfer\nvisual-linguistic knowledge from image captioning domain to our task.\nExperimental results demonstrate that our method effectively utilizes both\nlocal and global information, outperforming previous baseline methods\nsignificantly on different datasets and scenarios, and can exploit large scale\nvision and language datasets for knowledge transfer.", "no": 71}, {"url": "https://arxiv.org/abs/1511.06361", "title": "Order-Embeddings of Images and Language", "cites": "500", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval.", "no": 72}, {"url": "https://arxiv.org/abs/1505.05612", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering", "cites": "468", "abstract": "In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html", "no": 73}, {"url": "https://arxiv.org/abs/1511.03745", "title": "Grounding of Textual Phrases in Images by Reconstruction", "cites": "457", "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets.", "no": 74}, {"url": "https://arxiv.org/abs/1504.06654", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in\n  Vector Space", "cites": "454", "abstract": "There is rising interest in vector-space word embeddings and their use in\nNLP, especially given recent methods for their fast estimation at very large\nscale. Nearly all this work, however, assumes a single vector per word type\nignoring polysemy and thus jeopardizing their usefulness for downstream tasks.\nWe present an extension to the Skip-gram model that efficiently learns multiple\nembeddings per word type. It differs from recent related work by jointly\nperforming word sense discrimination and embedding learning, by\nnon-parametrically estimating the number of senses per word type, and by its\nefficiency and scalability. We present new state-of-the-art results in the word\nsimilarity in context task and demonstrate its scalability by training with one\nmachine on a corpus of nearly 1 billion tokens in less than 6 hours.", "no": 75}, {"url": "https://arxiv.org/abs/1507.00955", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "cites": "447", "abstract": "This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.", "no": 76}, {"url": "https://arxiv.org/abs/1512.02433", "title": "Minimum Risk Training for Neural Machine Translation", "cites": "432", "abstract": "We propose minimum risk training for end-to-end neural machine translation.\nUnlike conventional maximum likelihood estimation, minimum risk training is\ncapable of optimizing model parameters directly with respect to arbitrary\nevaluation metrics, which are not necessarily differentiable. Experiments show\nthat our approach achieves significant improvements over maximum likelihood\nestimation on a state-of-the-art neural machine translation system across\nvarious languages pairs. Transparent to architectures, our approach can be\napplied to more neural networks and potentially benefit more NLP tasks.", "no": 77}, {"url": "https://arxiv.org/abs/1507.06947", "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\n  Recognition", "cites": "428", "abstract": "We have recently shown that deep Long Short-Term Memory (LSTM) recurrent\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\nacoustic models for speech recognition. More recently, we have shown that the\nperformance of sequence trained context dependent (CD) hidden Markov model\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\nphone models initialized with connectionist temporal classification (CTC). In\nthis paper, we present techniques that further improve performance of LSTM RNN\nacoustic models for large vocabulary speech recognition. We show that frame\nstacking and reduced frame rate lead to more accurate models and faster\ndecoding. CD phone modeling leads to further improvements. We also present\ninitial results for LSTM RNN models outputting words directly.", "no": 78}, {"url": "https://arxiv.org/abs/1511.04108", "title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "cites": "425", "abstract": "In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.", "no": 79}, {"url": "https://arxiv.org/abs/1501.02530", "title": "A Dataset for Movie Description", "cites": "418", "abstract": "Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production.", "no": 80}, {"url": "https://arxiv.org/abs/1512.08849", "title": "Learning Natural Language Inference with LSTM", "cites": "407", "abstract": "Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.", "no": 81}, {"url": "https://arxiv.org/abs/1507.01053", "title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "cites": "382", "abstract": "Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.", "no": 82}, {"url": "https://arxiv.org/abs/1512.01818", "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment\n  analysis methods", "cites": "382", "abstract": "In the last few years thousands of scientific papers have investigated\nsentiment analysis, several startups that measure opinions on real data have\nemerged and a number of innovative products related to this theme have been\ndeveloped. There are multiple methods for measuring sentiments, including\nlexical-based and supervised machine learning methods. Despite the vast\ninterest on the theme and wide popularity of some methods, it is unclear which\none is better for identifying the polarity (i.e., positive or negative) of a\nmessage. Accordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, \\textit{as they are\nused in practice}, across multiple datasets originated from different data\nsources. Such a comparison is key for understanding the potential limitations,\nadvantages, and disadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-four popular sentiment\nanalysis methods (which we call the state-of-the-practice methods). Our\nevaluation is based on a benchmark of eighteen labeled datasets, covering\nmessages posted on social networks, movie and product reviews, as well as\nopinions and comments in news articles. Our results highlight the extent to\nwhich the prediction performance of these methods varies considerably across\ndatasets. Aiming at boosting the development of this research area, we open the\nmethods' codes and datasets used in this article, deploying them in a benchmark\nsystem, which provides an open API for accessing and comparing sentence-level\nsentiment analysis methods.", "no": 83}, {"url": "https://arxiv.org/abs/1508.01585", "title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "cites": "366", "abstract": "We apply a general deep learning framework to address the non-factoid\nquestion answering task. Our approach does not rely on any linguistic tools and\ncan be applied to different languages or domains. Various architectures are\npresented and compared. We create and release a QA corpus and setup a new QA\ntask in the insurance domain. Experimental results demonstrate superior\nperformance compared to the baseline methods and various technologies give\nfurther improvements. For this highly challenging task, the top-1 accuracy can\nreach up to 65.3% on a test set, which indicates a great potential for\npractical use.", "no": 84}, {"url": "https://arxiv.org/abs/1507.07998", "title": "Document Embedding with Paragraph Vectors", "cites": "347", "abstract": "Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.", "no": 85}, {"url": "https://arxiv.org/abs/1507.01526", "title": "Grid Long Short-Term Memory", "cites": "346", "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.", "no": 86}, {"url": "https://arxiv.org/abs/1507.01701", "title": "A Survey and Classification of Controlled Natural Languages", "cites": "345", "abstract": "What is here called controlled natural language (CNL) has traditionally been\ngiven many different names. Especially during the last four decades, a wide\nvariety of such languages have been designed. They are applied to improve\ncommunication among humans, to improve translation, or to provide natural and\nintuitive representations for formal notations. Despite the apparent\ndifferences, it seems sensible to put all these languages under the same\numbrella. To bring order to the variety of languages, a general classification\nscheme is presented here. A comprehensive survey of existing English-based CNLs\nis given, listing and describing 100 languages from 1930 until today.\nClassification of these languages reveals that they form a single scattered\ncloud filling the conceptual space between natural languages such as English on\nthe one end and formal languages such as propositional logic on the other. The\ngoal of this article is to provide a common terminology and a common model for\nCNL, to contribute to the understanding of their general nature, to provide a\nstarting point for researchers interested in the area, and to help developers\nto make design decisions.", "no": 87}, {"url": "https://arxiv.org/abs/1512.08422", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic\n  Matching", "cites": "338", "abstract": "In this paper, we propose the TBCNN-pair model to recognize entailment and\ncontradiction between two sentences. In our model, a tree-based convolutional\nneural network (TBCNN) captures sentence-level semantics; then heuristic\nmatching layers like concatenation, element-wise product/difference combine the\ninformation in individual sentences. Experimental results show that our model\noutperforms existing sentence encoding-based approaches by a large margin.", "no": 88}, {"url": "https://arxiv.org/abs/1506.01094", "title": "Traversing Knowledge Graphs in Vector Space", "cites": "337", "abstract": "Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.", "no": 89}, {"url": "https://arxiv.org/abs/1505.05008", "title": "Boosting Named Entity Recognition with Neural Character Embeddings", "cites": "333", "abstract": "Most state-of-the-art named entity recognition (NER) systems rely on\nhandcrafted features and on the output of other NLP tasks such as\npart-of-speech (POS) tagging and text chunking. In this work we propose a\nlanguage-independent NER system that uses automatically learned features only.\nOur approach is based on the CharWNN deep neural network, which uses word-level\nand character-level representations (embeddings) to perform sequential\nclassification. We perform an extensive number of experiments using two\nannotated corpora in two different languages: HAREM I corpus, which contains\ntexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in\nSpanish. Our experimental results shade light on the contribution of neural\ncharacter embeddings for NER. Moreover, we demonstrate that the same neural\nnetwork which has been successfully applied to POS tagging can also achieve\nstate-of-the-art results for language-independet NER, using the same\nhyperparameters, and without any handcrafted features. For the HAREM I corpus,\nCharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score\nfor the total scenario (ten NE classes), and by 7.2 points in the F1 for the\nselective scenario (five NE classes).", "no": 90}, {"url": "https://arxiv.org/abs/1512.05742", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "cites": "326", "abstract": "During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.", "no": 91}, {"url": "https://arxiv.org/abs/1511.08277", "title": "A Deep Architecture for Semantic Matching with Multiple Positional\n  Sentence Representations", "cites": "324", "abstract": "Matching natural language sentences is central for many applications such as\ninformation retrieval and question answering. Existing deep models rely on a\nsingle sentence representation or multiple granularity representations for\nmatching. However, such methods cannot well capture the contextualized local\ninformation in the matching process. To tackle this problem, we present a new\ndeep architecture to match two sentences with multiple positional sentence\nrepresentations. Specifically, each positional sentence representation is a\nsentence representation at this position, generated by a bidirectional long\nshort term memory (Bi-LSTM). The matching score is finally produced by\naggregating interactions between these different positional sentence\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\nsentence is leveraged to capture the contextualized local information in each\npositional sentence representation; (2) By matching with multiple positional\nsentence representations, it is flexible to aggregate different important\ncontextualized local information in a sentence to support the matching; (3)\nExperiments on different tasks such as question answering and sentence\ncompletion demonstrate the superiority of our model.", "no": 92}, {"url": "https://arxiv.org/abs/1508.01006", "title": "Relation Classification via Recurrent Neural Network", "cites": "323", "abstract": "Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.", "no": 93}, {"url": "https://arxiv.org/abs/1506.08941", "title": "Language Understanding for Text-based Games Using Deep Reinforcement\n  Learning", "cites": "321", "abstract": "In this paper, we consider the task of learning control policies for\ntext-based games. In these games, all interactions in the virtual world are\nthrough text and the underlying state is not observed. The resulting language\nbarrier makes such environments challenging for automatic game players. We\nemploy a deep reinforcement learning framework to jointly learn state\nrepresentations and action policies using game rewards as feedback. This\nframework enables us to map text descriptions into vector representations that\ncapture the semantics of the game states. We evaluate our approach on two game\nworlds, comparing against baselines using bag-of-words and bag-of-bigrams for\nstate representations. Our algorithm outperforms the baselines on both worlds\ndemonstrating the importance of learning expressive representations.", "no": 94}, {"url": "https://arxiv.org/abs/1507.05523", "title": "How to Generate a Good Word Embedding?", "cites": "319", "abstract": "We analyze three critical components of word embedding training: the model,\nthe corpus, and the training parameters. We systematize existing\nneural-network-based word embedding algorithms and compare them using the same\ncorpus. We evaluate each word embedding in three ways: analyzing its semantic\nproperties, using it as a feature for supervised tasks and using it to\ninitialize neural networks. We also provide several simple guidelines for\ntraining word embeddings. First, we discover that corpus domain is more\nimportant than corpus size. We recommend choosing a corpus in a suitable domain\nfor the desired task, after that, using a larger corpus yields better results.\nSecond, we find that faster models provide sufficient performance in most\ncases, and more complex models can be used if the training corpus is\nsufficiently large. Third, the early stopping metric for iterating should rely\non the development set of the desired task rather than the validation loss of\ntraining embedding.", "no": 95}, {"url": "https://arxiv.org/abs/1504.06063", "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence", "cites": "318", "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs)\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\nwith convolutional architectures to exploit image representation, word\ncomposition, and the matching relations between the two modalities. More\nspecifically, it consists of one image CNN encoding the image content, and one\nmatching CNN learning the joint representation of image and sentence. The\nmatching CNN composes words to different semantic fragments and learns the\ninter-modal relations between image and the composed fragments at different\nlevels, thus fully exploit the matching relations between image and sentence.\nExperimental results on benchmark databases of bidirectional image and sentence\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\ninformation necessary for image and sentence matching. Specifically, our\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\nMicrosoft COCO databases achieve the state-of-the-art performances.", "no": 96}, {"url": "https://arxiv.org/abs/1511.05756", "title": "Image Question Answering using Convolutional Neural Network with Dynamic\n  Parameter Prediction", "cites": "318", "abstract": "We tackle image question answering (ImageQA) problem by learning a\nconvolutional neural network (CNN) with a dynamic parameter layer whose weights\nare determined adaptively based on questions. For the adaptive parameter\nprediction, we employ a separate parameter prediction network, which consists\nof gated recurrent unit (GRU) taking a question as its input and a\nfully-connected layer generating a set of candidate weights as its output.\nHowever, it is challenging to construct a parameter prediction network for a\nlarge number of parameters in the fully-connected dynamic parameter layer of\nthe CNN. We reduce the complexity of this problem by incorporating a hashing\ntechnique, where the candidate weights given by the parameter prediction\nnetwork are selected using a predefined hash function to determine individual\nweights in the dynamic parameter layer. The proposed network---joint network\nwith the CNN for ImageQA and the parameter prediction network---is trained\nend-to-end through back-propagation, where its weights are initialized using a\npre-trained CNN and GRU. The proposed algorithm illustrates the\nstate-of-the-art performance on all available public ImageQA benchmarks.", "no": 97}, {"url": "https://arxiv.org/abs/1512.02167", "title": "Simple Baseline for Visual Question Answering", "cites": "306", "abstract": "We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. .", "no": 98}, {"url": "https://arxiv.org/abs/1511.05099", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "cites": "299", "abstract": "The complex compositional structure of language makes problems at the\nintersection of vision and language challenging. But language also provides a\nstrong prior that can result in good superficial performance, without the\nunderlying models truly understanding the visual content. This can hinder\nprogress in pushing state of art in the computer vision aspects of multi-modal\nAI. In this paper, we address binary Visual Question Answering (VQA) on\nabstract scenes. We formulate this problem as visual verification of concepts\ninquired in the questions. Specifically, we convert the question to a tuple\nthat concisely summarizes the visual concept to be detected in the image. If\nthe concept can be found in the image, the answer to the question is \"yes\", and\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\nthe high-level semantics of the VQA task as opposed to the low-level\nrecognition problems, and perhaps more importantly, (2) They provide us the\nmodality to balance the dataset such that language priors are controlled, and\nthe role of vision is essential. In particular, we collect fine-grained pairs\nof scenes for every question, such that the answer to the question is \"yes\" for\none scene, and \"no\" for the other for the exact same question. Indeed, language\npriors alone do not perform better than chance on our balanced dataset.\nMoreover, our proposed approach matches the performance of a state-of-the-art\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\ndataset.", "no": 99}, {"url": "https://arxiv.org/abs/1508.00657", "title": "Improved Transition-Based Parsing by Modeling Characters instead of\n  Words with LSTMs", "cites": "296", "abstract": "We present extensions to a continuous-state dependency parsing method that\nmakes it applicable to morphologically rich languages. Starting with a\nhigh-performance transition-based parser that uses long short-term memory\n(LSTM) recurrent neural networks to learn representations of the parser state,\nwe replace lookup-based word representations with representations constructed\nfrom the orthographic representations of the words, also using LSTMs. This\nallows statistical sharing across word forms that are similar on the surface.\nExperiments for morphologically rich languages show that the parsing model\nbenefits from incorporating the character-based encodings of words.", "no": 100}]