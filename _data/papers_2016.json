[{"url": "https://arxiv.org/abs/1607.04606", "title": "Enriching Word Vectors with Subword Information", "cites": "9 521", "abstract": "Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks.", "no": 1}, {"url": "https://arxiv.org/abs/1606.05250", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "cites": "7 529", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading\ncomprehension dataset consisting of 100,000+ questions posed by crowdworkers on\na set of Wikipedia articles, where the answer to each question is a segment of\ntext from the corresponding reading passage. We analyze the dataset to\nunderstand the types of reasoning required to answer the questions, leaning\nheavily on dependency and constituency trees. We build a strong logistic\nregression model, which achieves an F1 score of 51.0%, a significant\nimprovement over a simple baseline (20%). However, human performance (86.8%) is\nmuch higher, indicating that the dataset presents a good challenge problem for\nfuture research.\n  The dataset is freely available at https://stanford-qa.com", "no": 2}, {"url": "https://arxiv.org/abs/1609.08144", "title": "Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation", "cites": "6 531", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.", "no": 3}, {"url": "https://arxiv.org/abs/1607.01759", "title": "Bag of Tricks for Efficient Text Classification", "cites": "4 399", "abstract": "This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.", "no": 4}, {"url": "https://arxiv.org/abs/1603.01360", "title": "Neural Architectures for Named Entity Recognition", "cites": "3 889", "abstract": "State-of-the-art named entity recognition systems rely heavily on\nhand-crafted features and domain-specific knowledge in order to learn\neffectively from the small, supervised training corpora that are available. In\nthis paper, we introduce two new neural architectures---one based on\nbidirectional LSTMs and conditional random fields, and the other that\nconstructs and labels segments using a transition-based approach inspired by\nshift-reduce parsers. Our models rely on two sources of information about\nwords: character-based word representations learned from the supervised corpus\nand unsupervised word representations learned from unannotated corpora. Our\nmodels obtain state-of-the-art performance in NER in four languages without\nresorting to any language-specific knowledge or resources such as gazetteers.", "no": 5}, {"url": "https://arxiv.org/abs/1607.06520", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "cites": "2 865", "abstract": "The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.", "no": 6}, {"url": "https://arxiv.org/abs/1612.00837", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "cites": "2 769", "abstract": "Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.", "no": 7}, {"url": "https://arxiv.org/abs/1612.03975", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "cites": "2 672", "abstract": "Machine learning about language can be improved by supplying it with specific\nknowledge and sources of external information. We present here a new version of\nthe linked open data resource ConceptNet that is particularly well suited to be\nused with modern NLP techniques such as word embeddings.\n  ConceptNet is a knowledge graph that connects words and phrases of natural\nlanguage with labeled edges. Its knowledge is collected from many sources that\ninclude expert-created resources, crowd-sourcing, and games with a purpose. It\nis designed to represent the general knowledge involved in understanding\nlanguage, improving natural language applications by allowing the application\nto better understand the meanings behind the words people use.\n  When ConceptNet is combined with word embeddings acquired from distributional\nsemantics (such as word2vec), it provides applications with understanding that\nthey would not acquire from distributional semantics alone, nor from narrower\nresources such as WordNet or DBPedia. We demonstrate this with state-of-the-art\nresults on intrinsic evaluations of word relatedness that translate into\nimprovements on applications of word vectors, including solving SAT-style\nanalogies.", "no": 8}, {"url": "https://arxiv.org/abs/1603.01354", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "cites": "2 569", "abstract": "State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER.", "no": 9}, {"url": "https://arxiv.org/abs/1608.07187", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "cites": "2 452", "abstract": "Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.", "no": 10}, {"url": "https://arxiv.org/abs/1611.09268", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "cites": "2 413", "abstract": "We introduce a large scale MAchine Reading COmprehension dataset, which we\nname MS MARCO. The dataset comprises of 1,010,916 anonymized\nquestions---sampled from Bing's search query logs---each with a human generated\nanswer and 182,669 completely human rewritten generated answers. In addition,\nthe dataset contains 8,841,823 passages---extracted from 3,563,535 web\ndocuments retrieved by Bing---that provide the information necessary for\ncurating the natural language answers. A question in the MS MARCO dataset may\nhave multiple answers or no answers at all. Using this dataset, we propose\nthree different tasks with varying levels of difficulty: (i) predict if a\nquestion is answerable given a set of context passages, and extract and\nsynthesize the answer as a human would (ii) generate a well-formed answer (if\npossible) based on the context passages that can be understood with the\nquestion and passage context, and finally (iii) rank a set of retrieved\npassages given a question. The size of the dataset and the fact that the\nquestions are derived from real user search queries distinguishes MS MARCO from\nother well-known publicly available datasets for machine reading comprehension\nand question-answering. We believe that the scale and the real-world nature of\nthis dataset makes it attractive for benchmarking machine reading comprehension\nand question-answering models.", "no": 11}, {"url": "https://arxiv.org/abs/1602.06023", "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and\n  Beyond", "cites": "2 407", "abstract": "In this work, we model abstractive text summarization using Attentional\nEncoder-Decoder Recurrent Neural Networks, and show that they achieve\nstate-of-the-art performance on two different corpora. We propose several novel\nmodels that address critical problems in summarization that are not adequately\nmodeled by the basic architecture, such as modeling key-words, capturing the\nhierarchy of sentence-to-word structure, and emitting words that are rare or\nunseen at training time. Our work shows that many of our proposed models\ncontribute to further improvement in performance. We also propose a new dataset\nconsisting of multi-sentence summaries, and establish performance benchmarks\nfor further research.", "no": 12}, {"url": "https://arxiv.org/abs/1609.07843", "title": "Pointer Sentinel Mixture Models", "cites": "2 378", "abstract": "Recent neural network sequence models with softmax classifiers have achieved\ntheir best language modeling performance only with very large hidden states and\nlarge vocabularies. Even then they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer\nsentinel mixture architecture for neural sequence models which has the ability\nto either reproduce a word from the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinel-LSTM model achieves state of\nthe art language modeling performance on the Penn Treebank (70.9 perplexity)\nwhile using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit longer contexts and deal with\nmore realistic vocabularies and larger corpora we also introduce the freely\navailable WikiText corpus.", "no": 13}, {"url": "https://arxiv.org/abs/1612.08083", "title": "Language Modeling with Gated Convolutional Networks", "cites": "2 210", "abstract": "The pre-dominant approach to language modeling to date is based on recurrent\nneural networks. Their success on this task is often linked to their ability to\ncapture unbounded context. In this paper we develop a finite context approach\nthrough stacked convolutions, which can be more efficient since they allow\nparallelization over sequential tokens. We propose a novel simplified gating\nmechanism that outperforms Oord et al (2016) and investigate the impact of key\narchitectural decisions. The proposed approach achieves state-of-the-art on the\nWikiText-103 benchmark, even though it features long-term dependencies, as well\nas competitive results on the Google Billion Words benchmark. Our model reduces\nthe latency to score a sentence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the first time a non-recurrent\napproach is competitive with strong recurrent models on these large scale\nlanguage tasks.", "no": 14}, {"url": "https://arxiv.org/abs/1612.06890", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "cites": "2 142", "abstract": "When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.", "no": 15}, {"url": "https://arxiv.org/abs/1611.01603", "title": "Bidirectional Attention Flow for Machine Comprehension", "cites": "2 063", "abstract": "Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest.", "no": 16}, {"url": "https://arxiv.org/abs/1611.04558", "title": "Google's Multilingual Neural Machine Translation System: Enabling\n  Zero-Shot Translation", "cites": "2 007", "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT)\nmodel to translate between multiple languages. Our solution requires no change\nin the model architecture from our base system but instead introduces an\nartificial token at the beginning of the input sentence to specify the required\ntarget language. The rest of the model, which includes encoder, decoder and\nattention, remains unchanged and is shared across all languages. Using a shared\nwordpiece vocabulary, our approach enables Multilingual NMT using a single\nmodel without any increase in parameters, which is significantly simpler than\nprevious proposals for Multilingual NMT. Our method often improves the\ntranslation quality of all involved language pairs, even while keeping the\ntotal number of model parameters constant. On the WMT'14 benchmarks, a single\nmultilingual model achieves comparable performance for\nEnglish$\\rightarrow$French and surpasses state-of-the-art results for\nEnglish$\\rightarrow$German. Similarly, a single multilingual model surpasses\nstate-of-the-art results for French$\\rightarrow$English and\nGerman$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On\nproduction corpora, multilingual models of up to twelve language pairs allow\nfor better translation of many individual pairs. In addition to improving the\ntranslation quality of language pairs that the model was trained with, our\nmodels can also learn to perform implicit bridging between language pairs never\nseen explicitly during training, showing that transfer learning and zero-shot\ntranslation is possible for neural translation. Finally, we show analyses that\nhints at a universal interlingua representation in our models and show some\ninteresting examples when mixing languages.", "no": 17}, {"url": "https://arxiv.org/abs/1607.08822", "title": "SPICE: Semantic Propositional Image Caption Evaluation", "cites": "1 733", "abstract": "There is considerable interest in the task of automatically generating image\ncaptions. However, evaluation is challenging. Existing automatic evaluation\nmetrics are primarily sensitive to n-gram overlap, which is neither necessary\nnor sufficient for the task of simulating human judgment. We hypothesize that\nsemantic propositional content is an important component of human caption\nevaluation, and propose a new automated caption evaluation metric defined over\nscene graphs coined SPICE. Extensive evaluations across a range of models and\ndatasets indicate that SPICE captures human judgments over model-generated\ncaptions better than other automatic metrics (e.g., system-level correlation of\n0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and\n0.53 for METEOR). Furthermore, SPICE can answer questions such as `which\ncaption-generator best understands colors?' and `can caption-generators count?'", "no": 18}, {"url": "https://arxiv.org/abs/1606.00061", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "cites": "1 551", "abstract": "A number of recent works have proposed attention models for Visual Question\nAnswering (VQA) that generate spatial maps highlighting image regions relevant\nto answering the question. In this paper, we argue that in addition to modeling\n\"where to look\" or visual attention, it is equally important to model \"what\nwords to listen to\" or question attention. We present a novel co-attention\nmodel for VQA that jointly reasons about image and question attention. In\naddition, our model reasons about the question (and consequently the image via\nthe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional\nconvolution neural networks (CNN). Our model improves the state-of-the-art on\nthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA\ndataset. By using ResNet, the performance is further improved to 62.1% for VQA\nand 65.4% for COCO-QA.", "no": 19}, {"url": "https://arxiv.org/abs/1603.06393", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "cites": "1 512", "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.", "no": 20}, {"url": "https://arxiv.org/abs/1606.01847", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and\n  Visual Grounding", "cites": "1 426", "abstract": "Modeling textual or visual information with vector representations trained\nfrom large language or visual datasets has been successfully explored in recent\nyears. However, tasks such as visual question answering require combining these\nvector representations with each other. Approaches to multimodal pooling\ninclude element-wise product or sum, as well as concatenation of the visual and\ntextual representations. We hypothesize that these methods are not as\nexpressive as an outer product of the visual and textual vectors. As the outer\nproduct is typically infeasible due to its high dimensionality, we instead\npropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and\nexpressively combine multimodal features. We extensively evaluate MCB on the\nvisual question answering and grounding tasks. We consistently show the benefit\nof MCB over ablations without MCB. For visual question answering, we present an\narchitecture which uses MCB twice, once for predicting attention over spatial\nfeatures and again to combine the attended representation with the question\nrepresentation. This model outperforms the state-of-the-art on the Visual7W\ndataset and the VQA challenge.", "no": 21}, {"url": "https://arxiv.org/abs/1606.01933", "title": "A Decomposable Attention Model for Natural Language Inference", "cites": "1 340", "abstract": "We propose a simple neural architecture for natural language inference. Our\napproach uses attention to decompose the problem into subproblems that can be\nsolved separately, thus making it trivially parallelizable. On the Stanford\nNatural Language Inference (SNLI) dataset, we obtain state-of-the-art results\nwith almost an order of magnitude fewer parameters than previous work and\nwithout relying on any word-order information. Adding intra-sentence attention\nthat takes a minimum amount of order into account yields further improvements.", "no": 22}, {"url": "https://arxiv.org/abs/1606.01541", "title": "Deep Reinforcement Learning for Dialogue Generation", "cites": "1 301", "abstract": "Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.", "no": 23}, {"url": "https://arxiv.org/abs/1603.08023", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "cites": "1 266", "abstract": "We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.", "no": 24}, {"url": "https://arxiv.org/abs/1605.05101", "title": "Recurrent Neural Network for Text Classification with Multi-Task\n  Learning", "cites": "1 228", "abstract": "Neural network based methods have obtained great progress on a variety of\nnatural language processing tasks. However, in most previous works, the models\nare learned based on single-task supervised objectives, which often suffer from\ninsufficient training data. In this paper, we use the multi-task learning\nframework to jointly learn across multiple related tasks. Based on recurrent\nneural network, we propose three different mechanisms of sharing information to\nmodel text with task-specific and shared layers. The entire network is trained\njointly on all these tasks. Experiments on four benchmark text classification\ntasks show that our proposed models can improve the performance of a task with\nthe help of other related tasks.", "no": 25}, {"url": "https://arxiv.org/abs/1611.04230", "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for\n  Extractive Summarization of Documents", "cites": "1 220", "abstract": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model\nfor extractive summarization of documents and show that it achieves performance\nbetter than or comparable to state-of-the-art. Our model has the additional\nadvantage of being very interpretable, since it allows visualization of its\npredictions broken up by abstract features such as information content,\nsalience and novelty. Another novel contribution of our work is abstractive\ntraining of our extractive model that can train on human generated reference\nsummaries alone, eliminating the need for sentence-level extractive labels.", "no": 26}, {"url": "https://arxiv.org/abs/1611.01734", "title": "Deep Biaffine Attention for Neural Dependency Parsing", "cites": "1 182", "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches.", "no": 27}, {"url": "https://arxiv.org/abs/1601.00770", "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree\n  Structures", "cites": "1 136", "abstract": "We present a novel end-to-end neural model to extract entities and relations\nbetween them. Our recurrent neural network based model captures both word\nsequence and dependency tree substructure information by stacking bidirectional\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\nour model to jointly represent both entities and relations with shared\nparameters in a single model. We further encourage detection of entities during\ntraining and use of entity information in relation extraction via entity\npretraining and scheduled sampling. Our model improves over the\nstate-of-the-art feature-based model on end-to-end relation extraction,\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\nACE2004, respectively. We also show that our LSTM-RNN based model compares\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\nablation analysis of several model components.", "no": 28}, {"url": "https://arxiv.org/abs/1612.03651", "title": "FastText.zip: Compressing text classification models", "cites": "1 126", "abstract": "We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.", "no": 29}, {"url": "https://arxiv.org/abs/1602.02410", "title": "Exploring the Limits of Language Modeling", "cites": "1 122", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.", "no": 30}, {"url": "https://arxiv.org/abs/1609.06038", "title": "Enhanced LSTM for Natural Language Inference", "cites": "1 093", "abstract": "Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result---it further improves the performance even when\nadded to the already very strong model.", "no": 31}, {"url": "https://arxiv.org/abs/1605.06069", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating\n  Dialogues", "cites": "1 088", "abstract": "Sequential data often possesses a hierarchical structure with complex\ndependencies between subsequences, such as found between the utterances in a\ndialogue. In an effort to model this kind of generative process, we propose a\nneural network-based generative architecture, with latent stochastic variables\nthat span a variable number of time steps. We apply the proposed model to the\ntask of dialogue response generation and compare it with recent neural network\narchitectures. We evaluate the model performance through automatic evaluation\nmetrics and by carrying out a human evaluation. The experiments demonstrate\nthat our model improves upon recently proposed models and that the latent\nvariables facilitate the generation of long outputs and maintain the context.", "no": 32}, {"url": "https://arxiv.org/abs/1608.00272", "title": "Modeling Context in Referring Expressions", "cites": "1 088", "abstract": "Humans refer to objects in their environments all the time, especially in\ndialogue with other people. We explore generating and comprehending natural\nlanguage referring expressions for objects in images. In particular, we focus\non incorporating better measures of visual context into referring expression\nmodels and find that visual comparison to other objects within an image helps\nimprove performance significantly. We also develop methods to tie the language\ngeneration process together, so that we generate expressions for all objects of\na particular category jointly. Evaluation on three recent datasets - RefCOCO,\nRefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring\nexpression generation and comprehension.", "no": 33}, {"url": "https://arxiv.org/abs/1604.04562", "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "cites": "1 075", "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.", "no": 34}, {"url": "https://arxiv.org/abs/1601.06733", "title": "Long Short-Term Memory-Networks for Machine Reading", "cites": "1 068", "abstract": "In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.", "no": 35}, {"url": "https://arxiv.org/abs/1606.07947", "title": "Sequence-Level Knowledge Distillation", "cites": "1 036", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.", "no": 36}, {"url": "https://arxiv.org/abs/1603.06155", "title": "A Persona-Based Neural Conversation Model", "cites": "1 015", "abstract": "We present persona-based models for handling the issue of speaker consistency\nin neural response generation. A speaker model encodes personas in distributed\nembeddings that capture individual characteristics such as background\ninformation and speaking style. A dyadic speaker-addressee model captures\nproperties of interactions between two interlocutors. Our models yield\nqualitative performance improvements in both perplexity and BLEU scores over\nbaseline sequence-to-sequence models, with similar gains in speaker consistency\nas measured by human judges.", "no": 37}, {"url": "https://arxiv.org/abs/1611.08669", "title": "Visual Dialog", "cites": "948", "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org", "no": 38}, {"url": "https://arxiv.org/abs/1606.03126", "title": "Key-Value Memory Networks for Directly Reading Documents", "cites": "916", "abstract": "Directly reading documents and being able to answer questions from them is an\nunsolved challenge. To avoid its inherent difficulty, question answering (QA)\nhas been directed towards using Knowledge Bases (KBs) instead, which has proven\neffective. Unfortunately KBs often suffer from being too restrictive, as the\nschema cannot support certain types of answers, and too sparse, e.g. Wikipedia\ncontains much more information than Freebase. In this work we introduce a new\nmethod, Key-Value Memory Networks, that makes reading documents more viable by\nutilizing different encodings in the addressing and output stages of the memory\nread operation. To compare using KBs, information extraction or Wikipedia\ndocuments directly in a single framework we construct an analysis tool,\nWikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in\nthe domain of movies. Our method reduces the gap between all three settings. It\nalso achieves state-of-the-art results on the existing WikiQA benchmark.", "no": 39}, {"url": "https://arxiv.org/abs/1605.09096", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "cites": "891", "abstract": "Understanding how words change their meanings over time is key to models of\nlanguage and cultural evolution, but historical data on meaning is scarce,\nmaking theories hard to develop and test. Word embeddings show promise as a\ndiachronic tool, but have not been carefully evaluated. We develop a robust\nmethodology for quantifying semantic change by evaluating word embeddings\n(PPMI, SVD, word2vec) against known historical changes. We then use this\nmethodology to reveal statistical laws of semantic evolution. Using six\nhistorical corpora spanning four languages and two centuries, we propose two\nquantitative laws of semantic change: (i) the law of conformity---the rate of\nsemantic change scales with an inverse power-law of word frequency; (ii) the\nlaw of innovation---independent of frequency, words that are more polysemous\nhave higher rates of semantic change.", "no": 40}, {"url": "https://arxiv.org/abs/1609.06773", "title": "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task\n  Learning", "cites": "891", "abstract": "Recently, there has been an increasing interest in end-to-end speech\nrecognition that directly transcribes speech to text without any predefined\nalignments. One approach is the attention-based encoder-decoder framework that\nlearns a mapping between variable-length input and output sequences in one step\nusing a purely data-driven method. The attention model has often been shown to\nimprove the performance over another end-to-end approach, the Connectionist\nTemporal Classification (CTC), mainly because it explicitly uses the history of\nthe target character without any conditional independence assumptions. However,\nwe observed that the performance of the attention has shown poor results in\nnoisy condition and is hard to learn in the initial training stage with long\ninput sequences. This is because the attention model is too flexible to predict\nproper alignments in such cases due to the lack of left-to-right constraints as\nused in CTC. This paper presents a novel method for end-to-end speech\nrecognition to improve robustness and achieve fast convergence by using a joint\nCTC-attention model within the multi-task learning framework, thereby\nmitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks\ndemonstrates its advantages over both the CTC and attention-based\nencoder-decoder baselines, showing 5.4-14.6% relative improvements in Character\nError Rate (CER).", "no": 41}, {"url": "https://arxiv.org/abs/1605.08900", "title": "Aspect Level Sentiment Classification with Deep Memory Network", "cites": "870", "abstract": "We introduce a deep memory network for aspect level sentiment classification.\nUnlike feature-based SVM and sequential neural models such as LSTM, this\napproach explicitly captures the importance of each context word when inferring\nthe sentiment polarity of an aspect. Such importance degree and text\nrepresentation are calculated with multiple computational layers, each of which\nis a neural attention model over an external memory. Experiments on laptop and\nrestaurant datasets demonstrate that our approach performs comparable to\nstate-of-art feature based SVM system, and substantially better than LSTM and\nattention-based LSTM architectures. On both datasets we show that multiple\ncomputational layers could improve the performance. Moreover, our approach is\nalso fast. The deep memory network with 9 layers is 15 times faster than LSTM\nwith a CPU implementation.", "no": 42}, {"url": "https://arxiv.org/abs/1611.01368", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "cites": "866", "abstract": "The success of long short-term memory (LSTM) neural networks in language\nprocessing is typically attributed to their ability to capture long-distance\nstatistical regularities. Linguistic regularities are often sensitive to\nsyntactic structure; can such dependencies be captured by LSTMs, which do not\nhave explicit structural representations? We begin addressing this question\nusing number agreement in English subject-verb dependencies. We probe the\narchitecture's grammatical competence both using training objectives with an\nexplicit grammatical target (number prediction, grammaticality judgments) and\nusing language models. In the strongly supervised settings, the LSTM achieved\nvery high overall accuracy (less than 1% errors), but errors increased when\nsequential and structural information conflicted. The frequency of such errors\nrose sharply in the language-modeling setting. We conclude that LSTMs can\ncapture a non-trivial amount of grammatical structure given targeted\nsupervision, but stronger architectures may be required to further reduce\nerrors; furthermore, the language modeling signal is insufficient for capturing\nsyntax-sensitive dependencies, and should be supplemented with more direct\nsupervision if such dependencies need to be captured.", "no": 43}, {"url": "https://arxiv.org/abs/1611.09830", "title": "NewsQA: A Machine Comprehension Dataset", "cites": "860", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA.", "no": 44}, {"url": "https://arxiv.org/abs/1611.00179", "title": "Dual Learning for Machine Translation", "cites": "830", "abstract": "While neural machine translation (NMT) is making good progress in the past\ntwo years, tens of millions of bilingual sentence pairs are needed for its\ntraining. However, human labeling is very costly. To tackle this training data\nbottleneck, we develop a dual-learning mechanism, which can enable an NMT\nsystem to automatically learn from unlabeled data through a dual-learning game.\nThis mechanism is inspired by the following observation: any machine\ntranslation task has a dual task, e.g., English-to-French translation (primal)\nversus French-to-English translation (dual); the primal and dual tasks can form\na closed loop, and generate informative feedback signals to train the\ntranslation models, even if without the involvement of a human labeler. In the\ndual-learning mechanism, we use one agent to represent the model for the primal\ntask and the other agent to represent the model for the dual task, then ask\nthem to teach each other through a reinforcement learning process. Based on the\nfeedback signals generated during this process (e.g., the language-model\nlikelihood of the output of a model, and the reconstruction error of the\noriginal sentence after the primal and dual translations), we can iteratively\nupdate the two models until convergence (e.g., using the policy gradient\nmethods). We call the corresponding approach to neural machine translation\n\\emph{dual-NMT}. Experiments show that dual-NMT works very well on\nEnglish$\\leftrightarrow$French translation; especially, by learning from\nmonolingual data (with 10% bilingual data for warm start), it achieves a\ncomparable accuracy to NMT trained from the full bilingual data for the\nFrench-to-English translation task.", "no": 45}, {"url": "https://arxiv.org/abs/1604.02201", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "cites": "823", "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.", "no": 46}, {"url": "https://arxiv.org/abs/1607.00325", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent\n  Multi-talker Speech Separation", "cites": "820", "abstract": "We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common.", "no": 47}, {"url": "https://arxiv.org/abs/1603.07252", "title": "Neural Summarization by Extracting Sentences and Words", "cites": "787", "abstract": "Traditional approaches to extractive summarization rely heavily on\nhuman-engineered features. In this work we propose a data-driven approach based\non neural networks and continuous sentence features. We develop a general\nframework for single-document summarization composed of a hierarchical document\nencoder and an attention-based extractor. This architecture allows us to\ndevelop different classes of summarization models which can extract sentences\nor words. We train our models on large scale corpora containing hundreds of\nthousands of document-summary pairs. Experimental results on two summarization\ndatasets demonstrate that our models obtain results comparable to the state of\nthe art without any access to linguistic annotation.", "no": 48}, {"url": "https://arxiv.org/abs/1606.04155", "title": "Rationalizing Neural Predictions", "cites": "778", "abstract": "Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.", "no": 49}, {"url": "https://arxiv.org/abs/1605.07683", "title": "Learning End-to-End Goal-Oriented Dialog", "cites": "774", "abstract": "Traditional dialog systems used in goal-oriented applications require a lot\nof domain-specific handcrafting, which hinders scaling up to new domains.\nEnd-to-end dialog systems, in which all components are trained from the dialogs\nthemselves, escape this limitation. But the encouraging success recently\nobtained in chit-chat dialog may not carry over to goal-oriented settings. This\npaper proposes a testbed to break down the strengths and shortcomings of\nend-to-end dialog systems in goal-oriented applications. Set in the context of\nrestaurant reservation, our tasks require manipulating sentences and symbols,\nso as to properly conduct conversations, issue API calls and use the outputs of\nsuch calls. We show that an end-to-end dialog system based on Memory Networks\ncan reach promising, yet imperfect, performance and learn to perform\nnon-trivial operations. We confirm those results by comparing our system to a\nhand-crafted slot-filling baseline on data from the second Dialog State\nTracking Challenge (Henderson et al., 2014a). We show similar result patterns\non data extracted from an online concierge service.", "no": 50}, {"url": "https://arxiv.org/abs/1603.01417", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "cites": "744", "abstract": "Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.", "no": 51}, {"url": "https://arxiv.org/abs/1601.04811", "title": "Modeling Coverage for Neural Machine Translation", "cites": "727", "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation\n(NMT) by jointly learning to align and translate. It tends to ignore past\nalignment information, however, which often leads to over-translation and\nunder-translation. To address this problem, we propose coverage-based NMT in\nthis paper. We maintain a coverage vector to keep track of the attention\nhistory. The coverage vector is fed to the attention model to help adjust\nfuture attention, which lets NMT system to consider more about untranslated\nsource words. Experiments show that the proposed approach significantly\nimproves both translation quality and alignment quality over standard\nattention-based NMT.", "no": 52}, {"url": "https://arxiv.org/abs/1610.08914", "title": "Ex Machina: Personal Attacks Seen at Scale", "cites": "722", "abstract": "The damage personal attacks cause to online discourse motivates many\nplatforms to try to curb the phenomenon. However, understanding the prevalence\nand impact of personal attacks in online platforms at scale remains\nsurprisingly difficult. The contribution of this paper is to develop and\nillustrate a method that combines crowdsourcing and machine learning to analyze\npersonal attacks at scale. We show an evaluation method for a classifier in\nterms of the aggregated number of crowd-workers it can approximate. We apply\nour methodology to English Wikipedia, generating a corpus of over 100k high\nquality human-labeled comments and 63M machine-labeled ones from a classifier\nthat is as good as the aggregate of 3 crowd-workers, as measured by the area\nunder the ROC curve and Spearman correlation. Using this corpus of\nmachine-labeled scores, our methodology allows us to explore some of the open\nquestions about the nature of online personal attacks. This reveals that the\nmajority of personal attacks on Wikipedia are not the result of a few malicious\nusers, nor primarily the consequence of allowing anonymous contributions from\nunregistered users.", "no": 53}, {"url": "https://arxiv.org/abs/1601.01280", "title": "Language to Logical Form with Neural Attention", "cites": "717", "abstract": "Semantic parsing aims at mapping natural language to machine interpretable\nmeaning representations. Traditional approaches rely on high-quality lexicons,\nmanually-built templates, and linguistic features which are either domain- or\nrepresentation-specific. In this paper we present a general method based on an\nattention-enhanced encoder-decoder model. We encode input utterances into\nvector representations, and generate their logical forms by conditioning the\noutput sequences or trees on the encoding vectors. Experimental results on four\ndatasets show that our approach performs competitively without using\nhand-engineered features and is easy to adapt across domains and meaning\nrepresentations.", "no": 54}, {"url": "https://arxiv.org/abs/1608.05859", "title": "Using the Output Embedding to Improve Language Models", "cites": "709", "abstract": "We study the topmost weight matrix of neural network language models. We show\nthat this matrix constitutes a valid word embedding. When training language\nmodels, we recommend tying the input embedding and this output embedding. We\nanalyze the resulting update rules and show that the tied embedding evolves in\na more similar way to the output embedding than to the input embedding in the\nuntied model. We also offer a new method of regularizing the output embedding.\nOur methods lead to a significant reduction in perplexity, as we are able to\nshow on a variety of neural network language models. Finally, we show that\nweight tying can reduce the size of neural translation models to less than half\nof their original size without harming their performance.", "no": 55}, {"url": "https://arxiv.org/abs/1611.01604", "title": "Dynamic Coattention Networks For Question Answering", "cites": "676", "abstract": "Several deep learning models have been proposed for question answering.\nHowever, due to their single-pass nature, they have no way to recover from\nlocal maxima corresponding to incorrect answers. To address this problem, we\nintroduce the Dynamic Coattention Network (DCN) for question answering. The DCN\nfirst fuses co-dependent representations of the question and the document in\norder to focus on relevant parts of both. Then a dynamic pointing decoder\niterates over potential answer spans. This iterative procedure enables the\nmodel to recover from initial local maxima corresponding to incorrect answers.\nOn the Stanford question answering dataset, a single DCN model improves the\nprevious state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains\n80.4% F1.", "no": 56}, {"url": "https://arxiv.org/abs/1604.01696", "title": "A Corpus and Evaluation Framework for Deeper Understanding of\n  Commonsense Stories", "cites": "673", "abstract": "Representation and learning of commonsense knowledge is one of the\nfoundational problems in the quest to enable deep language understanding. This\nissue is particularly challenging for understanding casual and correlational\nrelationships between events. While this topic has received a lot of interest\nin the NLP community, research has been hindered by the lack of a proper\nevaluation framework. This paper attempts to address this problem with a new\nframework for evaluating story understanding and script learning: the 'Story\nCloze Test'. This test requires a system to choose the correct ending to a\nfour-sentence story. We created a new corpus of ~50k five-sentence commonsense\nstories, ROCStories, to enable this evaluation. This corpus is unique in two\nways: (1) it captures a rich set of causal and temporal commonsense relations\nbetween daily events, and (2) it is a high quality collection of everyday life\nstories that can also be used for story generation. Experimental evaluation\nshows that a host of baselines and state-of-the-art models based on shallow\nlanguage understanding struggle to achieve a high score on the Story Cloze\nTest. We discuss these implications for script and story learning, and offer\nsuggestions for deeper language understanding.", "no": 57}, {"url": "https://arxiv.org/abs/1603.04351", "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature\n  Representations", "cites": "660", "abstract": "We present a simple and effective scheme for dependency parsing which is\nbased on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with\na BiLSTM vector representing the token in its sentential context, and feature\nvectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is\ntrained jointly with the parser objective, resulting in very effective feature\nextractors for parsing. We demonstrate the effectiveness of the approach by\napplying it to a greedy transition-based parser as well as to a globally\noptimized graph-based parser. The resulting parsers have very simple\narchitectures, and match or surpass the state-of-the-art accuracies on English\nand Chinese.", "no": 58}, {"url": "https://arxiv.org/abs/1602.02068", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label\n  Classification", "cites": "659", "abstract": "We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.", "no": 59}, {"url": "https://arxiv.org/abs/1609.01454", "title": "Attention-Based Recurrent Neural Network Models for Joint Intent\n  Detection and Slot Filling", "cites": "657", "abstract": "Attention-based encoder-decoder neural network models have recently shown\npromising results in machine translation and speech recognition. In this work,\nwe propose an attention-based neural network model for joint intent detection\nand slot filling, both of which are critical steps for many speech\nunderstanding and dialog systems. Unlike in machine translation and speech\nrecognition, alignment is explicit in slot filling. We explore different\nstrategies in incorporating this alignment information to the encoder-decoder\nframework. Learning from the attention mechanism in encoder-decoder model, we\nfurther propose introducing attention to the alignment-based RNN models. Such\nattentions provide additional information to the intent classification and slot\nlabel prediction. Our independent task models achieve state-of-the-art intent\ndetection error rate and slot filling F1 score on the benchmark ATIS task. Our\njoint training model further obtains 0.56% absolute (23.8% relative) error\nreduction on intent detection and 0.23% absolute gain on slot filling over the\nindependent task models.", "no": 60}, {"url": "https://arxiv.org/abs/1607.05368", "title": "An Empirical Evaluation of doc2vec with Practical Insights into Document\n  Embedding Generation", "cites": "629", "abstract": "Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec\n(Mikolov et al., 2013a) to learn document-level embeddings. Despite promising\nresults in the original paper, others have struggled to reproduce those\nresults. This paper presents a rigorous empirical evaluation of doc2vec over\ntwo tasks. We compare doc2vec to two baselines and two state-of-the-art\ndocument embedding methodologies. We found that doc2vec performs robustly when\nusing models trained on large external corpora, and can be further improved by\nusing pre-trained word embeddings. We also provide recommendations on\nhyper-parameter settings for general purpose applications, and release source\ncode to induce document embeddings using our trained doc2vec models.", "no": 61}, {"url": "https://arxiv.org/abs/1601.01073", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared\n  Attention Mechanism", "cites": "618", "abstract": "We propose multi-way, multilingual neural machine translation. The proposed\napproach enables a single neural translation model to translate between\nmultiple languages, with a number of parameters that grows only linearly with\nthe number of languages. This is made possible by having a single attention\nmechanism that is shared across all language pairs. We train the proposed\nmulti-way, multilingual model on ten language pairs from WMT'15 simultaneously\nand observe clear performance improvements over models trained on only one\nlanguage pair. In particular, we observe that the proposed model significantly\nimproves the translation quality of low-resource language pairs.", "no": 62}, {"url": "https://arxiv.org/abs/1612.00694", "title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA", "cites": "606", "abstract": "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order\nto achieve higher prediction accuracy, machine learning scientists have built\nlarger and larger models. Such large model is both computation intensive and\nmemory intensive. Deploying such bulky model results in high power consumption\nand leads to high total cost of ownership (TCO) of a data center. In order to\nspeedup the prediction and make it energy efficient, we first propose a\nload-balance-aware pruning method that can compress the LSTM model size by 20x\n(10x from pruning and 2x from quantization) with negligible loss of the\nprediction accuracy. The pruned model is friendly for parallel processing.\nNext, we propose scheduler that encodes and partitions the compressed model to\neach PE for parallelism, and schedule the complicated LSTM data flow. Finally,\nwe design the hardware architecture, named Efficient Speech Recognition Engine\n(ESE) that works directly on the compressed model. Implemented on Xilinx\nXCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working\ndirectly on the compressed LSTM network, corresponding to 2.52 TOPS on the\nuncompressed one, and processes a full LSTM for speech recognition with a power\ndissipation of 41 Watts. Evaluated on the LSTM for speech recognition\nbenchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X\nGPU implementations. It achieves 40x and 11.5x higher energy efficiency\ncompared with the CPU and GPU respectively.", "no": 63}, {"url": "https://arxiv.org/abs/1603.08507", "title": "Generating Visual Explanations", "cites": "603", "abstract": "Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.", "no": 64}, {"url": "https://arxiv.org/abs/1603.06318", "title": "Harnessing Deep Neural Networks with Logic Rules", "cites": "600", "abstract": "Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.", "no": 65}, {"url": "https://arxiv.org/abs/1608.07905", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "cites": "588", "abstract": "Machine comprehension of text is an important problem in natural language\nprocessing. A recently released dataset, the Stanford Question Answering\nDataset (SQuAD), offers a large number of real questions and their answers\ncreated by humans through crowdsourcing. SQuAD provides a challenging testbed\nfor evaluating machine comprehension algorithms, partly because compared with\nprevious datasets, in SQuAD the answers do not come from a small set of\ncandidate answers and they have variable lengths. We propose an end-to-end\nneural architecture for the task. The architecture is based on match-LSTM, a\nmodel we proposed previously for textual entailment, and Pointer Net, a\nsequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the\noutput tokens to be from the input sequences. We propose two ways of using\nPointer Net for our task. Our experiments show that both of our two models\nsubstantially outperform the best results obtained by Rajpurkar et al.(2016)\nusing logistic regression and manually crafted features.", "no": 66}, {"url": "https://arxiv.org/abs/1606.02960", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "cites": "581", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.", "no": 67}, {"url": "https://arxiv.org/abs/1602.03001", "title": "A Convolutional Attention Network for Extreme Summarization of Source\n  Code", "cites": "568", "abstract": "Attention mechanisms in neural networks have proved useful for problems in\nwhich the input and output do not have fixed dimension. Often there exist\nfeatures that are locally translation invariant and would be valuable for\ndirecting the model's attention, but previous attentional architectures are not\nconstructed to learn such features specifically. We introduce an attentional\nneural network that employs convolution on the input tokens to detect local\ntime-invariant and long-range topical attention features in a context-dependent\nway. We apply this architecture to the problem of extreme summarization of\nsource code snippets into short, descriptive function name-like summaries.\nUsing those features, the model sequentially generates a summary by\nmarginalizing over two attention mechanisms: one that predicts the next summary\ntoken based on the attention weights of the input tokens and another that is\nable to copy a code token as-is directly into the summary. We demonstrate our\nconvolutional attention neural network's performance on 10 popular Java\nprojects showing that it achieves better performance compared to previous\nattentional mechanisms.", "no": 68}, {"url": "https://arxiv.org/abs/1603.06042", "title": "Globally Normalized Transition-Based Neural Networks", "cites": "566", "abstract": "We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.", "no": 69}, {"url": "https://arxiv.org/abs/1610.05256", "title": "Achieving Human Parity in Conversational Speech Recognition", "cites": "566", "abstract": "Conversational speech recognition has served as a flagship speech recognition\ntask since the release of the Switchboard corpus in the 1990s. In this paper,\nwe measure the human error rate on the widely used NIST 2000 test set, and find\nthat our latest automated system has reached human parity. The error rate of\nprofessional transcribers is 5.9% for the Switchboard portion of the data, in\nwhich newly acquainted pairs of people discuss an assigned topic, and 11.3% for\nthe CallHome portion where friends and family members have open-ended\nconversations. In both cases, our automated system establishes a new state of\nthe art, and edges past the human benchmark, achieving error rates of 5.8% and\n11.0%, respectively. The key to our system's performance is the use of various\nconvolutional and LSTM acoustic model architectures, combined with a novel\nspatial smoothing method and lattice-free MMI acoustic training, multiple\nrecurrent neural network language modeling approaches, and a systematic use of\nsystem combination.", "no": 70}, {"url": "https://arxiv.org/abs/1611.01587", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "cites": "562", "abstract": "Transfer and multi-task learning have traditionally focused on either a\nsingle source-target pair or very few, similar tasks. Ideally, the linguistic\nlevels of morphology, syntax and semantics would benefit each other by being\ntrained in a single model. We introduce a joint many-task model together with a\nstrategy for successively growing its depth to solve increasingly complex\ntasks. Higher layers include shortcut connections to lower-level task\npredictions to reflect linguistic hierarchies. We use a simple regularization\nterm to allow for optimizing all model weights to improve one task's loss\nwithout exhibiting catastrophic interference of the other tasks. Our single\nend-to-end model obtains state-of-the-art or competitive results on five\ndifferent tasks from tagging, parsing, relatedness, and entailment tasks.", "no": 71}, {"url": "https://arxiv.org/abs/1602.03483", "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "cites": "559", "abstract": "Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.", "no": 72}, {"url": "https://arxiv.org/abs/1606.02858", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "cites": "558", "abstract": "Enabling a computer to understand a document so that it can answer\ncomprehension questions is a central, yet unsolved goal of NLP. A key factor\nimpeding its solution by machine learned systems is the limited availability of\nhuman-annotated data. Hermann et al. (2015) seek to solve this problem by\ncreating over a million training examples by pairing CNN and Daily Mail news\narticles with their summarized bullet points, and show that a neural network\ncan then be trained to give good performance on this task. In this paper, we\nconduct a thorough examination of this new reading comprehension task. Our\nprimary aim is to understand what depth of language understanding is required\nto do well on this task. We approach this from one side by doing a careful\nhand-analysis of a small subset of the problems and from the other by showing\nthat simple, carefully designed systems can obtain accuracies of 73.6% and\n76.6% on these two datasets, exceeding current state-of-the-art results by\n7-10% and approaching what we believe is the ceiling for performance on this\ntask.", "no": 73}, {"url": "https://arxiv.org/abs/1601.01705", "title": "Learning to Compose Neural Networks for Question Answering", "cites": "554", "abstract": "We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.", "no": 74}, {"url": "https://arxiv.org/abs/1612.01556", "title": "The Evolution of Sentiment Analysis - A Review of Research Topics,\n  Venues, and Top Cited Papers", "cites": "552", "abstract": "Sentiment analysis is one of the fastest growing research areas in computer\nscience, making it challenging to keep track of all the activities in the area.\nWe present a computer-assisted literature review, where we utilize both text\nmining and qualitative coding, and analyze 6,996 papers from Scopus. We find\nthat the roots of sentiment analysis are in the studies on public opinion\nanalysis at the beginning of 20th century and in the text subjectivity analysis\nperformed by the computational linguistics community in 1990's. However, the\noutbreak of computer-based sentiment analysis only occurred with the\navailability of subjective texts on the Web. Consequently, 99% of the papers\nhave been published after 2004. Sentiment analysis papers are scattered to\nmultiple publication venues, and the combined number of papers in the top-15\nvenues only represent ca. 30% of the papers in total. We present the top-20\ncited papers from Google Scholar and Scopus and a taxonomy of research topics.\nIn recent years, sentiment analysis has shifted from analyzing online product\nreviews to social media texts from Twitter and Facebook. Many topics beyond\nproduct reviews like stock markets, elections, disasters, medicine, software\nengineering and cyberbullying extend the utilization of sentiment analysis", "no": 75}, {"url": "https://arxiv.org/abs/1605.08535", "title": "Deep API Learning", "cites": "547", "abstract": "Developers often wonder how to implement a certain functionality (e.g., how\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\nAPI-related natural language query is very helpful in this regard. Given a\nquery, existing approaches utilize information retrieval models to search for\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\n(i.e., keyword matching or word-to-word alignment) and lack a deep\nunderstanding of the semantics of the query.\n  We propose DeepAPI, a deep learning based approach to generate API usage\nsequences for a given natural language query. Instead of a bags-of-words\nassumption, it learns the sequence of words in a query and the sequence of\nassociated APIs. DeepAPI adapts a neural language model named RNN\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\ncontext vector, and generates an API sequence based on the context vector. We\nalso augment the RNN Encoder-Decoder by considering the importance of\nindividual APIs. We empirically evaluate our approach with more than 7 million\nannotated code snippets collected from GitHub. The results show that our\napproach generates largely accurate API sequences and outperforms the related\napproaches.", "no": 76}, {"url": "https://arxiv.org/abs/1606.06031", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "cites": "545", "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.", "no": 77}, {"url": "https://arxiv.org/abs/1602.06359", "title": "Text Matching as Image Recognition", "cites": "544", "abstract": "Matching two texts is a fundamental problem in many natural language\nprocessing tasks. An effective way is to extract meaningful matching patterns\nfrom words, phrases, and sentences to produce the matching score. Inspired by\nthe success of convolutional neural network in image recognition, where neurons\ncan capture many complicated patterns based on the extracted elementary visual\npatterns such as oriented edges and corners, we propose to model text matching\nas the problem of image recognition. Firstly, a matching matrix whose entries\nrepresent the similarities between words is constructed and viewed as an image.\nThen a convolutional neural network is utilized to capture rich matching\npatterns in a layer-by-layer way. We show that by resembling the compositional\nhierarchies of patterns in image recognition, our model can successfully\nidentify salient signals such as n-gram and n-term matchings. Experimental\nresults demonstrate its superiority against the baselines.", "no": 78}, {"url": "https://arxiv.org/abs/1605.00459", "title": "Multi30K: Multilingual English-German Image Descriptions", "cites": "544", "abstract": "We introduce the Multi30K dataset to stimulate multilingual multimodal\nresearch. Recent advances in image description have been demonstrated on\nEnglish-language datasets almost exclusively, but image description should not\nbe limited to English. This dataset extends the Flickr30K dataset with i)\nGerman translations created by professional translators over a subset of the\nEnglish descriptions, and ii) descriptions crowdsourced independently of the\noriginal English descriptions. We outline how the data can be used for\nmultilingual image description and multimodal machine translation, but we\nanticipate the data will be useful for a broader range of tasks.", "no": 79}, {"url": "https://arxiv.org/abs/1612.08220", "title": "Understanding Neural Networks through Representation Erasure", "cites": "541", "abstract": "While neural networks have been successfully applied to many natural language\nprocessing tasks, they come at the cost of interpretability. In this paper, we\npropose a general methodology to analyze and interpret decisions from a neural\nmodel by observing the effects on the model of erasing various parts of the\nrepresentation, such as input word-vector dimensions, intermediate hidden\nunits, or input words. We present several approaches to analyzing the effects\nof such erasure, from computing the relative difference in evaluation metrics,\nto using reinforcement learning to erase the minimum set of input words in\norder to flip a neural model's decision. In a comprehensive analysis of\nmultiple NLP tasks, including linguistic feature classification, sentence-level\nsentiment analysis, and document level sentiment aspect prediction, we show\nthat the proposed methodology not only offers clear explanations about neural\nmodel decisions, but also provides a way to conduct error analysis on neural\nmodels.", "no": 80}, {"url": "https://arxiv.org/abs/1610.10099", "title": "Neural Machine Translation in Linear Time", "cites": "537", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.", "no": 81}, {"url": "https://arxiv.org/abs/1601.06971", "title": "Sentiment Analysis of Twitter Data: A Survey of Techniques", "cites": "535", "abstract": "With the advancement of web technology and its growth, there is a huge volume\nof data present in the web for internet users and a lot of data is generated\ntoo. Internet has become a platform for online learning, exchanging ideas and\nsharing opinions. Social networking sites like Twitter, Facebook, Google+ are\nrapidly gaining popularity as they allow people to share and express their\nviews about topics,have discussion with different communities, or post messages\nacross the world. There has been lot of work in the field of sentiment analysis\nof twitter data. This survey focuses mainly on sentiment analysis of twitter\ndata which is helpful to analyze the information in the tweets where opinions\nare highly unstructured, heterogeneous and are either positive or negative, or\nneutral in some cases. In this paper, we provide a survey and a comparative\nanalyses of existing techniques for opinion mining like machine learning and\nlexicon-based approaches, together with evaluation metrics. Using various\nmachine learning algorithms like Naive Bayes, Max Entropy, and Support Vector\nMachine, we provide a research on twitter data streams.General challenges and\napplications of Sentiment Analysis on Twitter are also discussed in this paper.", "no": 82}, {"url": "https://arxiv.org/abs/1608.04207", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction\n  Tasks", "cites": "524", "abstract": "There is a lot of research interest in encoding variable length sentences\ninto fixed length vectors, in a way that preserves the sentence meanings. Two\ncommon methods include representations based on averaging word vectors, and\nrepresentations based on the hidden states of recurrent neural networks such as\nLSTMs. The sentence vectors are used as features for subsequent machine\nlearning tasks or for pre-training in the context of deep learning. However,\nnot much is known about the properties that are encoded in these sentence\nrepresentations and about the language information they capture. We propose a\nframework that facilitates better understanding of the encoded representations.\nWe define prediction tasks around isolated aspects of sentence structure\n(namely sentence length, word content, and word order), and score\nrepresentations by the ability to train a classifier to solve each prediction\ntask when using the representation as input. We demonstrate the potential\ncontribution of the approach by analyzing different sentence representation\nmechanisms. The analysis sheds light on the relative strengths of different\nsentence embedding methods with respect to these low level prediction tasks,\nand on the effect of the encoded vector's dimensionality on the resulting\nrepresentations.", "no": 83}, {"url": "https://arxiv.org/abs/1602.07776", "title": "Recurrent Neural Network Grammars", "cites": "521", "abstract": "We introduce recurrent neural network grammars, probabilistic models of\nsentences with explicit phrase structure. We explain efficient inference\nprocedures that allow application to both parsing and language modeling.\nExperiments show that they provide better parsing in English than any single\npreviously published supervised generative model and better language modeling\nthan state-of-the-art sequential RNNs in English and Chinese.", "no": 84}, {"url": "https://arxiv.org/abs/1603.08148", "title": "Pointing the Unknown Words", "cites": "521", "abstract": "The problem of rare and unknown words is an important issue that can\npotentially influence the performance of many NLP systems, including both the\ntraditional count-based and the deep learning models. We propose a novel way to\ndeal with the rare and unseen words for the neural network models using\nattention. Our model uses two softmax layers in order to predict the next word\nin conditional language models: one predicts the location of a word in the\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\neach time-step, the decision of which softmax layer to use choose adaptively\nmade by an MLP which is conditioned on the context.~We motivate our work from a\npsychological evidence that humans naturally have a tendency to point towards\nobjects in the context or the environment when the name of an object is not\nknown.~We observe improvements on two tasks, neural machine translation on the\nEuroparl English to French parallel corpora and text summarization on the\nGigaword dataset using our proposed model.", "no": 85}, {"url": "https://arxiv.org/abs/1610.02424", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n  Models", "cites": "505", "abstract": "Neural sequence models are widely used to model time-series data. Equally\nubiquitous is the usage of beam search (BS) as an approximate inference\nalgorithm to decode output sequences from these models. BS explores the search\nspace in a greedy left-right fashion retaining only the top-B candidates -\nresulting in sequences that differ only slightly from each other. Producing\nlists of nearly identical sequences is not only computationally wasteful but\nalso typically fails to capture the inherent ambiguity of complex AI tasks. To\novercome this problem, we propose Diverse Beam Search (DBS), an alternative to\nBS that decodes a list of diverse outputs by optimizing for a\ndiversity-augmented objective. We observe that our method finds better top-1\nsolutions by controlling for the exploration and exploitation of the search\nspace - implying that DBS is a better search algorithm. Moreover, these gains\nare achieved with minimal computational or memory over- head as compared to\nbeam search. To demonstrate the broad applicability of our method, we present\nresults on image captioning, machine translation and visual question generation\nusing both standard quantitative metrics and qualitative human studies.\nFurther, we study the role of diversity for image-grounded language generation\ntasks as the complexity of the image changes. We observe that our method\nconsistently outperforms BS and previously proposed techniques for diverse\ndecoding from neural sequence models.", "no": 86}, {"url": "https://arxiv.org/abs/1611.03954", "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge\n  Alignment", "cites": "488", "abstract": "Many recent works have demonstrated the benefits of knowledge graph\nembeddings in completing monolingual knowledge graphs. Inasmuch as related\nknowledge bases are built in several different languages, achieving\ncross-lingual knowledge alignment will help people in constructing a coherent\nknowledge base, and assist machines in dealing with different expressions of\nentity relationships across diverse human languages. Unfortunately, achieving\nthis highly desirable crosslingual alignment by human labor is very costly and\nerrorprone. Thus, we propose MTransE, a translation-based model for\nmultilingual knowledge graph embeddings, to provide a simple and automated\nsolution. By encoding entities and relations of each language in a separated\nembedding space, MTransE provides transitions for each embedding vector to its\ncross-lingual counterparts in other spaces, while preserving the\nfunctionalities of monolingual embeddings. We deploy three different techniques\nto represent cross-lingual transitions, namely axis calibration, translation\nvectors, and linear transformations, and derive five variants for MTransE using\ndifferent loss functions. Our models can be trained on partially aligned\ngraphs, where just a small portion of triples are aligned with their\ncross-lingual counterparts. The experiments on cross-lingual entity matching\nand triple-wise alignment verification show promising results, with some\nvariants consistently outperforming others on different tasks. We also explore\nhow MTransE preserves the key properties of its monolingual counterpart TransE.", "no": 87}, {"url": "https://arxiv.org/abs/1612.01627", "title": "Sequential Matching Network: A New Architecture for Multi-turn Response\n  Selection in Retrieval-based Chatbots", "cites": "488", "abstract": "We study response selection for multi-turn conversation in retrieval-based\nchatbots. Existing work either concatenates utterances in context or matches a\nresponse with a highly abstract context vector finally, which may lose\nrelationships among utterances or important contextual information. We propose\na sequential matching network (SMN) to address both problems. SMN first matches\na response with each utterance in the context on multiple levels of\ngranularity, and distills important matching information from each pair as a\nvector with convolution and pooling operations. The vectors are then\naccumulated in a chronological order through a recurrent neural network (RNN)\nwhich models relationships among utterances. The final matching score is\ncalculated with the hidden states of the RNN. An empirical study on two public\ndata sets shows that SMN can significantly outperform state-of-the-art methods\nfor response selection in multi-turn conversation.", "no": 88}, {"url": "https://arxiv.org/abs/1611.06639", "title": "Text Classification Improved by Integrating Bidirectional LSTM with\n  Two-dimensional Max Pooling", "cites": "484", "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures used\nin Natural Language Processsing (NLP) tasks because its recurrent structure is\nvery suitable to process variable-length text. RNN can utilize distributed\nrepresentations of words by first converting the tokens comprising each text\ninto vectors, which form a matrix. And this matrix includes two dimensions: the\ntime-step dimension and the feature vector dimension. Then most existing models\nusually utilize one-dimensional (1D) max pooling operation or attention-based\noperation only on the time-step dimension to obtain a fixed-length vector.\nHowever, the features on the feature vector dimension are not mutually\nindependent, and simply applying 1D pooling operation over the time-step\ndimension independently may destroy the structure of the feature\nrepresentation. On the other hand, applying two-dimensional (2D) pooling\noperation over the two dimensions may sample more meaningful features for\nsequence modeling tasks. To integrate the features on both dimensions of the\nmatrix, this paper explores applying 2D max pooling operation to obtain a\nfixed-length representation of the text. This paper also utilizes 2D\nconvolution to sample more meaningful information of the matrix. Experiments\nare conducted on six text classification tasks, including sentiment analysis,\nquestion classification, subjectivity classification and newsgroup\nclassification. Compared with the state-of-the-art models, the proposed models\nachieve excellent performance on 4 out of 6 tasks. Specifically, one of the\nproposed models achieves highest accuracy on Stanford Sentiment Treebank binary\nclassification and fine-grained classification tasks.", "no": 89}, {"url": "https://arxiv.org/abs/1603.00892", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "cites": "476", "abstract": "In this work, we present a novel counter-fitting method which injects\nantonymy and synonymy constraints into vector space representations in order to\nimprove the vectors' capability for judging semantic similarity. Applying this\nmethod to publicly available pre-trained word vectors leads to a new state of\nthe art performance on the SimLex-999 dataset. We also show how the method can\nbe used to tailor the word vector space for the downstream task of dialogue\nstate tracking, resulting in robust improvements across different dialogue\ndomains.", "no": 90}, {"url": "https://arxiv.org/abs/1606.03777", "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "cites": "473", "abstract": "One of the core components of modern spoken dialogue systems is the belief\ntracker, which estimates the user's goal at every step of the dialogue.\nHowever, most current approaches have difficulty scaling to larger, more\ncomplex dialogue domains. This is due to their dependency on either: a) Spoken\nLanguage Understanding models that require large amounts of annotated training\ndata; or b) hand-crafted lexicons for capturing some of the linguistic\nvariation in users' language. We propose a novel Neural Belief Tracking (NBT)\nframework which overcomes these problems by building on recent advances in\nrepresentation learning. NBT models reason over pre-trained word vectors,\nlearning to compose them into distributed representations of user utterances\nand dialogue context. Our evaluation on two datasets shows that this approach\nsurpasses past limitations, matching the performance of state-of-the-art models\nwhich rely on hand-crafted semantic lexicons and outperforming them when such\nlexicons are not provided.", "no": 91}, {"url": "https://arxiv.org/abs/1606.03622", "title": "Data Recombination for Neural Semantic Parsing", "cites": "457", "abstract": "Modeling crisp logical regularities is crucial in semantic parsing, making it\ndifficult for neural models with no task-specific prior knowledge to achieve\ngood results. In this paper, we introduce data recombination, a novel framework\nfor injecting such prior knowledge into a model. From the training data, we\ninduce a high-precision synchronous context-free grammar, which captures\nimportant conditional independence properties commonly found in semantic\nparsing. We then train a sequence-to-sequence recurrent network (RNN) model\nwith a novel attention-based copying mechanism on datapoints sampled from this\ngrammar, thereby teaching the model about these structural properties. Data\nrecombination improves the accuracy of our RNN model on three semantic parsing\ndatasets, leading to new state-of-the-art performance on the standard GeoQuery\ndataset for models with comparable supervision.", "no": 92}, {"url": "https://arxiv.org/abs/1606.03556", "title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "cites": "455", "abstract": "We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.", "no": 93}, {"url": "https://arxiv.org/abs/1610.03017", "title": "Fully Character-Level Neural Machine Translation without Explicit\n  Segmentation", "cites": "455", "abstract": "Most existing machine translation systems operate at the level of words,\nrelying on explicit segmentation to extract tokens. We introduce a neural\nmachine translation (NMT) model that maps a source character sequence to a\ntarget character sequence without any segmentation. We employ a character-level\nconvolutional network with max-pooling at the encoder to reduce the length of\nsource representation, allowing the model to be trained at a speed comparable\nto subword-level models while capturing local regularities. Our\ncharacter-to-character model outperforms a recently proposed baseline with a\nsubword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable\nperformance on FI-EN and RU-EN. We then demonstrate that it is possible to\nshare a single character-level encoder across multiple languages by training a\nmodel on a many-to-one translation task. In this multilingual setting, the\ncharacter-level encoder significantly outperforms the subword-level encoder on\nall the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality\nof the multilingual character-level translation even surpasses the models\nspecifically trained on that language pair alone, both in terms of BLEU score\nand human judgment.", "no": 94}, {"url": "https://arxiv.org/abs/1611.02344", "title": "A Convolutional Encoder Model for Neural Machine Translation", "cites": "438", "abstract": "The prevalent approach to neural machine translation relies on bi-directional\nLSTMs to encode the source sentence. In this paper we present a faster and\nsimpler architecture based on a succession of convolutional layers. This allows\nto encode the entire source sentence simultaneously compared to recurrent\nnetworks for which computation is constrained by temporal dependencies. On\nWMT'16 English-Romanian translation we achieve competitive accuracy to the\nstate-of-the-art and we outperform several recently published results on the\nWMT'15 English-German task. Our models obtain almost the same accuracy as a\nvery deep LSTM setup on WMT'14 English-French translation. Our convolutional\nencoder speeds up CPU decoding by more than two times at the same or higher\naccuracy as a strong bi-directional LSTM baseline.", "no": 95}, {"url": "https://arxiv.org/abs/1604.03968", "title": "Visual Storytelling", "cites": "437", "abstract": "We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression.", "no": 96}, {"url": "https://arxiv.org/abs/1603.03827", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "cites": "436", "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.", "no": 97}, {"url": "https://arxiv.org/abs/1607.04423", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "cites": "433", "abstract": "Cloze-style queries are representative problems in reading comprehension.\nOver the past few months, we have seen much progress that utilizing neural\nnetwork approach to solve Cloze-style questions. In this paper, we present a\nnovel model called attention-over-attention reader for the Cloze-style reading\ncomprehension task. Our model aims to place another attention mechanism over\nthe document-level attention, and induces \"attended attention\" for final\npredictions. Unlike the previous works, our neural network model requires less\npre-defined hyper-parameters and uses an elegant architecture for modeling.\nExperimental results show that the proposed attention-over-attention model\nsignificantly outperforms various state-of-the-art systems by a large margin in\npublic datasets, such as CNN and Children's Book Test datasets.", "no": 98}, {"url": "https://arxiv.org/abs/1611.01576", "title": "Quasi-Recurrent Neural Networks", "cites": "426", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.", "no": 99}, {"url": "https://arxiv.org/abs/1605.01655", "title": "Stance and Sentiment in Tweets", "cites": "425", "abstract": "We can often detect from a person's utterances whether he/she is in favor of\nor against a given target entity -- their stance towards the target. However, a\nperson may express the same stance towards a target by using negative or\npositive language. Here for the first time we present a dataset of\ntweet--target pairs annotated for both stance and sentiment. The targets may or\nmay not be referred to in the tweets, and they may or may not be the target of\nopinion in the tweets. Partitions of this dataset were used as training and\ntest sets in a SemEval-2016 shared task competition. We propose a simple stance\ndetection system that outperforms submissions from all 19 teams that\nparticipated in the shared task. Additionally, access to both stance and\nsentiment annotations allows us to explore several research questions. We show\nthat while knowing the sentiment expressed by a tweet is beneficial for stance\nclassification, it alone is not sufficient. Finally, we use additional\nunlabeled data through distant supervision techniques and word embeddings to\nfurther improve stance classification.", "no": 100}]