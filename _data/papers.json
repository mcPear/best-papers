[{"url": "http://arxiv.org/abs/2101.00190v1", "inserted_at": "2021-01-01T08:00:36Z", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 400}, {"url": "http://arxiv.org/abs/2104.08821v4", "inserted_at": "2021-04-18T11:27:08Z", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 393}, {"url": "http://arxiv.org/abs/2102.05918v2", "inserted_at": "2021-02-11T10:08:12Z", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 353}, {"url": "http://arxiv.org/abs/2104.08691v2", "inserted_at": "2021-04-18T03:19:26Z", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 325}, {"url": "http://arxiv.org/abs/2108.05927v1", "inserted_at": "2021-08-12T19:02:53Z", "title": "Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive\n  Content Identification in Indo-European Languages", "cites": 238}, {"url": "http://arxiv.org/abs/2106.07447v1", "inserted_at": "2021-06-14T14:14:28Z", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 220}, {"url": "http://arxiv.org/abs/2102.07662v1", "inserted_at": "2021-02-15T16:47:00Z", "title": "Overview of the TREC 2020 deep learning track", "cites": 197}, {"url": "http://arxiv.org/abs/2103.17249v1", "inserted_at": "2021-03-31T17:51:25Z", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "cites": 193}, {"url": "http://arxiv.org/abs/2109.01652v5", "inserted_at": "2021-09-03T17:55:52Z", "title": "Finetuned Language Models Are Zero-Shot Learners", "cites": 157}, {"url": "http://arxiv.org/abs/2107.13586v1", "inserted_at": "2021-07-28T18:09:46Z", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "cites": 155}, {"url": "http://arxiv.org/abs/2103.10385v1", "inserted_at": "2021-03-18T17:13:50Z", "title": "GPT Understands, Too", "cites": 141}, {"url": "http://arxiv.org/abs/2102.06183v1", "inserted_at": "2021-02-11T18:50:16Z", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "cites": 141}, {"url": "http://arxiv.org/abs/2105.03075v5", "inserted_at": "2021-05-07T06:03:45Z", "title": "A Survey of Data Augmentation Approaches for NLP", "cites": 138}, {"url": "http://arxiv.org/abs/2102.09690v2", "inserted_at": "2021-02-19T00:23:59Z", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "cites": 134}, {"url": "http://arxiv.org/abs/2102.04664v2", "inserted_at": "2021-02-09T06:16:25Z", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "cites": 133}, {"url": "http://arxiv.org/abs/2110.08207v3", "inserted_at": "2021-10-15T17:08:57Z", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "cites": 132}, {"url": "http://arxiv.org/abs/2105.08050v2", "inserted_at": "2021-05-17T17:55:04Z", "title": "Pay Attention to MLPs", "cites": 126}, {"url": "http://arxiv.org/abs/2104.12763v2", "inserted_at": "2021-04-26T17:55:33Z", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "cites": 125}, {"url": "http://arxiv.org/abs/2112.11446v2", "inserted_at": "2021-12-08T19:41:47Z", "title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "cites": 122}, {"url": "http://arxiv.org/abs/2102.03902v3", "inserted_at": "2021-02-07T20:06:59Z", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating\n  Self-Attention", "cites": 120}]