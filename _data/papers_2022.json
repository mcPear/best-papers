[{"url": "http://arxiv.org/abs/2204.02311v3", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 132}, {"url": "http://arxiv.org/abs/2203.02155v1", "title": "Training language models to follow instructions with human feedback", "cites": 78}, {"url": "http://arxiv.org/abs/2201.11990v3", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "cites": 76}, {"url": "http://arxiv.org/abs/2201.08239v3", "title": "LaMDA: Language Models for Dialog Applications", "cites": 74}, {"url": "http://arxiv.org/abs/2202.12575v1", "title": "Mining Naturally-occurring Corrections and Paraphrases from Wikipedia's\n  Revision History", "cites": 71}, {"url": "http://arxiv.org/abs/2204.03071v1", "title": "Urdu Morphology, Orthography and Lexicon Extraction", "cites": 63}, {"url": "http://arxiv.org/abs/2201.11903v4", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "cites": 52}, {"url": "http://arxiv.org/abs/2203.15556v1", "title": "Training Compute-Optimal Large Language Models", "cites": 50}, {"url": "http://arxiv.org/abs/2205.01068v4", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": 41}, {"url": "http://arxiv.org/abs/2205.06118v1", "title": "Findings of the Shared Task on Offensive Span Identification from\n  Code-Mixed Tamil-English Comments", "cites": 38}, {"url": "http://arxiv.org/abs/2203.05482v3", "title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "cites": 34}, {"url": "http://arxiv.org/abs/2202.03052v2", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "cites": 34}, {"url": "http://arxiv.org/abs/2206.04615v2", "title": "Beyond the Imitation Game: Quantifying and extrapolating the\n  capabilities of language models", "cites": 29}, {"url": "http://arxiv.org/abs/2202.07646v2", "title": "Quantifying Memorization Across Neural Language Models", "cites": 28}, {"url": "http://arxiv.org/abs/2201.07207v2", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "cites": 27}, {"url": "http://arxiv.org/abs/2202.12837v1", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": 26}, {"url": "http://arxiv.org/abs/2203.05557v1", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": 25}, {"url": "http://arxiv.org/abs/2205.07123v1", "title": "The VoicePrivacy 2020 Challenge Evaluation Plan", "cites": 24}, {"url": "http://arxiv.org/abs/2203.12468v2", "title": "The VoicePrivacy 2022 Challenge Evaluation Plan", "cites": 24}, {"url": "http://arxiv.org/abs/2201.05966v2", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": 24}, {"url": "http://arxiv.org/abs/2204.01691v2", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "cites": 23}, {"url": "http://arxiv.org/abs/2203.11171v2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": 23}, {"url": "http://arxiv.org/abs/2202.08791v1", "title": "cosFormer: Rethinking Softmax in Attention", "cites": 22}, {"url": "http://arxiv.org/abs/2202.03286v1", "title": "Red Teaming Language Models with Language Models", "cites": 22}, {"url": "http://arxiv.org/abs/2207.11893v1", "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020", "cites": 20}, {"url": "http://arxiv.org/abs/2207.05133v1", "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021", "cites": 20}, {"url": "http://arxiv.org/abs/2204.06745v1", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "cites": 19}, {"url": "http://arxiv.org/abs/2204.02329v3", "title": "Can language models learn from explanations in context?", "cites": 19}, {"url": "http://arxiv.org/abs/2203.00555v1", "title": "DeepNet: Scaling Transformers to 1,000 Layers", "cites": 19}, {"url": "http://arxiv.org/abs/2204.10050v2", "title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence\n  Embedding", "cites": 18}, {"url": "http://arxiv.org/abs/2203.05794v1", "title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "cites": 18}, {"url": "http://arxiv.org/abs/2201.12122v3", "title": "Can Wikipedia Help Offline Reinforcement Learning?", "cites": 17}, {"url": "http://arxiv.org/abs/2201.10005v1", "title": "Text and Code Embeddings by Contrastive Pre-Training", "cites": 17}, {"url": "http://arxiv.org/abs/2201.03546v2", "title": "Language-driven Semantic Segmentation", "cites": 17}, {"url": "http://arxiv.org/abs/2203.08913v1", "title": "Memorizing Transformers", "cites": 16}, {"url": "http://arxiv.org/abs/2202.13169v3", "title": "A Systematic Evaluation of Large Language Models of Code", "cites": 16}, {"url": "http://arxiv.org/abs/2202.06539v2", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "cites": 16}, {"url": "http://arxiv.org/abs/2201.05955v3", "title": "WANLI: Worker and AI Collaboration for Natural Language Inference\n  Dataset Creation", "cites": 16}, {"url": "http://arxiv.org/abs/2203.13474v3", "title": "A Conversational Paradigm for Program Synthesis", "cites": 15}, {"url": "http://arxiv.org/abs/2202.09694v1", "title": "MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain\n  Acronym Extraction", "cites": 15}, {"url": "http://arxiv.org/abs/2202.06991v2", "title": "Transformer Memory as a Differentiable Search Index", "cites": 15}, {"url": "http://arxiv.org/abs/2201.08808v1", "title": "Conversational Information Seeking", "cites": 15}, {"url": "http://arxiv.org/abs/2201.06796v2", "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for\n  Exploring Language Model Capabilities", "cites": 15}, {"url": "http://arxiv.org/abs/2206.14579v1", "title": "Competence-based Multimodal Curriculum Learning for Medical Report\n  Generation", "cites": 14}, {"url": "http://arxiv.org/abs/2205.11916v2", "title": "Large Language Models are Zero-Shot Reasoners", "cites": 14}, {"url": "http://arxiv.org/abs/2205.06175v2", "title": "A Generalist Agent", "cites": 14}, {"url": "http://arxiv.org/abs/2203.05765v1", "title": "Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval", "cites": 14}, {"url": "http://arxiv.org/abs/2202.06935v1", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation\n  Practices for Generated Text", "cites": 14}, {"url": "http://arxiv.org/abs/2202.06417v2", "title": "A Contrastive Framework for Neural Text Generation", "cites": 14}, {"url": "http://arxiv.org/abs/2202.01771v3", "title": "Pre-Trained Language Models for Interactive Decision-Making", "cites": 14}, {"url": "http://arxiv.org/abs/2202.01169v2", "title": "Unified Scaling Laws for Routed Language Models", "cites": 13}, {"url": "http://arxiv.org/abs/2201.06910v1", "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves\n  Zero-Shot Generalization", "cites": 13}, {"url": "http://arxiv.org/abs/2204.05999v2", "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "cites": 12}, {"url": "http://arxiv.org/abs/2204.00598v2", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "cites": 12}, {"url": "http://arxiv.org/abs/2202.07206v2", "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning", "cites": 12}, {"url": "http://arxiv.org/abs/2202.03829v2", "title": "TimeLMs: Diachronic Language Models from Twitter", "cites": 12}, {"url": "http://arxiv.org/abs/2201.05176v1", "title": "Neural Approaches to Conversational Information Retrieval", "cites": 12}, {"url": "http://arxiv.org/abs/2201.03533v1", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences", "cites": 12}, {"url": "http://arxiv.org/abs/2205.13858v1", "title": "Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word\n  Embeddings", "cites": 11}, {"url": "http://arxiv.org/abs/2204.13258v1", "title": "Cross-modal Memory Networks for Radiology Report Generation", "cites": 11}, {"url": "http://arxiv.org/abs/2204.05832v1", "title": "What Language Model Architecture and Pretraining Objective Work Best for\n  Zero-Shot Generalization?", "cites": 11}, {"url": "http://arxiv.org/abs/2202.09583v1", "title": "Models and Datasets for Cross-Lingual Summarisation", "cites": 11}, {"url": "http://arxiv.org/abs/2202.08005v2", "title": "Should You Mask 15% in Masked Language Modeling?", "cites": 11}, {"url": "http://arxiv.org/abs/2201.07520v1", "title": "CM3: A Causal Masked Multimodal Model of the Internet", "cites": 11}, {"url": "http://arxiv.org/abs/2201.02639v4", "title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and\n  Sound", "cites": 11}, {"url": "http://arxiv.org/abs/2205.05593v1", "title": "Identifying Moments of Change from Longitudinal User Text", "cites": 10}, {"url": "http://arxiv.org/abs/2203.17189v1", "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$", "cites": 10}, {"url": "http://arxiv.org/abs/2202.03629v4", "title": "Survey of Hallucination in Natural Language Generation", "cites": 10}, {"url": "http://arxiv.org/abs/2202.01279v3", "title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "cites": 10}, {"url": "http://arxiv.org/abs/2202.00666v3", "title": "Typical Decoding for Natural Language Generation", "cites": 10}, {"url": "http://arxiv.org/abs/2203.14465v2", "title": "STaR: Bootstrapping Reasoning With Reasoning", "cites": 9}, {"url": "http://arxiv.org/abs/2203.13224v2", "title": "Language Models that Seek for Knowledge: Modular Search & Generation for\n  Dialogue and Prompt Completion", "cites": 9}, {"url": "http://arxiv.org/abs/2203.13131v1", "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "cites": 9}, {"url": "http://arxiv.org/abs/2202.11705v2", "title": "COLD Decoding: Energy-based Constrained Text Generation with Langevin\n  Dynamics", "cites": 9}, {"url": "http://arxiv.org/abs/2202.10936v2", "title": "A Survey of Vision-Language Pre-Trained Models", "cites": 9}, {"url": "http://arxiv.org/abs/2202.08938v1", "title": "Improving Intrinsic Exploration with Language Abstractions", "cites": 9}, {"url": "http://arxiv.org/abs/2202.05520v2", "title": "What Does it Mean for a Language Model to Preserve Privacy?", "cites": 9}, {"url": "http://arxiv.org/abs/2202.01374v1", "title": "mSLAM: Massively multilingual joint pre-training for speech and text", "cites": 9}, {"url": "http://arxiv.org/abs/2204.05862v1", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "cites": 8}, {"url": "http://arxiv.org/abs/2203.14655v2", "title": "Few-Shot Learning with Siamese Networks and Label Tuning", "cites": 8}, {"url": "http://arxiv.org/abs/2203.11147v1", "title": "Teaching language models to support answers with verified quotes", "cites": 8}, {"url": "http://arxiv.org/abs/2203.06904v2", "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for\n  Pre-trained Language Models", "cites": 8}, {"url": "http://arxiv.org/abs/2203.03850v1", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "cites": 8}, {"url": "http://arxiv.org/abs/2202.11923v1", "title": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural\n  Language Processing beyond Gender", "cites": 8}, {"url": "http://arxiv.org/abs/2202.10447v2", "title": "Transformer Quality in Linear Time", "cites": 8}, {"url": "http://arxiv.org/abs/2202.06523v1", "title": "MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution\n  Shifts and Training Conflicts", "cites": 8}, {"url": "http://arxiv.org/abs/2202.04053v1", "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of\n  Text-to-Image Generative Transformers", "cites": 8}, {"url": "http://arxiv.org/abs/2201.11114v2", "title": "Natural Language Descriptions of Deep Visual Features", "cites": 8}, {"url": "http://arxiv.org/abs/2206.02079v1", "title": "Multilingual Neural Machine Translation with Deep Encoder and Multiple\n  Shallow Decoders", "cites": 7}, {"url": "http://arxiv.org/abs/2205.05131v1", "title": "Unifying Language Learning Paradigms", "cites": 7}, {"url": "http://arxiv.org/abs/2204.10628v1", "title": "Autoregressive Search Engines: Generating Substrings as Document\n  Identifiers", "cites": 7}, {"url": "http://arxiv.org/abs/2204.07705v2", "title": "Benchmarking Generalization via In-Context Instructions on 1,600+\n  Language Tasks", "cites": 7}, {"url": "http://arxiv.org/abs/2203.13088v1", "title": "Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized\n  Late Interactions using Enhanced Reduction", "cites": 7}, {"url": "http://arxiv.org/abs/2203.00274v2", "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding", "cites": 7}, {"url": "http://arxiv.org/abs/2202.02476v1", "title": "Semantic Similarity Computing Model Based on Multi Model Fine-Grained\n  Nonlinear Fusion", "cites": 7}, {"url": "http://arxiv.org/abs/2201.12091v3", "title": "Linear Adversarial Concept Erasure", "cites": 7}, {"url": "http://arxiv.org/abs/2201.11732v2", "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and\n  Languages", "cites": 7}, {"url": "http://arxiv.org/abs/2201.08277v3", "title": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual\n  Sentiment Analysis", "cites": 7}, {"url": "http://arxiv.org/abs/2201.07126v1", "title": "Instance-aware Prompt Learning for Language Understanding and Generation", "cites": 7}, {"url": "http://arxiv.org/abs/2201.06642v1", "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus", "cites": 7}]