[{"url": "https://arxiv.org/abs/2203.02155", "title": "Training language models to follow instructions with human feedback", "cites": 2677}, {"url": "https://arxiv.org/abs/2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 2065}, {"url": "https://arxiv.org/abs/2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "cites": 1871}, {"url": "https://arxiv.org/abs/2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": 1153}, {"url": "https://arxiv.org/abs/2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "cites": 776}, {"url": "https://arxiv.org/abs/2201.08239", "title": "LaMDA: Language Models for Dialog Applications", "cites": 775}, {"url": "https://arxiv.org/abs/2206.07682", "title": "Emergent Abilities of Large Language Models", "cites": 759}, {"url": "https://arxiv.org/abs/2210.11416", "title": "Scaling Instruction-Finetuned Language Models", "cites": 694}, {"url": "https://arxiv.org/abs/2203.15556", "title": "Training Compute-Optimal Large Language Models", "cites": 690}, {"url": "https://arxiv.org/abs/2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "cites": 670}, {"url": "https://arxiv.org/abs/2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": 578}, {"url": "https://arxiv.org/abs/2212.04356", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "cites": 546}, {"url": "https://arxiv.org/abs/2204.01691", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "cites": 540}, {"url": "https://arxiv.org/abs/2202.03629", "title": "Survey of Hallucination in Natural Language Generation", "cites": 539}, {"url": "https://arxiv.org/abs/2202.03052", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "cites": 460}, {"url": "https://arxiv.org/abs/2201.11990", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "cites": 412}, {"url": "https://arxiv.org/abs/2208.01618", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion", "cites": 407}, {"url": "https://arxiv.org/abs/2202.12837", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": 387}, {"url": "https://arxiv.org/abs/2212.10560", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "cites": 377}, {"url": "https://arxiv.org/abs/2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": 374}, {"url": "https://arxiv.org/abs/2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "cites": 373}, {"url": "https://arxiv.org/abs/2205.06175", "title": "A Generalist Agent", "cites": 363}, {"url": "https://arxiv.org/abs/2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "cites": 350}, {"url": "https://arxiv.org/abs/2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "cites": 346}, {"url": "https://arxiv.org/abs/2203.05794", "title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "cites": 335}, {"url": "https://arxiv.org/abs/2205.10625", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "cites": 333}, {"url": "https://arxiv.org/abs/2204.06745", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "cites": 306}, {"url": "https://arxiv.org/abs/2201.07207", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "cites": 302}, {"url": "https://arxiv.org/abs/2203.05482", "title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "cites": 289}, {"url": "https://arxiv.org/abs/2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "cites": 285}, {"url": "https://arxiv.org/abs/2207.04672", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "cites": 271}, {"url": "https://arxiv.org/abs/2210.02414", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "cites": 268}, {"url": "https://arxiv.org/abs/2205.14217", "title": "Diffusion-LM Improves Controllable Text Generation", "cites": 261}, {"url": "https://arxiv.org/abs/2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "cites": 261}, {"url": "https://arxiv.org/abs/2206.14858", "title": "Solving Quantitative Reasoning Problems with Language Models", "cites": 259}, {"url": "https://arxiv.org/abs/2212.13138", "title": "Large Language Models Encode Clinical Knowledge", "cites": 256}, {"url": "https://arxiv.org/abs/2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "cites": 251}, {"url": "https://arxiv.org/abs/2207.05608", "title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "cites": 242}, {"url": "https://arxiv.org/abs/2203.13131", "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "cites": 238}, {"url": "https://arxiv.org/abs/2211.09085", "title": "Galactica: A Large Language Model for Science", "cites": 221}, {"url": "https://arxiv.org/abs/2204.05999", "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "cites": 214}, {"url": "https://arxiv.org/abs/2202.07646", "title": "Quantifying Memorization Across Neural Language Models", "cites": 212}, {"url": "https://arxiv.org/abs/2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis", "cites": 211}, {"url": "https://arxiv.org/abs/2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "cites": 209}, {"url": "https://arxiv.org/abs/2201.03546", "title": "Language-driven Semantic Segmentation", "cites": 208}, {"url": "https://arxiv.org/abs/2204.07705", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions\n  on 1600+ NLP Tasks", "cites": 208}, {"url": "https://arxiv.org/abs/2209.06794", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "cites": 206}, {"url": "https://arxiv.org/abs/2202.13169", "title": "A Systematic Evaluation of Large Language Models of Code", "cites": 181}, {"url": "https://arxiv.org/abs/2210.10341", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text\n  Generation and Mining", "cites": 181}, {"url": "https://arxiv.org/abs/2208.03299", "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "cites": 180}, {"url": "https://arxiv.org/abs/2210.03493", "title": "Automatic Chain of Thought Prompting in Large Language Models", "cites": 179}, {"url": "https://arxiv.org/abs/2205.05638", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning", "cites": 175}, {"url": "https://arxiv.org/abs/2212.06817", "title": "RT-1: Robotics Transformer for Real-World Control at Scale", "cites": 175}, {"url": "https://arxiv.org/abs/2211.10435", "title": "PAL: Program-aided Language Models", "cites": 172}, {"url": "https://arxiv.org/abs/2202.05262", "title": "Locating and Editing Factual Associations in GPT", "cites": 169}, {"url": "https://arxiv.org/abs/2202.03286", "title": "Red Teaming Language Models with Language Models", "cites": 167}, {"url": "https://arxiv.org/abs/2201.05966", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": 165}, {"url": "https://arxiv.org/abs/2209.14375", "title": "Improving alignment of dialogue agents via targeted human judgements", "cites": 165}, {"url": "https://arxiv.org/abs/2202.01279", "title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "cites": 163}, {"url": "https://arxiv.org/abs/2211.01910", "title": "Large Language Models Are Human-Level Prompt Engineers", "cites": 160}, {"url": "https://arxiv.org/abs/2210.09261", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "cites": 158}, {"url": "https://arxiv.org/abs/2205.01996", "title": "EmoBank: Studying the Impact of Annotation Perspective and\n  Representation Format on Dimensional Emotion Analysis", "cites": 155}, {"url": "https://arxiv.org/abs/2203.03850", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "cites": 154}, {"url": "https://arxiv.org/abs/2203.14465", "title": "STaR: Bootstrapping Reasoning With Reasoning", "cites": 151}, {"url": "https://arxiv.org/abs/2207.05221", "title": "Language Models (Mostly) Know What They Know", "cites": 143}, {"url": "https://arxiv.org/abs/2211.12588", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "cites": 135}, {"url": "https://arxiv.org/abs/2209.12356", "title": "News Summarization and Evaluation in the Era of GPT-3", "cites": 134}, {"url": "https://arxiv.org/abs/2204.02329", "title": "Can language models learn from explanations in context?", "cites": 130}, {"url": "https://arxiv.org/abs/2210.03350", "title": "Measuring and Narrowing the Compositionality Gap in Language Models", "cites": 130}, {"url": "https://arxiv.org/abs/2204.03162", "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic\n  Compositionality", "cites": 129}, {"url": "https://arxiv.org/abs/2208.03188", "title": "BlenderBot 3: a deployed conversational agent that continually learns to\n  responsibly engage", "cites": 129}, {"url": "https://arxiv.org/abs/2209.05451", "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", "cites": 128}, {"url": "https://arxiv.org/abs/2209.11302", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models", "cites": 128}, {"url": "https://arxiv.org/abs/2211.07636", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at\n  Scale", "cites": 128}, {"url": "https://arxiv.org/abs/2204.08387", "title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking", "cites": 127}, {"url": "https://arxiv.org/abs/2205.09712", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning", "cites": 127}, {"url": "https://arxiv.org/abs/2205.10643", "title": "Self-Supervised Speech Representation Learning: A Review", "cites": 124}, {"url": "https://arxiv.org/abs/2205.15868", "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via\n  Transformers", "cites": 124}, {"url": "https://arxiv.org/abs/2210.11610", "title": "Large Language Models Can Self-Improve", "cites": 124}, {"url": "https://arxiv.org/abs/2203.12277", "title": "Unified Structure Generation for Universal Information Extraction", "cites": 122}, {"url": "https://arxiv.org/abs/2209.07858", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors,\n  and Lessons Learned", "cites": 122}, {"url": "https://arxiv.org/abs/2203.15827", "title": "LinkBERT: Pretraining Language Models with Document Links", "cites": 121}, {"url": "https://arxiv.org/abs/2209.09513", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science\n  Question Answering", "cites": 115}, {"url": "https://arxiv.org/abs/2201.06796", "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for\n  Exploring Language Model Capabilities", "cites": 111}, {"url": "https://arxiv.org/abs/2203.16804", "title": "BRIO: Bringing Order to Abstractive Summarization", "cites": 111}, {"url": "https://arxiv.org/abs/2212.09292", "title": "ChatGPT: The End of Online Exam Integrity?", "cites": 109}, {"url": "https://arxiv.org/abs/2201.05955", "title": "WANLI: Worker and AI Collaboration for Natural Language Inference\n  Dataset Creation", "cites": 108}, {"url": "https://arxiv.org/abs/2211.15661", "title": "What learning algorithm is in-context learning? Investigations with\n  linear models", "cites": 107}, {"url": "https://arxiv.org/abs/2203.06904", "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for\n  Pre-trained Language Models", "cites": 105}, {"url": "https://arxiv.org/abs/2203.17189", "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$", "cites": 104}, {"url": "https://arxiv.org/abs/2206.08853", "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge", "cites": 104}, {"url": "https://arxiv.org/abs/2206.14576", "title": "Using cognitive psychology to understand GPT-3", "cites": 104}, {"url": "https://arxiv.org/abs/2207.04429", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,\n  Vision, and Action", "cites": 104}, {"url": "https://arxiv.org/abs/2208.01066", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function\n  Classes", "cites": 102}, {"url": "https://arxiv.org/abs/2210.00720", "title": "Complexity-Based Prompting for Multi-Step Reasoning", "cites": 102}, {"url": "https://arxiv.org/abs/2212.14882", "title": "ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on\n  Simplified Radiology Reports", "cites": 102}, {"url": "https://arxiv.org/abs/2206.05836", "title": "GLIPv2: Unifying Localization and Vision-Language Understanding", "cites": 101}, {"url": "https://arxiv.org/abs/2212.10403", "title": "Towards Reasoning in Large Language Models: A Survey", "cites": 101}, {"url": "https://arxiv.org/abs/2201.02639", "title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and\n  Sound", "cites": 100}, {"url": "https://arxiv.org/abs/2201.10005", "title": "Text and Code Embeddings by Contrastive Pre-Training", "cites": 100}]