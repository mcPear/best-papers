[{"url": "http://arxiv.org/abs/2204.02311v5", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 385}, {"url": "http://arxiv.org/abs/2201.11903v5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "cites": 217}, {"url": "http://arxiv.org/abs/2203.02155v1", "title": "Training language models to follow instructions with human feedback", "cites": 191}, {"url": "http://arxiv.org/abs/2205.01068v4", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": 173}, {"url": "http://arxiv.org/abs/2201.08239v3", "title": "LaMDA: Language Models for Dialog Applications", "cites": 172}, {"url": "http://arxiv.org/abs/2201.11990v3", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "cites": 152}, {"url": "http://arxiv.org/abs/2203.15556v1", "title": "Training Compute-Optimal Large Language Models", "cites": 151}, {"url": "http://arxiv.org/abs/2204.02311v3", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 132}, {"url": "http://arxiv.org/abs/2202.03052v2", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "cites": 106}, {"url": "http://arxiv.org/abs/2204.01691v2", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "cites": 105}, {"url": "http://arxiv.org/abs/2206.04615v2", "title": "Beyond the Imitation Game: Quantifying and extrapolating the\n  capabilities of language models", "cites": 97}, {"url": "http://arxiv.org/abs/2203.05482v3", "title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "cites": 95}, {"url": "http://arxiv.org/abs/2205.11916v3", "title": "Large Language Models are Zero-Shot Reasoners", "cites": 94}, {"url": "http://arxiv.org/abs/2205.06175v3", "title": "A Generalist Agent", "cites": 88}, {"url": "http://arxiv.org/abs/2203.05557v2", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": 83}, {"url": "http://arxiv.org/abs/2201.07207v2", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "cites": 76}, {"url": "http://arxiv.org/abs/2206.07682v2", "title": "Emergent Abilities of Large Language Models", "cites": 74}, {"url": "http://arxiv.org/abs/2203.11171v3", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": 73}, {"url": "http://arxiv.org/abs/2201.05966v3", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": 73}, {"url": "http://arxiv.org/abs/2202.12837v2", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": 72}, {"url": "http://arxiv.org/abs/2202.12575v1", "title": "Mining Naturally-occurring Corrections and Paraphrases from Wikipedia's\n  Revision History", "cites": 72}, {"url": "http://arxiv.org/abs/2203.05794v1", "title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "cites": 58}, {"url": "http://arxiv.org/abs/2203.13131v1", "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "cites": 55}, {"url": "http://arxiv.org/abs/2208.10442v2", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "cites": 54}, {"url": "http://arxiv.org/abs/2204.06745v1", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "cites": 53}, {"url": "http://arxiv.org/abs/2201.11903v4", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "cites": 52}, {"url": "http://arxiv.org/abs/2204.03071v1", "title": "Urdu Morphology, Orthography and Lexicon Extraction", "cites": 51}, {"url": "http://arxiv.org/abs/2204.00598v2", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "cites": 50}, {"url": "http://arxiv.org/abs/2202.07646v2", "title": "Quantifying Memorization Across Neural Language Models", "cites": 48}, {"url": "http://arxiv.org/abs/2209.11429v3", "title": "News Category Dataset", "cites": 47}, {"url": "http://arxiv.org/abs/2208.01618v1", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion", "cites": 47}, {"url": "http://arxiv.org/abs/2206.14858v2", "title": "Solving Quantitative Reasoning Problems with Language Models", "cites": 46}, {"url": "http://arxiv.org/abs/2202.08791v1", "title": "cosFormer: Rethinking Softmax in Attention", "cites": 45}, {"url": "http://arxiv.org/abs/2205.10625v2", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "cites": 43}, {"url": "http://arxiv.org/abs/2202.03286v1", "title": "Red Teaming Language Models with Language Models", "cites": 42}, {"url": "http://arxiv.org/abs/2204.02329v4", "title": "Can language models learn from explanations in context?", "cites": 41}, {"url": "http://arxiv.org/abs/2205.14217v1", "title": "Diffusion-LM Improves Controllable Text Generation", "cites": 40}, {"url": "http://arxiv.org/abs/2205.06118v1", "title": "Findings of the Shared Task on Offensive Span Identification from\n  Code-Mixed Tamil-English Comments", "cites": 38}, {"url": "http://arxiv.org/abs/2202.01279v3", "title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "cites": 38}, {"url": "http://arxiv.org/abs/2205.07123v1", "title": "The VoicePrivacy 2020 Challenge Evaluation Plan", "cites": 37}, {"url": "http://arxiv.org/abs/2203.12468v3", "title": "The VoicePrivacy 2022 Challenge Evaluation Plan", "cites": 37}, {"url": "http://arxiv.org/abs/2201.05955v5", "title": "WANLI: Worker and AI Collaboration for Natural Language Inference\n  Dataset Creation", "cites": 37}, {"url": "http://arxiv.org/abs/2207.04672v3", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "cites": 36}, {"url": "http://arxiv.org/abs/2204.05999v2", "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "cites": 36}, {"url": "http://arxiv.org/abs/2202.07206v2", "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning", "cites": 36}, {"url": "http://arxiv.org/abs/2201.12122v3", "title": "Can Wikipedia Help Offline Reinforcement Learning?", "cites": 36}, {"url": "http://arxiv.org/abs/2201.03546v2", "title": "Language-driven Semantic Segmentation", "cites": 36}, {"url": "http://arxiv.org/abs/2203.06904v2", "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for\n  Pre-trained Language Models", "cites": 35}, {"url": "http://arxiv.org/abs/2202.13169v3", "title": "A Systematic Evaluation of Large Language Models of Code", "cites": 34}, {"url": "http://arxiv.org/abs/2202.03629v5", "title": "Survey of Hallucination in Natural Language Generation", "cites": 34}, {"url": "http://arxiv.org/abs/2203.00555v1", "title": "DeepNet: Scaling Transformers to 1,000 Layers", "cites": 33}, {"url": "http://arxiv.org/abs/2205.05638v2", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning", "cites": 32}, {"url": "http://arxiv.org/abs/2202.06935v1", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation\n  Practices for Generated Text", "cites": 32}, {"url": "http://arxiv.org/abs/2203.14465v2", "title": "STaR: Bootstrapping Reasoning With Reasoning", "cites": 31}, {"url": "http://arxiv.org/abs/2202.03829v2", "title": "TimeLMs: Diachronic Language Models from Twitter", "cites": 31}, {"url": "http://arxiv.org/abs/2202.01374v1", "title": "mSLAM: Massively multilingual joint pre-training for speech and text", "cites": 31}, {"url": "http://arxiv.org/abs/2204.05832v1", "title": "What Language Model Architecture and Pretraining Objective Work Best for\n  Zero-Shot Generalization?", "cites": 30}, {"url": "http://arxiv.org/abs/2203.17189v1", "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$", "cites": 30}, {"url": "http://arxiv.org/abs/2202.06991v3", "title": "Transformer Memory as a Differentiable Search Index", "cites": 30}, {"url": "http://arxiv.org/abs/2202.01771v4", "title": "Pre-Trained Language Models for Interactive Decision-Making", "cites": 30}, {"url": "http://arxiv.org/abs/2201.02639v4", "title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and\n  Sound", "cites": 30}, {"url": "http://arxiv.org/abs/2208.01626v1", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "cites": 29}, {"url": "http://arxiv.org/abs/2205.10643v3", "title": "Self-Supervised Speech Representation Learning: A Review", "cites": 29}, {"url": "http://arxiv.org/abs/2203.08913v1", "title": "Memorizing Transformers", "cites": 29}, {"url": "http://arxiv.org/abs/2202.08005v2", "title": "Should You Mask 15% in Masked Language Modeling?", "cites": 29}, {"url": "http://arxiv.org/abs/2201.10005v1", "title": "Text and Code Embeddings by Contrastive Pre-Training", "cites": 29}, {"url": "http://arxiv.org/abs/2202.12837v1", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": 26}, {"url": "http://arxiv.org/abs/2207.05608v1", "title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "cites": 26}, {"url": "http://arxiv.org/abs/2204.10298v1", "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings", "cites": 26}, {"url": "http://arxiv.org/abs/2203.15827v1", "title": "LinkBERT: Pretraining Language Models with Document Links", "cites": 26}, {"url": "http://arxiv.org/abs/2202.06417v3", "title": "A Contrastive Framework for Neural Text Generation", "cites": 26}, {"url": "http://arxiv.org/abs/2203.05557v1", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": 25}, {"url": "http://arxiv.org/abs/2204.08387v3", "title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking", "cites": 25}, {"url": "http://arxiv.org/abs/2204.03162v2", "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic\n  Compositionality", "cites": 25}, {"url": "http://arxiv.org/abs/2203.12468v2", "title": "The VoicePrivacy 2022 Challenge Evaluation Plan", "cites": 24}, {"url": "http://arxiv.org/abs/2201.05966v2", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": 24}, {"url": "http://arxiv.org/abs/2206.14579v2", "title": "Competence-based Multimodal Curriculum Learning for Medical Report\n  Generation", "cites": 24}, {"url": "http://arxiv.org/abs/2203.06849v1", "title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark\n  for Semantic and Generative Capabilities", "cites": 24}, {"url": "http://arxiv.org/abs/2202.06539v2", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "cites": 24}, {"url": "http://arxiv.org/abs/2202.01169v2", "title": "Unified Scaling Laws for Routed Language Models", "cites": 24}, {"url": "http://arxiv.org/abs/2201.06796v2", "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for\n  Exploring Language Model Capabilities", "cites": 24}, {"url": "http://arxiv.org/abs/2203.11171v2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": 23}, {"url": "http://arxiv.org/abs/2205.09712v1", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning", "cites": 23}, {"url": "http://arxiv.org/abs/2204.05862v1", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "cites": 23}, {"url": "http://arxiv.org/abs/2203.12277v1", "title": "Unified Structure Generation for Universal Information Extraction", "cites": 23}, {"url": "http://arxiv.org/abs/2201.06910v2", "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves\n  Zero-Shot Generalization", "cites": 23}, {"url": "http://arxiv.org/abs/2201.03533v2", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences", "cites": 23}, {"url": "http://arxiv.org/abs/2204.10050v2", "title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence\n  Embedding", "cites": 22}, {"url": "http://arxiv.org/abs/2203.03850v1", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "cites": 22}, {"url": "http://arxiv.org/abs/2202.10447v2", "title": "Transformer Quality in Linear Time", "cites": 22}, {"url": "http://arxiv.org/abs/2201.07520v1", "title": "CM3: A Causal Masked Multimodal Model of the Internet", "cites": 22}, {"url": "http://arxiv.org/abs/2204.13258v1", "title": "Cross-modal Memory Networks for Radiology Report Generation", "cites": 21}, {"url": "http://arxiv.org/abs/2201.05337v1", "title": "A Survey of Controllable Text Generation using Transformer-based\n  Pre-trained Language Models", "cites": 21}, {"url": "http://arxiv.org/abs/2207.11893v1", "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020", "cites": 20}, {"url": "http://arxiv.org/abs/2207.05133v1", "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021", "cites": 20}, {"url": "http://arxiv.org/abs/2204.14109v2", "title": "TEMOS: Generating diverse human motions from textual descriptions", "cites": 20}, {"url": "http://arxiv.org/abs/2203.13224v2", "title": "Language Models that Seek for Knowledge: Modular Search & Generation for\n  Dialogue and Prompt Completion", "cites": 20}, {"url": "http://arxiv.org/abs/2203.09509v4", "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and\n  Implicit Hate Speech Detection", "cites": 20}, {"url": "http://arxiv.org/abs/2203.02053v2", "title": "Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive\n  Representation Learning", "cites": 20}, {"url": "http://arxiv.org/abs/2203.00274v2", "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding", "cites": 20}]