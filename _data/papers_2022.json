[{"url": "https://arxiv.org/abs/2203.02155", "title": "Training language models to follow instructions with human feedback", "cites": "3 934", "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.", "no": 1}, {"url": "https://arxiv.org/abs/2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": "2 794", "abstract": "Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.", "no": 2}, {"url": "https://arxiv.org/abs/2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "cites": "2 499", "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.", "no": 3}, {"url": "https://arxiv.org/abs/2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": "1 537", "abstract": "Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.", "no": 4}, {"url": "https://arxiv.org/abs/2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "cites": "1 186", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.", "no": 5}, {"url": "https://arxiv.org/abs/2210.11416", "title": "Scaling Instruction-Finetuned Language Models", "cites": "1 147", "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.", "no": 6}, {"url": "https://arxiv.org/abs/2206.07682", "title": "Emergent Abilities of Large Language Models", "cites": "1 009", "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.", "no": 7}, {"url": "https://arxiv.org/abs/2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "cites": "971", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.", "no": 8}, {"url": "https://arxiv.org/abs/2201.08239", "title": "LaMDA: Language Models for Dialog Applications", "cites": "931", "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.", "no": 9}, {"url": "https://arxiv.org/abs/2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": "883", "abstract": "Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).", "no": 10}, {"url": "https://arxiv.org/abs/2203.15556", "title": "Training Compute-Optimal Large Language Models", "cites": "831", "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.", "no": 11}, {"url": "https://arxiv.org/abs/2212.04356", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "cites": "812", "abstract": "We study the capabilities of speech processing systems trained simply to\npredict large amounts of transcripts of audio on the internet. When scaled to\n680,000 hours of multilingual and multitask supervision, the resulting models\ngeneralize well to standard benchmarks and are often competitive with prior\nfully supervised results but in a zero-shot transfer setting without the need\nfor any fine-tuning. When compared to humans, the models approach their\naccuracy and robustness. We are releasing models and inference code to serve as\na foundation for further work on robust speech processing.", "no": 12}, {"url": "https://arxiv.org/abs/2202.03629", "title": "Survey of Hallucination in Natural Language Generation", "cites": "744", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; and (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation.\nThis survey serves to facilitate collaborative efforts among researchers in\ntackling the challenge of hallucinated texts in NLG.", "no": 13}, {"url": "https://arxiv.org/abs/2204.01691", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "cites": "729", "abstract": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.", "no": 14}, {"url": "https://arxiv.org/abs/2212.10560", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "cites": "648", "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.", "no": 15}, {"url": "https://arxiv.org/abs/2208.01618", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion", "cites": "618", "abstract": "Text-to-image models offer unprecedented freedom to guide creation through\nnatural language. Yet, it is unclear how such freedom can be exercised to\ngenerate images of specific unique concepts, modify their appearance, or\ncompose them in new roles and novel scenes. In other words, we ask: how can we\nuse language-guided models to turn our cat into a painting, or imagine a new\nproduct based on our favorite toy? Here we present a simple approach that\nallows such creative freedom. Using only 3-5 images of a user-provided concept,\nlike an object or a style, we learn to represent it through new \"words\" in the\nembedding space of a frozen text-to-image model. These \"words\" can be composed\ninto natural language sentences, guiding personalized creation in an intuitive\nway. Notably, we find evidence that a single word embedding is sufficient for\ncapturing unique and varied concepts. We compare our approach to a wide range\nof baselines, and demonstrate that it can more faithfully portray the concepts\nacross a range of applications and tasks.\n  Our code, data and new words will be available at:\nhttps://textual-inversion.github.io", "no": 16}, {"url": "https://arxiv.org/abs/2206.04615", "title": "Beyond the Imitation Game: Quantifying and extrapolating the\n  capabilities of language models", "cites": "581", "abstract": "Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.", "no": 17}, {"url": "https://arxiv.org/abs/2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "cites": "572", "abstract": "Recent large-scale text-driven synthesis models have attracted much attention\nthanks to their remarkable capabilities of generating highly diverse images\nthat follow given text prompts. Such text-based synthesis methods are\nparticularly appealing to humans who are used to verbally describe their\nintent. Therefore, it is only natural to extend the text-driven image synthesis\nto text-driven image editing. Editing is challenging for these generative\nmodels, since an innate property of an editing technique is to preserve most of\nthe original image, while in the text-based models, even a small modification\nof the text prompt often leads to a completely different outcome.\nState-of-the-art methods mitigate this by requiring the users to provide a\nspatial mask to localize the edit, hence, ignoring the original structure and\ncontent within the masked region. In this paper, we pursue an intuitive\nprompt-to-prompt editing framework, where the edits are controlled by text\nonly. To this end, we analyze a text-conditioned model in depth and observe\nthat the cross-attention layers are the key to controlling the relation between\nthe spatial layout of the image to each word in the prompt. With this\nobservation, we present several applications which monitor the image synthesis\nby editing the textual prompt only. This includes localized editing by\nreplacing a word, global editing by adding a specification, and even delicately\ncontrolling the extent to which a word is reflected in the image. We present\nour results over diverse images and prompts, demonstrating high-quality\nsynthesis and fidelity to the edited prompts.", "no": 18}, {"url": "https://arxiv.org/abs/2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "cites": "566", "abstract": "We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.", "no": 19}, {"url": "https://arxiv.org/abs/2202.12837", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": "557", "abstract": "Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.", "no": 20}, {"url": "https://arxiv.org/abs/2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "cites": "486", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io", "no": 21}, {"url": "https://arxiv.org/abs/2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": "485", "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning -- a recent trend in NLP -- to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp's static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.", "no": 22}, {"url": "https://arxiv.org/abs/2202.03052", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "cites": "484", "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA.", "no": 23}, {"url": "https://arxiv.org/abs/2201.11990", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "cites": "475", "abstract": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.", "no": 24}, {"url": "https://arxiv.org/abs/2203.05794", "title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "cites": "457", "abstract": "Topic models can be useful tools to discover latent topics in collections of\ndocuments. Recent studies have shown the feasibility of approach topic modeling\nas a clustering task. We present BERTopic, a topic model that extends this\nprocess by extracting coherent topic representation through the development of\na class-based variation of TF-IDF. More specifically, BERTopic generates\ndocument embedding with pre-trained transformer-based language models, clusters\nthese embeddings, and finally, generates topic representations with the\nclass-based TF-IDF procedure. BERTopic generates coherent topics and remains\ncompetitive across a variety of benchmarks involving classical models and those\nthat follow the more recent clustering approach of topic modeling.", "no": 25}, {"url": "https://arxiv.org/abs/2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "cites": "452", "abstract": "We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models -- a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) -- to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.", "no": 26}, {"url": "https://arxiv.org/abs/2212.13138", "title": "Large Language Models Encode Clinical Knowledge", "cites": "452", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.", "no": 27}, {"url": "https://arxiv.org/abs/2205.10625", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "cites": "442", "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.", "no": 28}, {"url": "https://arxiv.org/abs/2205.06175", "title": "A Generalist Agent", "cites": "439", "abstract": "Inspired by progress in large-scale language modeling, we apply a similar\napproach towards building a single generalist agent beyond the realm of text\noutputs. The agent, which we refer to as Gato, works as a multi-modal,\nmulti-task, multi-embodiment generalist policy. The same network with the same\nweights can play Atari, caption images, chat, stack blocks with a real robot\narm and much more, deciding based on its context whether to output text, joint\ntorques, button presses, or other tokens. In this report we describe the model\nand the data, and document the current capabilities of Gato.", "no": 29}, {"url": "https://arxiv.org/abs/2210.02414", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "cites": "427", "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\n\\url{https://github.com/THUDM/GLM-130B/}.", "no": 30}, {"url": "https://arxiv.org/abs/2201.07207", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "cites": "414", "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner", "no": 31}, {"url": "https://arxiv.org/abs/2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "cites": "408", "abstract": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.", "no": 32}, {"url": "https://arxiv.org/abs/2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "cites": "403", "abstract": "A big convergence of language, vision, and multimodal pretraining is\nemerging. In this work, we introduce a general-purpose multimodal foundation\nmodel BEiT-3, which achieves state-of-the-art transfer performance on both\nvision and vision-language tasks. Specifically, we advance the big convergence\nfrom three aspects: backbone architecture, pretraining task, and model scaling\nup. We introduce Multiway Transformers for general-purpose modeling, where the\nmodular architecture enables both deep fusion and modality-specific encoding.\nBased on the shared backbone, we perform masked \"language\" modeling on images\n(Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a\nunified manner. Experimental results show that BEiT-3 obtains state-of-the-art\nperformance on object detection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual question answering\n(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).", "no": 33}, {"url": "https://arxiv.org/abs/2204.06745", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "cites": "395", "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}'s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.", "no": 34}, {"url": "https://arxiv.org/abs/2207.04672", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "cites": "383", "abstract": "Driven by the goal of eradicating language barriers on a global scale,\nmachine translation has solidified itself as a key focus of artificial\nintelligence research today. However, such efforts have coalesced around a\nsmall subset of languages, leaving behind the vast majority of mostly\nlow-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical\nconsiderations in mind? In No Language Left Behind, we took on this challenge\nby first contextualizing the need for low-resource language translation support\nthrough exploratory interviews with native speakers. Then, we created datasets\nand models aimed at narrowing the performance gap between low and high-resource\nlanguages. More specifically, we developed a conditional compute model based on\nSparsely Gated Mixture of Experts that is trained on data obtained with novel\nand effective data mining techniques tailored for low-resource languages. We\npropose multiple architectural and training improvements to counteract\noverfitting while training on thousands of tasks. Critically, we evaluated the\nperformance of over 40,000 different translation directions using a\nhuman-translated benchmark, Flores-200, and combined human evaluation with a\nnovel toxicity benchmark covering all languages in Flores-200 to assess\ntranslation safety. Our model achieves an improvement of 44% BLEU relative to\nthe previous state-of-the-art, laying important groundwork towards realizing a\nuniversal translation system. Finally, we open source all contributions\ndescribed in this work, accessible at\nhttps://github.com/facebookresearch/fairseq/tree/nllb.", "no": 35}, {"url": "https://arxiv.org/abs/2207.05608", "title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "cites": "347", "abstract": "Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.", "no": 36}, {"url": "https://arxiv.org/abs/2203.05482", "title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "cites": "344", "abstract": "The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.", "no": 37}, {"url": "https://arxiv.org/abs/2205.14217", "title": "Diffusion-LM Improves Controllable Text Generation", "cites": "339", "abstract": "Controlling the behavior of language models (LMs) without re-training is a\nmajor open problem in natural language generation. While recent works have\ndemonstrated successes on controlling simple sentence attributes (e.g.,\nsentiment), there has been little progress on complex, fine-grained controls\n(e.g., syntactic structure). To address this challenge, we develop a new\nnon-autoregressive language model based on continuous diffusions that we call\nDiffusion-LM. Building upon the recent successes of diffusion models in\ncontinuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian\nvectors into word vectors, yielding a sequence of intermediate latent\nvariables. The continuous, hierarchical nature of these intermediate variables\nenables a simple gradient-based algorithm to perform complex, controllable\ngeneration tasks. We demonstrate successful control of Diffusion-LM for six\nchallenging fine-grained control tasks, significantly outperforming prior work.", "no": 38}, {"url": "https://arxiv.org/abs/2206.14858", "title": "Solving Quantitative Reasoning Problems with Language Models", "cites": "331", "abstract": "Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.", "no": 39}, {"url": "https://arxiv.org/abs/2211.09085", "title": "Galactica: A Large Language Model for Science", "cites": "309", "abstract": "Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.", "no": 40}, {"url": "https://arxiv.org/abs/2204.07705", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions\n  on 1600+ NLP Tasks", "cites": "302", "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided\nwith task instructions? To address this question, we first introduce\nSuper-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their\nexpert-written instructions. Our collection covers 76 distinct task types,\nincluding but not limited to classification, extraction, infilling, sequence\ntagging, text rewriting, and text composition. This large and diverse\ncollection of tasks enables rigorous benchmarking of cross-task generalization\nunder instructions -- training models to follow instructions on a subset of\ntasks and evaluating them on the remaining unseen ones. Furthermore, we build\nTk-Instruct, a transformer model trained to follow a variety of in-context\ninstructions (plain language task definitions or k-shot examples). Our\nexperiments show that Tk-Instruct outperforms existing instruction-following\nmodels such as InstructGPT by over 9% on our benchmark despite being an order\nof magnitude smaller. We further analyze generalization as a function of\nvarious scaling parameters, such as the number of observed tasks, the number of\ninstances per task, and model sizes. We hope our dataset and model facilitate\nfuture progress towards more general-purpose NLP models.", "no": 41}, {"url": "https://arxiv.org/abs/2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "cites": "298", "abstract": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.", "no": 42}, {"url": "https://arxiv.org/abs/2209.06794", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "cites": "291", "abstract": "Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.", "no": 43}, {"url": "https://arxiv.org/abs/2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis", "cites": "290", "abstract": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.", "no": 44}, {"url": "https://arxiv.org/abs/2212.06817", "title": "RT-1: Robotics Transformer for Real-World Control at Scale", "cites": "284", "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer1.github.io", "no": 45}, {"url": "https://arxiv.org/abs/2203.13131", "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "cites": "278", "abstract": "Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.", "no": 46}, {"url": "https://arxiv.org/abs/2201.03546", "title": "Language-driven Semantic Segmentation", "cites": "275", "abstract": "We present LSeg, a novel model for language-driven semantic image\nsegmentation. LSeg uses a text encoder to compute embeddings of descriptive\ninput labels (e.g., \"grass\" or \"building\") together with a transformer-based\nimage encoder that computes dense per-pixel embeddings of the input image. The\nimage encoder is trained with a contrastive objective to align pixel embeddings\nto the text embedding of the corresponding semantic class. The text embeddings\nprovide a flexible label representation in which semantically similar labels\nmap to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This\nallows LSeg to generalize to previously unseen categories at test time, without\nretraining or even requiring a single additional training sample. We\ndemonstrate that our approach achieves highly competitive zero-shot performance\ncompared to existing zero- and few-shot semantic segmentation methods, and even\nmatches the accuracy of traditional segmentation algorithms when a fixed label\nset is provided. Code and demo are available at\nhttps://github.com/isl-org/lang-seg.", "no": 47}, {"url": "https://arxiv.org/abs/2202.05262", "title": "Locating and Editing Factual Associations in GPT", "cites": "272", "abstract": "We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel's factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/", "no": 48}, {"url": "https://arxiv.org/abs/2204.05999", "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "cites": "271", "abstract": "Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models", "no": 49}, {"url": "https://arxiv.org/abs/2202.07646", "title": "Quantifying Memorization Across Neural Language Models", "cites": "266", "abstract": "Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.", "no": 50}, {"url": "https://arxiv.org/abs/2211.01910", "title": "Large Language Models Are Human-Level Prompt Engineers", "cites": "259", "abstract": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.", "no": 51}, {"url": "https://arxiv.org/abs/2210.10341", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text\n  Generation and Mining", "cites": "257", "abstract": "Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.", "no": 52}, {"url": "https://arxiv.org/abs/2205.05638", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning", "cites": "256", "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)$^3$ that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.", "no": 53}, {"url": "https://arxiv.org/abs/2210.09261", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "cites": "245", "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\nfocuses on tasks believed to be beyond the capabilities of current language\nmodels. Language models have already made good progress on this benchmark, with\nthe best model in the BIG-Bench paper outperforming average reported\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\non what tasks do language models fall short of average human-rater performance,\nand are those tasks actually unsolvable by current language models?\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\nevaluations did not outperform the average human-rater. We find that applying\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\naverage human-rater performance on 10 of the 23 tasks, and Codex\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\n2022), substantially underestimates the best performance and capabilities of\nlanguage models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH,\nfinding that CoT enables emergent task performance on several BBH tasks with\notherwise flat scaling curves.", "no": 54}, {"url": "https://arxiv.org/abs/2208.03299", "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "cites": "242", "abstract": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.", "no": 55}, {"url": "https://arxiv.org/abs/2202.13169", "title": "A Systematic Evaluation of Large Language Models of Code", "cites": "237", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.", "no": 56}, {"url": "https://arxiv.org/abs/2209.14375", "title": "Improving alignment of dialogue agents via targeted human judgements", "cites": "237", "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.", "no": 57}, {"url": "https://arxiv.org/abs/2202.03286", "title": "Red Teaming Language Models with Language Models", "cites": "236", "abstract": "Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (\"red teaming\") using another LM. We\nevaluate the target LM's replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot's\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.", "no": 58}, {"url": "https://arxiv.org/abs/2210.03493", "title": "Automatic Chain of Thought Prompting in Large Language Models", "cites": "234", "abstract": "Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot", "no": 59}, {"url": "https://arxiv.org/abs/2211.10435", "title": "PAL: Program-aided Language Models", "cites": "214", "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .", "no": 60}, {"url": "https://arxiv.org/abs/2211.12588", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "cites": "212", "abstract": "Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts", "no": 61}, {"url": "https://arxiv.org/abs/2207.05221", "title": "Language Models (Mostly) Know What They Know", "cites": "209", "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.", "no": 62}, {"url": "https://arxiv.org/abs/2210.03350", "title": "Measuring and Narrowing the Compositionality Gap in Language Models", "cites": "204", "abstract": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.", "no": 63}, {"url": "https://arxiv.org/abs/2203.03850", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "cites": "201", "abstract": "Pre-trained models for programming languages have recently demonstrated great\nsuccess on code intelligence. To support both code-related understanding and\ngeneration tasks, recent works attempt to pre-train unified encoder-decoder\nmodels. However, such encoder-decoder framework is sub-optimal for\nauto-regressive tasks, especially code completion that requires a decoder-only\nmanner for efficient inference. In this paper, we present UniXcoder, a unified\ncross-modal pre-trained model for programming language. The model utilizes mask\nattention matrices with prefix adapters to control the behavior of the model\nand leverages cross-modal contents like AST and code comment to enhance code\nrepresentation. To encode AST that is represented as a tree in parallel, we\npropose a one-to-one mapping method to transform AST in a sequence structure\nthat retains all structural information from the tree. Furthermore, we propose\nto utilize multi-modal contents to learn representation of code fragment with\ncontrastive learning, and then align representations among programming\nlanguages using a cross-modal generation task. We evaluate UniXcoder on five\ncode-related tasks over nine datasets. To further evaluate the performance of\ncode fragment representation, we also construct a dataset for a new task,\ncalled zero-shot code-to-code search. Results show that our model achieves\nstate-of-the-art performance on most tasks and analysis reveals that comment\nand AST can both enhance UniXcoder.", "no": 64}, {"url": "https://arxiv.org/abs/2202.01279", "title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "cites": "200", "abstract": "PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.", "no": 65}, {"url": "https://arxiv.org/abs/2211.07636", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at\n  Scale", "cites": "198", "abstract": "We launch EVA, a vision-centric foundation model to explore the limits of\nvisual representation at scale using only publicly accessible data. EVA is a\nvanilla ViT pre-trained to reconstruct the masked out image-text aligned vision\nfeatures conditioned on visible image patches. Via this pretext task, we can\nefficiently scale up EVA to one billion parameters, and sets new records on a\nbroad range of representative vision downstream tasks, such as image\nrecognition, video action recognition, object detection, instance segmentation\nand semantic segmentation without heavy supervised training. Moreover, we\nobserve quantitative changes in scaling EVA result in qualitative changes in\ntransfer learning performance that are not present in other models. For\ninstance, EVA takes a great leap in the challenging large vocabulary instance\nsegmentation task: our model achieves almost the same state-of-the-art\nperformance on LVISv1.0 dataset with over a thousand categories and COCO\ndataset with only eighty categories. Beyond a pure vision encoder, EVA can also\nserve as a vision-centric, multi-modal pivot to connect images and text. We\nfind initializing the vision tower of a giant CLIP from EVA can greatly\nstabilize the training and outperform the training from scratch counterpart\nwith much fewer samples and less compute, providing a new direction for scaling\nup and accelerating the costly training of multi-modal foundation models. To\nfacilitate future research, we release all the code and models at\nhttps://github.com/baaivision/EVA.", "no": 66}, {"url": "https://arxiv.org/abs/2201.05966", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": "192", "abstract": "Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.", "no": 67}, {"url": "https://arxiv.org/abs/2209.11302", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models", "cites": "191", "abstract": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io", "no": 68}, {"url": "https://arxiv.org/abs/2205.15868", "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via\n  Transformers", "cites": "187", "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.", "no": 69}, {"url": "https://arxiv.org/abs/2209.09513", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science\n  Question Answering", "cites": "185", "abstract": "When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.", "no": 70}, {"url": "https://arxiv.org/abs/2203.14465", "title": "STaR: Bootstrapping Reasoning With Reasoning", "cites": "178", "abstract": "Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.", "no": 71}, {"url": "https://arxiv.org/abs/2210.11610", "title": "Large Language Models Can Self-Improve", "cites": "176", "abstract": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.", "no": 72}, {"url": "https://arxiv.org/abs/2205.09712", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning", "cites": "170", "abstract": "Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.", "no": 73}, {"url": "https://arxiv.org/abs/2209.05451", "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", "cites": "170", "abstract": "Transformers have revolutionized vision and natural language processing with\ntheir ability to scale with large datasets. But in robotic manipulation, data\nis both limited and expensive. Can manipulation still benefit from Transformers\nwith the right problem formulation? We investigate this question with PerAct, a\nlanguage-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.\nPerAct encodes language goals and RGB-D voxel observations with a Perceiver\nTransformer, and outputs discretized actions by ``detecting the next best voxel\naction''. Unlike frameworks that operate on 2D images, the voxelized 3D\nobservation and action space provides a strong structural prior for efficiently\nlearning 6-DoF actions. With this formulation, we train a single multi-task\nTransformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks\n(with 18 variations) from just a few demonstrations per task. Our results show\nthat PerAct significantly outperforms unstructured image-to-action agents and\n3D ConvNet baselines for a wide range of tabletop tasks.", "no": 74}, {"url": "https://arxiv.org/abs/2204.03162", "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic\n  Compositionality", "cites": "168", "abstract": "We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.", "no": 75}, {"url": "https://arxiv.org/abs/2209.12356", "title": "News Summarization and Evaluation in the Era of GPT-3", "cites": "168", "abstract": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.", "no": 76}, {"url": "https://arxiv.org/abs/2205.01996", "title": "EmoBank: Studying the Impact of Annotation Perspective and\n  Representation Format on Dimensional Emotion Analysis", "cites": "167", "abstract": "We describe EmoBank, a corpus of 10k English sentences balancing multiple\ngenres, which we annotated with dimensional emotion metadata in the\nValence-Arousal-Dominance (VAD) representation format. EmoBank excels with a\nbi-perspectival and bi-representational design. On the one hand, we distinguish\nbetween writer's and reader's emotions, on the other hand, a subset of the\ncorpus complements dimensional VAD annotations with categorical ones based on\nBasic Emotions. We find evidence for the supremacy of the reader's perspective\nin terms of IAA and rating intensity, and achieve close-to-human performance\nwhen mapping between dimensional and categorical formats.", "no": 77}, {"url": "https://arxiv.org/abs/2211.15661", "title": "What learning algorithm is in-context learning? Investigations with\n  linear models", "cites": "167", "abstract": "Neural sequence models, especially transformers, exhibit a remarkable\ncapacity for in-context learning. They can construct new predictors from\nsequences of labeled examples $(x, f(x))$ presented in the input without\nfurther parameter updates. We investigate the hypothesis that transformer-based\nin-context learners implement standard learning algorithms implicitly, by\nencoding smaller models in their activations, and updating these implicit\nmodels as new examples appear in the context. Using linear regression as a\nprototypical problem, we offer three sources of evidence for this hypothesis.\nFirst, we prove by construction that transformers can implement learning\nalgorithms for linear models based on gradient descent and closed-form ridge\nregression. Second, we show that trained in-context learners closely match the\npredictors computed by gradient descent, ridge regression, and exact\nleast-squares regression, transitioning between different predictors as\ntransformer depth and dataset noise vary, and converging to Bayesian estimators\nfor large widths and depths. Third, we present preliminary evidence that\nin-context learners share algorithmic features with these predictors: learners'\nlate layers non-linearly encode weight vectors and moment matrices. These\nresults suggest that in-context learning is understandable in algorithmic\nterms, and that (at least in the linear case) learners may rediscover standard\nestimation algorithms. Code and reference implementations are released at\nhttps://github.com/ekinakyurek/google-research/blob/master/incontext.", "no": 78}, {"url": "https://arxiv.org/abs/2204.08387", "title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking", "cites": "165", "abstract": "Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.", "no": 79}, {"url": "https://arxiv.org/abs/2209.07858", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors,\n  and Lessons Learned", "cites": "161", "abstract": "We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.", "no": 80}, {"url": "https://arxiv.org/abs/2205.10643", "title": "Self-Supervised Speech Representation Learning: A Review", "cites": "159", "abstract": "Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.", "no": 81}, {"url": "https://arxiv.org/abs/2203.12277", "title": "Unified Structure Generation for Universal Information Extraction", "cites": "156", "abstract": "Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.", "no": 82}, {"url": "https://arxiv.org/abs/2208.03188", "title": "BlenderBot 3: a deployed conversational agent that continually learns to\n  responsibly engage", "cites": "156", "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of\nopen-domain conversation with access to the internet and a long-term memory,\nand having been trained on a large number of user defined tasks. We release\nboth the model weights and code, and have also deployed the model on a public\nweb page to interact with organic users. This technical report describes how\nthe model was built (architecture, model and training scheme), and details of\nits deployment, including safety mechanisms. Human evaluations show its\nsuperiority to existing open-domain dialogue agents, including its predecessors\n(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for\ncontinual learning using the data collected from deployment, which will also be\npublicly released. The goal of this research program is thus to enable the\ncommunity to study ever-improving responsible agents that learn through\ninteraction.", "no": 83}, {"url": "https://arxiv.org/abs/2204.02329", "title": "Can language models learn from explanations in context?", "cites": "155", "abstract": "Language Models (LMs) can perform new tasks by adapting to a few in-context\nexamples. For humans, explanations that connect examples to task principles can\nimprove learning. We therefore investigate whether explanations of few-shot\nexamples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how\ndifferent types of explanations, instructions, and controls affect zero- and\nfew-shot performance. We analyze these results using statistical multilevel\nmodeling techniques that account for the nested dependencies among conditions,\ntasks, prompts, and models. We find that explanations can improve performance\n-- even without tuning. Furthermore, explanations hand-tuned for performance on\na small validation set offer substantially larger benefits, and building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Finally, even untuned explanations\noutperform carefully matched controls, suggesting that the benefits are due to\nthe link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit. In summary, explanations can\nsupport the in-context learning of large LMs on challenging tasks.", "no": 84}, {"url": "https://arxiv.org/abs/2203.15827", "title": "LinkBERT: Pretraining Language Models with Document Links", "cites": "154", "abstract": "Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.", "no": 85}, {"url": "https://arxiv.org/abs/2208.01066", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function\n  Classes", "cites": "152", "abstract": "In-context learning refers to the ability of a model to condition on a prompt\nsequence consisting of in-context examples (input-output pairs corresponding to\nsome task) along with a new query input, and generate the corresponding output.\nCrucially, in-context learning happens only at inference time without any\nparameter updates to the model. While large language models such as GPT-3\nexhibit some ability to perform in-context learning, it is unclear what the\nrelationship is between tasks on which this succeeds and what is present in the\ntraining data. To make progress towards understanding in-context learning, we\nconsider the well-defined problem of training a model to in-context learn a\nfunction class (e.g., linear functions): that is, given data derived from some\nfunctions in the class, can we train a model to in-context learn \"most\"\nfunctions from this class? We show empirically that standard Transformers can\nbe trained from scratch to perform in-context learning of linear functions --\nthat is, the trained model is able to learn unseen linear functions from\nin-context examples with performance comparable to the optimal least squares\nestimator. In fact, in-context learning is possible even under two forms of\ndistribution shift: (i) between the training data of the model and\ninference-time prompts, and (ii) between the in-context examples and the query\ninput during inference. We also show that we can train Transformers to\nin-context learn more complex function classes -- namely sparse linear\nfunctions, two-layer neural networks, and decision trees -- with performance\nthat matches or exceeds task-specific learning algorithms. Our code and models\nare available at https://github.com/dtsip/in-context-learning .", "no": 86}, {"url": "https://arxiv.org/abs/2212.09292", "title": "ChatGPT: The End of Online Exam Integrity?", "cites": "150", "abstract": "This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.", "no": 87}, {"url": "https://arxiv.org/abs/2212.10403", "title": "Towards Reasoning in Large Language Models: A Survey", "cites": "150", "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.", "no": 88}, {"url": "https://arxiv.org/abs/2207.04429", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,\n  Vision, and Action", "cites": "148", "abstract": "Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav", "no": 89}, {"url": "https://arxiv.org/abs/2212.12017", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the\n  Lens of Generalization", "cites": "148", "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats -- PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.", "no": 90}, {"url": "https://arxiv.org/abs/2203.16804", "title": "BRIO: Bringing Order to Abstractive Summarization", "cites": "147", "abstract": "Abstractive summarization models are commonly trained using maximum\nlikelihood estimation, which assumes a deterministic (one-point) target\ndistribution in which an ideal model will assign all the probability mass to\nthe reference summary. This assumption may lead to performance degradation\nduring inference, where the model needs to compare several system-generated\n(candidate) summaries that have deviated from the reference summary. To address\nthis problem, we propose a novel training paradigm which assumes a\nnon-deterministic distribution so that different candidate summaries are\nassigned probability mass according to their quality. Our method achieves a new\nstate-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07\nROUGE-1) datasets. Further analysis also shows that our model can estimate\nprobabilities of candidate summaries that are more correlated with their level\nof quality.", "no": 91}, {"url": "https://arxiv.org/abs/2206.08853", "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge", "cites": "147", "abstract": "Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.", "no": 92}, {"url": "https://arxiv.org/abs/2210.00720", "title": "Complexity-Based Prompting for Multi-Step Reasoning", "cites": "146", "abstract": "We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.", "no": 93}, {"url": "https://arxiv.org/abs/2206.05836", "title": "GLIPv2: Unifying Localization and Vision-Language Understanding", "cites": "145", "abstract": "We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.", "no": 94}, {"url": "https://arxiv.org/abs/2206.14576", "title": "Using cognitive psychology to understand GPT-3", "cites": "144", "abstract": "We study GPT-3, a recent large language model, using tools from cognitive\npsychology. More specifically, we assess GPT-3's decision-making, information\nsearch, deliberation, and causal reasoning abilities on a battery of canonical\nexperiments from the literature. We find that much of GPT-3's behavior is\nimpressive: it solves vignette-based tasks similarly or better than human\nsubjects, is able to make decent decisions from descriptions, outperforms\nhumans in a multi-armed bandit task, and shows signatures of model-based\nreinforcement learning. Yet we also find that small perturbations to\nvignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures\nof directed exploration, and that it fails miserably in a causal reasoning\ntask. These results enrich our understanding of current large language models\nand pave the way for future investigations using tools from cognitive\npsychology to study increasingly capable and opaque artificial agents.", "no": 95}, {"url": "https://arxiv.org/abs/2201.06796", "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for\n  Exploring Language Model Capabilities", "cites": "139", "abstract": "Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs' generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3's\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3's language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing \"collaborator\" under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs' promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.", "no": 96}, {"url": "https://arxiv.org/abs/2203.03540", "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information\n  from Unstructured Electronic Health Records", "cites": "137", "abstract": "There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using >90 billion words of text (including >82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.", "no": 97}, {"url": "https://arxiv.org/abs/2210.02406", "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks", "cites": "137", "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.", "no": 98}, {"url": "https://arxiv.org/abs/2201.10005", "title": "Text and Code Embeddings by Contrastive Pre-Training", "cites": "136", "abstract": "Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.", "no": 99}, {"url": "https://arxiv.org/abs/2205.05131", "title": "UL2: Unifying Language Learning Paradigms", "cites": "136", "abstract": "Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "no": 100}]