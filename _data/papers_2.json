[{"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": 2798}, {"url": "https://arxiv.org/abs/2203.02155", "title": "Training language models to follow instructions with human feedback", "cites": 2677}, {"url": "https://arxiv.org/abs/2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "cites": 2065}, {"url": "https://arxiv.org/abs/2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "cites": 1871}, {"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": 1749}, {"url": "https://arxiv.org/abs/2303.08774", "title": "GPT-4 Technical Report", "cites": 1308}, {"url": "https://arxiv.org/abs/2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "cites": 1153}, {"url": "https://arxiv.org/abs/2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "cites": 776}, {"url": "https://arxiv.org/abs/2201.08239", "title": "LaMDA: Language Models for Dialog Applications", "cites": 775}, {"url": "https://arxiv.org/abs/2206.07682", "title": "Emergent Abilities of Large Language Models", "cites": 759}, {"url": "https://arxiv.org/abs/2210.11416", "title": "Scaling Instruction-Finetuned Language Models", "cites": 694}, {"url": "https://arxiv.org/abs/2203.15556", "title": "Training Compute-Optimal Large Language Models", "cites": 690}, {"url": "https://arxiv.org/abs/2303.12712", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "cites": 676}, {"url": "https://arxiv.org/abs/2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "cites": 670}, {"url": "https://arxiv.org/abs/2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "cites": 578}, {"url": "https://arxiv.org/abs/2212.04356", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "cites": 546}, {"url": "https://arxiv.org/abs/2204.01691", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "cites": 540}, {"url": "https://arxiv.org/abs/2202.03629", "title": "Survey of Hallucination in Natural Language Generation", "cites": 539}, {"url": "https://arxiv.org/abs/2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "cites": 505}, {"url": "https://arxiv.org/abs/2202.03052", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "cites": 460}, {"url": "https://arxiv.org/abs/2201.11990", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "cites": 412}, {"url": "https://arxiv.org/abs/2208.01618", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion", "cites": 407}, {"url": "https://arxiv.org/abs/2303.18223", "title": "A Survey of Large Language Models", "cites": 402}, {"url": "https://arxiv.org/abs/2202.12837", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "cites": 387}, {"url": "https://arxiv.org/abs/2212.10560", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "cites": 377}, {"url": "https://arxiv.org/abs/2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "cites": 374}, {"url": "https://arxiv.org/abs/2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "cites": 373}, {"url": "https://arxiv.org/abs/2308.02976", "title": "Spanish Pre-trained BERT Model and Evaluation Data", "cites": 371}, {"url": "https://arxiv.org/abs/2205.06175", "title": "A Generalist Agent", "cites": 363}, {"url": "https://arxiv.org/abs/2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "cites": 350}, {"url": "https://arxiv.org/abs/2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "cites": 346}, {"url": "https://arxiv.org/abs/2302.04023", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on\n  Reasoning, Hallucination, and Interactivity", "cites": 341}, {"url": "https://arxiv.org/abs/2203.05794", "title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "cites": 335}, {"url": "https://arxiv.org/abs/2205.10625", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "cites": 333}, {"url": "https://arxiv.org/abs/2204.06745", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "cites": 306}, {"url": "https://arxiv.org/abs/2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "cites": 303}, {"url": "https://arxiv.org/abs/2201.07207", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "cites": 302}, {"url": "https://arxiv.org/abs/2203.05482", "title": "Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time", "cites": 289}, {"url": "https://arxiv.org/abs/2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "cites": 285}, {"url": "https://arxiv.org/abs/2207.04672", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "cites": 271}, {"url": "https://arxiv.org/abs/2210.02414", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "cites": 268}, {"url": "https://arxiv.org/abs/2205.14217", "title": "Diffusion-LM Improves Controllable Text Generation", "cites": 261}, {"url": "https://arxiv.org/abs/2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "cites": 261}, {"url": "https://arxiv.org/abs/2206.14858", "title": "Solving Quantitative Reasoning Problems with Language Models", "cites": 259}, {"url": "https://arxiv.org/abs/2212.13138", "title": "Large Language Models Encode Clinical Knowledge", "cites": 256}, {"url": "https://arxiv.org/abs/2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "cites": 251}, {"url": "https://arxiv.org/abs/2207.05608", "title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "cites": 242}, {"url": "https://arxiv.org/abs/2203.13131", "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "cites": 238}, {"url": "https://arxiv.org/abs/2305.10403", "title": "PaLM 2 Technical Report", "cites": 238}, {"url": "https://arxiv.org/abs/2308.11995", "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "cites": 237}, {"url": "https://arxiv.org/abs/2304.08485", "title": "Visual Instruction Tuning", "cites": 229}, {"url": "https://arxiv.org/abs/2211.09085", "title": "Galactica: A Large Language Model for Science", "cites": 221}, {"url": "https://arxiv.org/abs/2303.17580", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "cites": 221}, {"url": "https://arxiv.org/abs/2204.05999", "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "cites": 214}, {"url": "https://arxiv.org/abs/2202.07646", "title": "Quantifying Memorization Across Neural Language Models", "cites": 212}, {"url": "https://arxiv.org/abs/2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis", "cites": 211}, {"url": "https://arxiv.org/abs/2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "cites": 209}, {"url": "https://arxiv.org/abs/2201.03546", "title": "Language-driven Semantic Segmentation", "cites": 208}, {"url": "https://arxiv.org/abs/2204.07705", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions\n  on 1600+ NLP Tasks", "cites": 208}, {"url": "https://arxiv.org/abs/2209.06794", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "cites": 206}, {"url": "https://arxiv.org/abs/2302.06476", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "cites": 200}, {"url": "https://arxiv.org/abs/2306.05685", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "cites": 188}, {"url": "https://arxiv.org/abs/2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "cites": 185}, {"url": "https://arxiv.org/abs/2202.13169", "title": "A Systematic Evaluation of Large Language Models of Code", "cites": 181}, {"url": "https://arxiv.org/abs/2210.10341", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text\n  Generation and Mining", "cites": 181}, {"url": "https://arxiv.org/abs/2208.03299", "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "cites": 180}, {"url": "https://arxiv.org/abs/2210.03493", "title": "Automatic Chain of Thought Prompting in Large Language Models", "cites": 179}, {"url": "https://arxiv.org/abs/2205.05638", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning", "cites": 175}, {"url": "https://arxiv.org/abs/2212.06817", "title": "RT-1: Robotics Transformer for Real-World Control at Scale", "cites": 175}, {"url": "https://arxiv.org/abs/2211.10435", "title": "PAL: Program-aided Language Models", "cites": 172}, {"url": "https://arxiv.org/abs/2202.05262", "title": "Locating and Editing Factual Associations in GPT", "cites": 169}, {"url": "https://arxiv.org/abs/2202.03286", "title": "Red Teaming Language Models with Language Models", "cites": 167}, {"url": "https://arxiv.org/abs/2303.13375", "title": "Capabilities of GPT-4 on Medical Challenge Problems", "cites": 166}, {"url": "https://arxiv.org/abs/2201.05966", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "cites": 165}, {"url": "https://arxiv.org/abs/2209.14375", "title": "Improving alignment of dialogue agents via targeted human judgements", "cites": 165}, {"url": "https://arxiv.org/abs/2202.01279", "title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "cites": 163}, {"url": "https://arxiv.org/abs/2304.03277", "title": "Instruction Tuning with GPT-4", "cites": 163}, {"url": "https://arxiv.org/abs/2211.01910", "title": "Large Language Models Are Human-Level Prompt Engineers", "cites": 160}, {"url": "https://arxiv.org/abs/2210.09261", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "cites": 158}, {"url": "https://arxiv.org/abs/2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "cites": 158}, {"url": "https://arxiv.org/abs/2205.01996", "title": "EmoBank: Studying the Impact of Annotation Perspective and\n  Representation Format on Dimensional Emotion Analysis", "cites": 155}, {"url": "https://arxiv.org/abs/2203.03850", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "cites": 154}, {"url": "https://arxiv.org/abs/2203.14465", "title": "STaR: Bootstrapping Reasoning With Reasoning", "cites": 151}, {"url": "https://arxiv.org/abs/2301.07597", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,\n  and Detection", "cites": 151}, {"url": "https://arxiv.org/abs/2301.13688", "title": "The Flan Collection: Designing Data and Methods for Effective\n  Instruction Tuning", "cites": 148}, {"url": "https://arxiv.org/abs/2302.03494", "title": "A Categorical Archive of ChatGPT Failures", "cites": 148}, {"url": "https://arxiv.org/abs/2207.05221", "title": "Language Models (Mostly) Know What They Know", "cites": 143}, {"url": "https://arxiv.org/abs/2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "cites": 141}, {"url": "https://arxiv.org/abs/2304.01373", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "cites": 138}, {"url": "https://arxiv.org/abs/2211.12588", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "cites": 135}, {"url": "https://arxiv.org/abs/2209.12356", "title": "News Summarization and Evaluation in the Era of GPT-3", "cites": 134}, {"url": "https://arxiv.org/abs/2303.15056", "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "cites": 133}, {"url": "https://arxiv.org/abs/2204.02329", "title": "Can language models learn from explanations in context?", "cites": 130}, {"url": "https://arxiv.org/abs/2210.03350", "title": "Measuring and Narrowing the Compositionality Gap in Language Models", "cites": 130}, {"url": "https://arxiv.org/abs/2204.03162", "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic\n  Compositionality", "cites": 129}, {"url": "https://arxiv.org/abs/2208.03188", "title": "BlenderBot 3: a deployed conversational agent that continually learns to\n  responsibly engage", "cites": 129}, {"url": "https://arxiv.org/abs/2209.05451", "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", "cites": 128}, {"url": "https://arxiv.org/abs/2209.11302", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models", "cites": 128}, {"url": "https://arxiv.org/abs/2211.07636", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at\n  Scale", "cites": 128}, {"url": "https://arxiv.org/abs/2301.13867", "title": "Mathematical Capabilities of ChatGPT", "cites": 128}]