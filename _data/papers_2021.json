[{"url": "http://arxiv.org/abs/2104.08821v4", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 697}, {"url": "http://arxiv.org/abs/2101.00190v1", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 696}, {"url": "http://arxiv.org/abs/2102.05918v2", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 643}, {"url": "http://arxiv.org/abs/2104.08691v2", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 565}, {"url": "http://arxiv.org/abs/2012.15723v2", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": 545}, {"url": "http://arxiv.org/abs/2106.07447v1", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 420}, {"url": "http://arxiv.org/abs/2103.17249v1", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "cites": 348}, {"url": "http://arxiv.org/abs/2107.13586v1", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "cites": 330}, {"url": "http://arxiv.org/abs/2109.01652v5", "title": "Finetuned Language Models Are Zero-Shot Learners", "cites": 285}, {"url": "http://arxiv.org/abs/2110.08207v3", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "cites": 283}, {"url": "http://arxiv.org/abs/2105.01051v4", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "cites": 234}, {"url": "http://arxiv.org/abs/2112.11446v2", "title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "cites": 227}, {"url": "http://arxiv.org/abs/2105.03075v5", "title": "A Survey of Data Augmentation Approaches for NLP", "cites": 225}, {"url": "http://arxiv.org/abs/2103.10385v1", "title": "GPT Understands, Too", "cites": 224}, {"url": "http://arxiv.org/abs/2102.09690v2", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "cites": 223}, {"url": "http://arxiv.org/abs/2102.06183v1", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "cites": 215}, {"url": "http://arxiv.org/abs/2104.12763v2", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "cites": 212}, {"url": "http://arxiv.org/abs/2102.04664v2", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "cites": 202}, {"url": "http://arxiv.org/abs/2105.08050v2", "title": "Pay Attention to MLPs", "cites": 193}, {"url": "http://arxiv.org/abs/2110.13900v5", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech\n  Processing", "cites": 193}, {"url": "http://arxiv.org/abs/2108.10904v3", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "cites": 192}, {"url": "http://arxiv.org/abs/2102.08981v2", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize\n  Long-Tail Visual Concepts", "cites": 189}, {"url": "http://arxiv.org/abs/2106.09685v2", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "cites": 176}, {"url": "http://arxiv.org/abs/2106.10199v5", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "cites": 173}, {"url": "http://arxiv.org/abs/2104.06599v1", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "cites": 164}, {"url": "http://arxiv.org/abs/2102.02779v2", "title": "Unifying Vision-and-Language Tasks via Text Generation", "cites": 157}, {"url": "http://arxiv.org/abs/2105.11741v1", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence\n  Representation Transfer", "cites": 156}, {"url": "http://arxiv.org/abs/2102.03902v3", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating\n  Self-Attention", "cites": 156}, {"url": "http://arxiv.org/abs/2107.14795v3", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs", "cites": 152}, {"url": "http://arxiv.org/abs/2103.02143v2", "title": "Random Feature Attention", "cites": 150}, {"url": "http://arxiv.org/abs/2103.06333v2", "title": "Unified Pre-training for Program Understanding and Generation", "cites": 149}, {"url": "http://arxiv.org/abs/2104.08663v4", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "cites": 147}, {"url": "http://arxiv.org/abs/2106.07139v3", "title": "Pre-Trained Models: Past, Present and Future", "cites": 146}, {"url": "http://arxiv.org/abs/2104.06378v4", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question\n  Answering", "cites": 142}, {"url": "http://arxiv.org/abs/2109.00859v1", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for\n  Code Understanding and Generation", "cites": 141}, {"url": "http://arxiv.org/abs/2105.03824v4", "title": "FNet: Mixing Tokens with Fourier Transforms", "cites": 138}, {"url": "http://arxiv.org/abs/2109.02846v1", "title": "Datasets: A Community Library for Natural Language Processing", "cites": 137}, {"url": "http://arxiv.org/abs/2106.04554v2", "title": "A Survey of Transformers", "cites": 135}, {"url": "http://arxiv.org/abs/2105.11259v3", "title": "PTR: Prompt Tuning with Rules for Text Classification", "cites": 135}, {"url": "http://arxiv.org/abs/2102.01672v3", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and\n  Metrics", "cites": 134}, {"url": "http://arxiv.org/abs/2106.13884v2", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "cites": 134}, {"url": "http://arxiv.org/abs/2111.02114v1", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs", "cites": 129}, {"url": "http://arxiv.org/abs/2104.05240v2", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "cites": 128}, {"url": "http://arxiv.org/abs/2104.14337v1", "title": "Dynabench: Rethinking Benchmarking in NLP", "cites": 128}, {"url": "http://arxiv.org/abs/2105.11447v1", "title": "True Few-Shot Learning with Language Models", "cites": 127}, {"url": "http://arxiv.org/abs/2104.06967v2", "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "cites": 126}, {"url": "http://arxiv.org/abs/2108.00946v2", "title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "cites": 126}, {"url": "http://arxiv.org/abs/2110.04366v3", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "cites": 125}, {"url": "http://arxiv.org/abs/2106.11520v2", "title": "BARTScore: Evaluating Generated Text as Text Generation", "cites": 124}, {"url": "http://arxiv.org/abs/2104.08786v2", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "cites": 122}, {"url": "http://arxiv.org/abs/2107.06383v1", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "cites": 122}, {"url": "http://arxiv.org/abs/2101.06804v1", "title": "What Makes Good In-Context Examples for GPT-$3$?", "cites": 119}, {"url": "http://arxiv.org/abs/2110.07602v3", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks", "cites": 119}, {"url": "http://arxiv.org/abs/2110.14168v2", "title": "Training Verifiers to Solve Math Word Problems", "cites": 115}, {"url": "http://arxiv.org/abs/2102.07350v1", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "cites": 114}, {"url": "http://arxiv.org/abs/2104.06644v2", "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word\n  Matters Pre-training for Little", "cites": 113}, {"url": "http://arxiv.org/abs/2105.11084v3", "title": "Unsupervised Speech Recognition", "cites": 112}, {"url": "http://arxiv.org/abs/2101.00390v2", "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation", "cites": 111}, {"url": "http://arxiv.org/abs/2101.11038v1", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "cites": 110}, {"url": "http://arxiv.org/abs/2104.09864v4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "cites": 109}, {"url": "http://arxiv.org/abs/2112.04426v3", "title": "Improving language models by retrieving from trillions of tokens", "cites": 109}, {"url": "http://arxiv.org/abs/2105.13626v3", "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "cites": 105}, {"url": "http://arxiv.org/abs/2104.14478v1", "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation\n  for Machine Translation", "cites": 105}, {"url": "http://arxiv.org/abs/2102.10772v3", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer", "cites": 103}, {"url": "http://arxiv.org/abs/2112.06905v2", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "cites": 101}, {"url": "http://arxiv.org/abs/2109.14084v2", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text\n  Understanding", "cites": 101}, {"url": "http://arxiv.org/abs/2106.03193v1", "title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual\n  Machine Translation", "cites": 98}, {"url": "http://arxiv.org/abs/2101.00121v2", "title": "WARP: Word-level Adversarial ReProgramming", "cites": 98}, {"url": "http://arxiv.org/abs/2106.15561v3", "title": "A Survey on Neural Speech Synthesis", "cites": 97}, {"url": "http://arxiv.org/abs/2111.09296v3", "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at\n  Scale", "cites": 96}, {"url": "http://arxiv.org/abs/2102.08473v2", "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model\n  Pretraining", "cites": 95}, {"url": "http://arxiv.org/abs/2101.08231v4", "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "cites": 95}, {"url": "http://arxiv.org/abs/2111.09543v2", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing", "cites": 95}, {"url": "http://arxiv.org/abs/2104.07567v1", "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "cites": 94}, {"url": "http://arxiv.org/abs/2107.02137v1", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language\n  Understanding and Generation", "cites": 94}, {"url": "http://arxiv.org/abs/2106.02636v3", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "cites": 93}, {"url": "http://arxiv.org/abs/2104.08773v4", "title": "Cross-Task Generalization via Natural Language Crowdsourcing\n  Instructions", "cites": 92}, {"url": "http://arxiv.org/abs/2102.07033v1", "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them", "cites": 92}, {"url": "http://arxiv.org/abs/2101.09624v4", "title": "A Review of Speaker Diarization: Recent Advances with Deep Learning", "cites": 92}, {"url": "http://arxiv.org/abs/2109.12098v1", "title": "CLIPort: What and Where Pathways for Robotic Manipulation", "cites": 92}, {"url": "http://arxiv.org/abs/2112.04359v1", "title": "Ethical and social risks of harm from Language Models", "cites": 91}, {"url": "http://arxiv.org/abs/2101.05783v2", "title": "Persistent Anti-Muslim Bias in Large Language Models", "cites": 90}, {"url": "http://arxiv.org/abs/2106.10199v4", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "cites": 89}, {"url": "http://arxiv.org/abs/2112.09332v3", "title": "WebGPT: Browser-assisted question-answering with human feedback", "cites": 89}, {"url": "http://arxiv.org/abs/2104.01027v2", "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised\n  Pre-Training", "cites": 87}, {"url": "http://arxiv.org/abs/2104.13346v2", "title": "Understanding Factuality in Abstractive Summarization with FRANK: A\n  Benchmark for Factuality Metrics", "cites": 86}, {"url": "http://arxiv.org/abs/2103.15316v1", "title": "Whitening Sentence Representations for Better Semantics and Faster\n  Retrieval", "cites": 86}, {"url": "http://arxiv.org/abs/2101.05779v3", "title": "Structured Prediction as Translation between Augmented Natural Languages", "cites": 86}, {"url": "http://arxiv.org/abs/2104.07186v1", "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List", "cites": 85}, {"url": "http://arxiv.org/abs/2107.03006v2", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces", "cites": 85}, {"url": "http://arxiv.org/abs/2103.12028v4", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets", "cites": 84}, {"url": "http://arxiv.org/abs/2109.04332v3", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning", "cites": 84}, {"url": "http://arxiv.org/abs/2103.01209v4", "title": "Generative Adversarial Transformers", "cites": 83}, {"url": "http://arxiv.org/abs/2112.04482v3", "title": "FLAVA: A Foundational Language And Vision Alignment Model", "cites": 83}, {"url": "http://arxiv.org/abs/2110.04544v1", "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "cites": 83}, {"url": "http://arxiv.org/abs/2108.05540v1", "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage\n  Retrieval", "cites": 83}, {"url": "http://arxiv.org/abs/2101.00529v2", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "cites": 82}, {"url": "http://arxiv.org/abs/2111.07991v3", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning", "cites": 81}, {"url": "http://arxiv.org/abs/2105.09501v3", "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine\n  Translation", "cites": 80}, {"url": "http://arxiv.org/abs/2104.08718v3", "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "cites": 80}]