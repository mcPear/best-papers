[{"url": "https://arxiv.org/abs/2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 1598}, {"url": "https://arxiv.org/abs/2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 1536}, {"url": "https://arxiv.org/abs/2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 1515}, {"url": "https://arxiv.org/abs/2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 1448}, {"url": "https://arxiv.org/abs/2107.13586", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "cites": 1446}, {"url": "https://arxiv.org/abs/2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "cites": 1191}, {"url": "https://arxiv.org/abs/2109.01652", "title": "Finetuned Language Models Are Zero-Shot Learners", "cites": 1105}, {"url": "https://arxiv.org/abs/2106.07447", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 1033}, {"url": "https://arxiv.org/abs/2110.08207", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "cites": 865}, {"url": "https://arxiv.org/abs/2103.17249", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "cites": 708}, {"url": "https://arxiv.org/abs/2112.11446", "title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "cites": 659}, {"url": "https://arxiv.org/abs/2102.09690", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "cites": 568}, {"url": "https://arxiv.org/abs/2109.00859", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for\n  Code Understanding and Generation", "cites": 551}, {"url": "https://arxiv.org/abs/2103.10385", "title": "GPT Understands, Too", "cites": 537}, {"url": "https://arxiv.org/abs/2110.13900", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech\n  Processing", "cites": 527}, {"url": "https://arxiv.org/abs/2110.14168", "title": "Training Verifiers to Solve Math Word Problems", "cites": 527}, {"url": "https://arxiv.org/abs/2111.02114", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs", "cites": 503}, {"url": "https://arxiv.org/abs/2102.04664", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "cites": 496}, {"url": "https://arxiv.org/abs/2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "cites": 463}, {"url": "https://arxiv.org/abs/2108.10904", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "cites": 460}, {"url": "https://arxiv.org/abs/2102.08981", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize\n  Long-Tail Visual Concepts", "cites": 449}, {"url": "https://arxiv.org/abs/2104.12763", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "cites": 440}, {"url": "https://arxiv.org/abs/2105.01051", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "cites": 437}, {"url": "https://arxiv.org/abs/2105.03075", "title": "A Survey of Data Augmentation Approaches for NLP", "cites": 411}, {"url": "https://arxiv.org/abs/2106.10199", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "cites": 404}, {"url": "https://arxiv.org/abs/2102.06183", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "cites": 373}, {"url": "https://arxiv.org/abs/2104.08786", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "cites": 371}, {"url": "https://arxiv.org/abs/2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "cites": 367}, {"url": "https://arxiv.org/abs/2106.04554", "title": "A Survey of Transformers", "cites": 366}, {"url": "https://arxiv.org/abs/2103.06333", "title": "Unified Pre-training for Program Understanding and Generation", "cites": 365}, {"url": "https://arxiv.org/abs/2112.04426", "title": "Improving language models by retrieving from trillions of tokens", "cites": 361}, {"url": "https://arxiv.org/abs/2106.11342", "title": "Dive into Deep Learning", "cites": 352}, {"url": "https://arxiv.org/abs/2105.08050", "title": "Pay Attention to MLPs", "cites": 346}, {"url": "https://arxiv.org/abs/2112.04359", "title": "Ethical and social risks of harm from Language Models", "cites": 342}, {"url": "https://arxiv.org/abs/2106.13884", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "cites": 339}, {"url": "https://arxiv.org/abs/2110.04366", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "cites": 337}, {"url": "https://arxiv.org/abs/2106.11520", "title": "BARTScore: Evaluating Generated Text as Text Generation", "cites": 335}, {"url": "https://arxiv.org/abs/2104.08663", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "cites": 333}, {"url": "https://arxiv.org/abs/2108.05927", "title": "Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive\n  Content Identification in Indo-European Languages", "cites": 328}, {"url": "https://arxiv.org/abs/2112.03857", "title": "Grounded Language-Image Pre-training", "cites": 328}, {"url": "https://arxiv.org/abs/2102.02779", "title": "Unifying Vision-and-Language Tasks via Text Generation", "cites": 324}, {"url": "https://arxiv.org/abs/2111.09543", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing", "cites": 319}, {"url": "https://arxiv.org/abs/2106.07139", "title": "Pre-Trained Models: Past, Present and Future", "cites": 316}, {"url": "https://arxiv.org/abs/2107.14795", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs", "cites": 316}, {"url": "https://arxiv.org/abs/2109.02846", "title": "Datasets: A Community Library for Natural Language Processing", "cites": 315}, {"url": "https://arxiv.org/abs/2104.09864", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "cites": 313}, {"url": "https://arxiv.org/abs/2110.07602", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks", "cites": 310}, {"url": "https://arxiv.org/abs/2105.11741", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence\n  Representation Transfer", "cites": 308}, {"url": "https://arxiv.org/abs/2104.06599", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "cites": 306}, {"url": "https://arxiv.org/abs/2102.07350", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "cites": 291}, {"url": "https://arxiv.org/abs/2107.03006", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces", "cites": 281}, {"url": "https://arxiv.org/abs/2102.07662", "title": "Overview of the TREC 2020 deep learning track", "cites": 280}, {"url": "https://arxiv.org/abs/2104.06378", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question\n  Answering", "cites": 279}, {"url": "https://arxiv.org/abs/2104.08718", "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "cites": 278}, {"url": "https://arxiv.org/abs/2109.07958", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "cites": 276}, {"url": "https://arxiv.org/abs/2112.04482", "title": "FLAVA: A Foundational Language And Vision Alignment Model", "cites": 273}, {"url": "https://arxiv.org/abs/2111.09296", "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at\n  Scale", "cites": 272}, {"url": "https://arxiv.org/abs/2104.08773", "title": "Cross-Task Generalization via Natural Language Crowdsourcing\n  Instructions", "cites": 269}, {"url": "https://arxiv.org/abs/2112.06905", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "cites": 269}, {"url": "https://arxiv.org/abs/2109.12098", "title": "CLIPort: What and Where Pathways for Robotic Manipulation", "cites": 265}, {"url": "https://arxiv.org/abs/2105.11259", "title": "PTR: Prompt Tuning with Rules for Text Classification", "cites": 264}, {"url": "https://arxiv.org/abs/2110.04544", "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "cites": 263}, {"url": "https://arxiv.org/abs/2105.03824", "title": "FNet: Mixing Tokens with Fourier Transforms", "cites": 259}, {"url": "https://arxiv.org/abs/2102.03902", "title": "Nystr\\\"omformer: A Nystr\\\"om-Based Algorithm for Approximating\n  Self-Attention", "cites": 255}, {"url": "https://arxiv.org/abs/2108.00946", "title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "cites": 251}, {"url": "https://arxiv.org/abs/2111.07991", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning", "cites": 246}, {"url": "https://arxiv.org/abs/2107.06383", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "cites": 238}, {"url": "https://arxiv.org/abs/2109.14084", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text\n  Understanding", "cites": 237}, {"url": "https://arxiv.org/abs/2101.00390", "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation", "cites": 235}, {"url": "https://arxiv.org/abs/2104.05240", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "cites": 233}, {"url": "https://arxiv.org/abs/2105.11447", "title": "True Few-Shot Learning with Language Models", "cites": 233}, {"url": "https://arxiv.org/abs/2103.07191", "title": "Are NLP Models really able to Solve Simple Math Word Problems?", "cites": 227}, {"url": "https://arxiv.org/abs/2106.03193", "title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual\n  Machine Translation", "cites": 225}, {"url": "https://arxiv.org/abs/2104.06967", "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "cites": 224}, {"url": "https://arxiv.org/abs/2109.04332", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning", "cites": 221}, {"url": "https://arxiv.org/abs/2101.02235", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit\n  Reasoning Strategies", "cites": 220}, {"url": "https://arxiv.org/abs/2103.03874", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "cites": 218}, {"url": "https://arxiv.org/abs/2105.06337", "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech", "cites": 216}, {"url": "https://arxiv.org/abs/2103.02143", "title": "Random Feature Attention", "cites": 214}, {"url": "https://arxiv.org/abs/2103.10360", "title": "GLM: General Language Model Pretraining with Autoregressive Blank\n  Infilling", "cites": 213}, {"url": "https://arxiv.org/abs/2112.08633", "title": "Learning To Retrieve Prompts for In-Context Learning", "cites": 212}, {"url": "https://arxiv.org/abs/2101.05783", "title": "Persistent Anti-Muslim Bias in Large Language Models", "cites": 211}, {"url": "https://arxiv.org/abs/2104.14337", "title": "Dynabench: Rethinking Benchmarking in NLP", "cites": 209}, {"url": "https://arxiv.org/abs/2105.13626", "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "cites": 208}, {"url": "https://arxiv.org/abs/2101.00121", "title": "WARP: Word-level Adversarial ReProgramming", "cites": 206}, {"url": "https://arxiv.org/abs/2102.01672", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and\n  Metrics", "cites": 205}, {"url": "https://arxiv.org/abs/2111.02358", "title": "VLMo: Unified Vision-Language Pre-Training with\n  Mixture-of-Modality-Experts", "cites": 205}, {"url": "https://arxiv.org/abs/2104.07567", "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "cites": 203}, {"url": "https://arxiv.org/abs/2106.02636", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "cites": 203}, {"url": "https://arxiv.org/abs/2106.04647", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers", "cites": 198}, {"url": "https://arxiv.org/abs/2106.15561", "title": "A Survey on Neural Speech Synthesis", "cites": 198}, {"url": "https://arxiv.org/abs/2111.02080", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "cites": 197}, {"url": "https://arxiv.org/abs/2111.02387", "title": "An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers", "cites": 194}, {"url": "https://arxiv.org/abs/2101.11038", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "cites": 188}, {"url": "https://arxiv.org/abs/2107.06499", "title": "Deduplicating Training Data Makes Language Models Better", "cites": 188}, {"url": "https://arxiv.org/abs/2104.07650", "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization\n  for Relation Extraction", "cites": 186}, {"url": "https://arxiv.org/abs/2106.01760", "title": "Template-Based Named Entity Recognition Using BART", "cites": 184}, {"url": "https://arxiv.org/abs/2108.05540", "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage\n  Retrieval", "cites": 184}, {"url": "https://arxiv.org/abs/2102.10772", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer", "cites": 182}, {"url": "https://arxiv.org/abs/2110.15943", "title": "MetaICL: Learning to Learn In Context", "cites": 182}]