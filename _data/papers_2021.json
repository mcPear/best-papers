[{"url": "http://arxiv.org/abs/2101.00190v1", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "cites": 424}, {"url": "http://arxiv.org/abs/2104.08821v4", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "cites": 420}, {"url": "http://arxiv.org/abs/2102.05918v2", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "cites": 383}, {"url": "http://arxiv.org/abs/2012.15723v2", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": 359}, {"url": "http://arxiv.org/abs/2104.08691v2", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "cites": 343}, {"url": "http://arxiv.org/abs/2106.07447v1", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "cites": 230}, {"url": "http://arxiv.org/abs/2103.17249v1", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "cites": 206}, {"url": "http://arxiv.org/abs/2107.13586v1", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "cites": 178}, {"url": "http://arxiv.org/abs/2109.01652v5", "title": "Finetuned Language Models Are Zero-Shot Learners", "cites": 164}, {"url": "http://arxiv.org/abs/2105.03075v5", "title": "A Survey of Data Augmentation Approaches for NLP", "cites": 148}, {"url": "http://arxiv.org/abs/2102.04664v2", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "cites": 146}, {"url": "http://arxiv.org/abs/2103.10385v1", "title": "GPT Understands, Too", "cites": 145}, {"url": "http://arxiv.org/abs/2102.06183v1", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "cites": 144}, {"url": "http://arxiv.org/abs/2110.08207v3", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "cites": 143}, {"url": "http://arxiv.org/abs/2102.09690v2", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "cites": 141}, {"url": "http://arxiv.org/abs/2104.12763v2", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "cites": 134}, {"url": "http://arxiv.org/abs/2112.11446v2", "title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "cites": 133}, {"url": "http://arxiv.org/abs/2105.08050v2", "title": "Pay Attention to MLPs", "cites": 130}, {"url": "http://arxiv.org/abs/2102.03902v3", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating\n  Self-Attention", "cites": 124}, {"url": "http://arxiv.org/abs/2105.01051v4", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "cites": 119}, {"url": "http://arxiv.org/abs/2104.06599v1", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "cites": 117}, {"url": "http://arxiv.org/abs/2103.06333v2", "title": "Unified Pre-training for Program Understanding and Generation", "cites": 110}, {"url": "http://arxiv.org/abs/2103.02143v2", "title": "Random Feature Attention", "cites": 108}, {"url": "http://arxiv.org/abs/2108.10904v3", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "cites": 106}, {"url": "http://arxiv.org/abs/2104.06967v2", "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "cites": 104}, {"url": "http://arxiv.org/abs/2102.08981v2", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize\n  Long-Tail Visual Concepts", "cites": 104}, {"url": "http://arxiv.org/abs/2104.08663v4", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "cites": 101}, {"url": "http://arxiv.org/abs/2102.01672v3", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and\n  Metrics", "cites": 100}, {"url": "http://arxiv.org/abs/2102.02779v2", "title": "Unifying Vision-and-Language Tasks via Text Generation", "cites": 99}, {"url": "http://arxiv.org/abs/2107.14795v3", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs", "cites": 95}, {"url": "http://arxiv.org/abs/2105.11741v1", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence\n  Representation Transfer", "cites": 94}, {"url": "http://arxiv.org/abs/2104.14337v1", "title": "Dynabench: Rethinking Benchmarking in NLP", "cites": 94}, {"url": "http://arxiv.org/abs/2106.09685v2", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "cites": 90}, {"url": "http://arxiv.org/abs/2106.10199v4", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "cites": 89}, {"url": "http://arxiv.org/abs/2105.03824v4", "title": "FNet: Mixing Tokens with Fourier Transforms", "cites": 88}, {"url": "http://arxiv.org/abs/2104.06378v4", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question\n  Answering", "cites": 88}, {"url": "http://arxiv.org/abs/2105.11447v1", "title": "True Few-Shot Learning with Language Models", "cites": 87}, {"url": "http://arxiv.org/abs/2106.13884v2", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "cites": 86}, {"url": "http://arxiv.org/abs/2104.05240v2", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "cites": 86}, {"url": "http://arxiv.org/abs/2109.00859v1", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for\n  Code Understanding and Generation", "cites": 85}, {"url": "http://arxiv.org/abs/2105.11259v3", "title": "PTR: Prompt Tuning with Rules for Text Classification", "cites": 85}, {"url": "http://arxiv.org/abs/2104.06644v2", "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word\n  Matters Pre-training for Little", "cites": 85}, {"url": "http://arxiv.org/abs/2107.06383v1", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "cites": 84}, {"url": "http://arxiv.org/abs/2101.11038v1", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "cites": 81}, {"url": "http://arxiv.org/abs/2110.13900v5", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech\n  Processing", "cites": 80}, {"url": "http://arxiv.org/abs/2106.07139v3", "title": "Pre-Trained Models: Past, Present and Future", "cites": 78}, {"url": "http://arxiv.org/abs/2104.14478v1", "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation\n  for Machine Translation", "cites": 78}, {"url": "http://arxiv.org/abs/2109.02846v1", "title": "Datasets: A Community Library for Natural Language Processing", "cites": 77}, {"url": "http://arxiv.org/abs/2101.00121v2", "title": "WARP: Word-level Adversarial ReProgramming", "cites": 74}, {"url": "http://arxiv.org/abs/2105.11084v3", "title": "Unsupervised Speech Recognition", "cites": 73}, {"url": "http://arxiv.org/abs/2110.07602v3", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks", "cites": 71}, {"url": "http://arxiv.org/abs/2102.07350v1", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "cites": 71}, {"url": "http://arxiv.org/abs/2101.00390v2", "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation", "cites": 70}, {"url": "http://arxiv.org/abs/2107.02137v1", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language\n  Understanding and Generation", "cites": 69}, {"url": "http://arxiv.org/abs/2105.13626v3", "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "cites": 69}, {"url": "http://arxiv.org/abs/2101.06804v1", "title": "What Makes Good In-Context Examples for GPT-$3$?", "cites": 68}, {"url": "http://arxiv.org/abs/2102.10772v3", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer", "cites": 67}, {"url": "http://arxiv.org/abs/2104.09864v4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "cites": 66}, {"url": "http://arxiv.org/abs/2104.07567v1", "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "cites": 66}, {"url": "http://arxiv.org/abs/2101.08231v4", "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "cites": 65}, {"url": "http://arxiv.org/abs/2106.04554v2", "title": "A Survey of Transformers", "cites": 64}, {"url": "http://arxiv.org/abs/2106.03193v1", "title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual\n  Machine Translation", "cites": 64}, {"url": "http://arxiv.org/abs/2101.05783v2", "title": "Persistent Anti-Muslim Bias in Large Language Models", "cites": 64}, {"url": "http://arxiv.org/abs/2101.00529v2", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "cites": 64}, {"url": "http://arxiv.org/abs/2104.07186v1", "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List", "cites": 62}, {"url": "http://arxiv.org/abs/2103.01209v4", "title": "Generative Adversarial Transformers", "cites": 62}, {"url": "http://arxiv.org/abs/2111.05988v2", "title": "Cross-language Information Retrieval", "cites": 61}, {"url": "http://arxiv.org/abs/2106.11520v2", "title": "BARTScore: Evaluating Generated Text as Text Generation", "cites": 61}, {"url": "http://arxiv.org/abs/2104.08786v2", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "cites": 61}, {"url": "http://arxiv.org/abs/2108.05540v1", "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage\n  Retrieval", "cites": 60}, {"url": "http://arxiv.org/abs/2108.00946v2", "title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "cites": 60}, {"url": "http://arxiv.org/abs/2103.11955v3", "title": "Improving and Simplifying Pattern Exploiting Training", "cites": 60}, {"url": "http://arxiv.org/abs/2102.07033v1", "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them", "cites": 60}, {"url": "http://arxiv.org/abs/2103.16716v1", "title": "BASE Layers: Simplifying Training of Large, Sparse Models", "cites": 59}, {"url": "http://arxiv.org/abs/2112.09301v1", "title": "Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive\n  Content Identification in English and Indo-Aryan Languages", "cites": 58}, {"url": "http://arxiv.org/abs/2109.14084v2", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text\n  Understanding", "cites": 58}, {"url": "http://arxiv.org/abs/2106.15561v3", "title": "A Survey on Neural Speech Synthesis", "cites": 58}, {"url": "http://arxiv.org/abs/2101.04840v1", "title": "Robustness Gym: Unifying the NLP Evaluation Landscape", "cites": 58}, {"url": "http://arxiv.org/abs/2112.06905v2", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "cites": 57}, {"url": "http://arxiv.org/abs/2110.04366v3", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "cites": 57}, {"url": "http://arxiv.org/abs/2106.02636v3", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "cites": 57}, {"url": "http://arxiv.org/abs/2103.12028v4", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets", "cites": 57}, {"url": "http://arxiv.org/abs/2101.09624v4", "title": "A Review of Speaker Diarization: Recent Advances with Deep Learning", "cites": 57}, {"url": "http://arxiv.org/abs/2101.09459v7", "title": "Advances and Challenges in Conversational Recommender Systems: A Survey", "cites": 57}, {"url": "http://arxiv.org/abs/2101.05779v3", "title": "Structured Prediction as Translation between Augmented Natural Languages", "cites": 57}, {"url": "http://arxiv.org/abs/2112.04426v3", "title": "Improving language models by retrieving from trillions of tokens", "cites": 55}, {"url": "http://arxiv.org/abs/2110.14168v2", "title": "Training Verifiers to Solve Math Word Problems", "cites": 55}, {"url": "http://arxiv.org/abs/2112.09332v3", "title": "WebGPT: Browser-assisted question-answering with human feedback", "cites": 54}, {"url": "http://arxiv.org/abs/2105.09501v3", "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine\n  Translation", "cites": 54}, {"url": "http://arxiv.org/abs/2104.07412v2", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation", "cites": 54}, {"url": "http://arxiv.org/abs/2104.01027v2", "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised\n  Pre-Training", "cites": 54}, {"url": "http://arxiv.org/abs/2103.00453v2", "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based\n  Bias in NLP", "cites": 54}, {"url": "http://arxiv.org/abs/2102.08473v2", "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model\n  Pretraining", "cites": 54}, {"url": "http://arxiv.org/abs/2107.06499v2", "title": "Deduplicating Training Data Makes Language Models Better", "cites": 53}, {"url": "http://arxiv.org/abs/2104.13346v2", "title": "Understanding Factuality in Abstractive Summarization with FRANK: A\n  Benchmark for Factuality Metrics", "cites": 53}, {"url": "http://arxiv.org/abs/2102.00086v1", "title": "Challenges in Automated Debiasing for Toxic Language Detection", "cites": 53}, {"url": "http://arxiv.org/abs/2101.09995v2", "title": "Re-imagining Algorithmic Fairness in India and Beyond", "cites": 53}, {"url": "http://arxiv.org/abs/2112.04359v1", "title": "Ethical and social risks of harm from Language Models", "cites": 52}, {"url": "http://arxiv.org/abs/2107.07566v1", "title": "Internet-Augmented Dialogue Generation", "cites": 52}, {"url": "http://arxiv.org/abs/2104.12369v1", "title": "PanGu-$\u03b1$: Large-scale Autoregressive Pretrained Chinese Language\n  Models with Auto-parallel Computation", "cites": 52}]