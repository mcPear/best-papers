[{"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": "3 022", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.", "no": 1}, {"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": "2 852", "abstract": "This book is the result of a seminar in which we reviewed multimodal\napproaches and attempted to create a solid overview of the field, starting with\nthe current state-of-the-art approaches in the two subfields of Deep Learning\nindividually. Further, modeling frameworks are discussed where one modality is\ntransformed into the other, as well as models in which one modality is utilized\nto enhance representation learning for the other. To conclude the second part,\narchitectures with a focus on handling both modalities simultaneously are\nintroduced. Finally, we also cover other modalities as well as general-purpose\nmulti-modal models, which are able to handle different tasks on different\nmodalities within one unified architecture. One interesting application\n(Generative Art) eventually caps off this booklet.", "no": 2}, {"url": "https://arxiv.org/abs/2303.08774", "title": "GPT-4 Technical Report", "cites": "2 058", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.", "no": 3}, {"url": "https://arxiv.org/abs/2310.16713", "title": "SkyMath: Technical Report", "cites": "2 058", "abstract": "Large language models (LLMs) have shown great potential to solve varieties of\nnatural language processing (NLP) tasks, including mathematical reasoning. In\nthis work, we present SkyMath, a large language model for mathematics with 13\nbillion parameters. By applying self-compare fine-tuning, we have enhanced\nmathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,\nSkyMath outperforms all known open-source models of similar size and has\nestablished a new SOTA performance.", "no": 4}, {"url": "https://arxiv.org/abs/2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "cites": "1 361", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "no": 5}, {"url": "https://arxiv.org/abs/2303.12712", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "cites": "1 057", "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.", "no": 6}, {"url": "https://arxiv.org/abs/2303.18223", "title": "A Survey of Large Language Models", "cites": "631", "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.", "no": 7}, {"url": "https://arxiv.org/abs/2304.08485", "title": "Visual Instruction Tuning", "cites": "529", "abstract": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.", "no": 8}, {"url": "https://arxiv.org/abs/2302.04023", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on\n  Reasoning, Hallucination, and Interactivity", "cites": "505", "abstract": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.", "no": 9}, {"url": "https://arxiv.org/abs/2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "cites": "483", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.", "no": 10}, {"url": "https://arxiv.org/abs/2308.02976", "title": "Spanish Pre-trained BERT Model and Evaluation Data", "cites": "427", "abstract": "The Spanish language is one of the top 5 spoken languages in the world.\nNevertheless, finding resources to train or evaluate Spanish language models is\nnot an easy task. In this paper we help bridge this gap by presenting a\nBERT-based language model pre-trained exclusively on Spanish data. As a second\ncontribution, we also compiled several tasks specifically for the Spanish\nlanguage in a single repository much in the spirit of the GLUE benchmark. By\nfine-tuning our pre-trained Spanish model, we obtain better results compared to\nother BERT-based models pre-trained on multilingual corpora for most of the\ntasks, even achieving a new state-of-the-art on some of them. We have publicly\nreleased our model, the pre-training data, and the compilation of the Spanish\nbenchmarks.", "no": 11}, {"url": "https://arxiv.org/abs/2306.05685", "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "cites": "413", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "no": 12}, {"url": "https://arxiv.org/abs/2305.10403", "title": "PaLM 2 Technical Report", "cites": "404", "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.", "no": 13}, {"url": "https://arxiv.org/abs/2303.17580", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "cites": "327", "abstract": "Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.", "no": 14}, {"url": "https://arxiv.org/abs/2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "cites": "327", "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.", "no": 15}, {"url": "https://arxiv.org/abs/2302.06476", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "cites": "296", "abstract": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.", "no": 16}, {"url": "https://arxiv.org/abs/2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "cites": "285", "abstract": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.", "no": 17}, {"url": "https://arxiv.org/abs/2303.13375", "title": "Capabilities of GPT-4 on Medical Challenge Problems", "cites": "259", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.", "no": 18}, {"url": "https://arxiv.org/abs/2308.11995", "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "cites": "255", "abstract": "Building socialbots that can have deep, engaging open-domain conversations\nwith humans is one of the grand challenges of artificial intelligence (AI). To\nthis end, bots need to be able to leverage world knowledge spanning several\ndomains effectively when conversing with humans who have their own world\nknowledge. Existing knowledge-grounded conversation datasets are primarily\nstylized with explicit roles for conversation partners. These datasets also do\nnot explore depth or breadth of topical coverage with transitions in\nconversations. We introduce Topical-Chat, a knowledge-grounded human-human\nconversation dataset where the underlying knowledge spans 8 broad topics and\nconversation partners don't have explicitly defined roles, to help further\nresearch in open-domain conversational AI. We also train several\nstate-of-the-art encoder-decoder conversational models on Topical-Chat and\nperform automated and human evaluation for benchmarking.", "no": 19}, {"url": "https://arxiv.org/abs/2304.01373", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "cites": "252", "abstract": "How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.", "no": 20}, {"url": "https://arxiv.org/abs/2304.03277", "title": "Instruction Tuning with GPT-4", "cites": "243", "abstract": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.", "no": 21}, {"url": "https://arxiv.org/abs/2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "cites": "235", "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "no": 22}, {"url": "https://arxiv.org/abs/2301.07597", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,\n  and Detection", "cites": "233", "abstract": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.", "no": 23}, {"url": "https://arxiv.org/abs/2306.01116", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "cites": "221", "abstract": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "no": 24}, {"url": "https://arxiv.org/abs/2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "cites": "217", "abstract": "A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.", "no": 25}, {"url": "https://arxiv.org/abs/2304.12244", "title": "WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions", "cites": "216", "abstract": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM", "no": 26}, {"url": "https://arxiv.org/abs/2303.16199", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "cites": "214", "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.", "no": 27}, {"url": "https://arxiv.org/abs/2303.16634", "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "cites": "210", "abstract": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval", "no": 28}, {"url": "https://arxiv.org/abs/2303.15056", "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "cites": "208", "abstract": "Many NLP applications require manual data annotations for a variety of tasks,\nnotably to train classifiers or evaluate the performance of unsupervised\nmodels. Depending on the size and degree of complexity, the tasks may be\nconducted by crowd-workers on platforms such as MTurk as well as trained\nannotators, such as research assistants. Using a sample of 2,382 tweets, we\ndemonstrate that ChatGPT outperforms crowd-workers for several annotation\ntasks, including relevance, stance, topics, and frames detection. Specifically,\nthe zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\nfive tasks, while ChatGPT's intercoder agreement exceeds that of both\ncrowd-workers and trained annotators for all tasks. Moreover, the\nper-annotation cost of ChatGPT is less than $0.003 -- about twenty times\ncheaper than MTurk. These results show the potential of large language models\nto drastically increase the efficiency of text classification.", "no": 29}, {"url": "https://arxiv.org/abs/2301.13688", "title": "The Flan Collection: Designing Data and Methods for Effective\n  Instruction Tuning", "cites": "205", "abstract": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.", "no": 30}, {"url": "https://arxiv.org/abs/2302.03494", "title": "A Categorical Archive of ChatGPT Failures", "cites": "194", "abstract": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.", "no": 31}, {"url": "https://arxiv.org/abs/2303.17564", "title": "BloombergGPT: A Large Language Model for Finance", "cites": "191", "abstract": "The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.", "no": 32}, {"url": "https://arxiv.org/abs/2302.09419", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT", "cites": "188", "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.", "no": 33}, {"url": "https://arxiv.org/abs/2301.11305", "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability\n  Curvature", "cites": "186", "abstract": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.", "no": 34}, {"url": "https://arxiv.org/abs/2302.07842", "title": "Augmented Language Models: a Survey", "cites": "177", "abstract": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.", "no": 35}, {"url": "https://arxiv.org/abs/2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "cites": "172", "abstract": "Large language models (LLMs) have demonstrated significant universal\ncapabilities as few/zero-shot learners in various tasks due to their\npre-training on vast amounts of text data, as exemplified by GPT-3, which\nboosted to InstrctGPT and ChatGPT, effectively following natural language\ninstructions to accomplish real-world tasks. In this paper, we propose to\nintroduce instruction tuning into multi-modal models, motivated by the Flamingo\nmodel's upstream interleaved format pretraining dataset. We adopt a similar\napproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)\ndataset. We then introduce Otter, a multi-modal model based on OpenFlamingo\n(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and\nshowcasing improved instruction-following ability and in-context learning. We\nalso optimize OpenFlamingo's implementation for researchers, democratizing the\nrequired training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs,\nand integrate both OpenFlamingo and Otter into Huggingface Transformers for\nmore researchers to incorporate the models into their customized training and\ninference pipelines.", "no": 36}, {"url": "https://arxiv.org/abs/2305.11206", "title": "LIMA: Less Is More for Alignment", "cites": "172", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.", "no": 37}, {"url": "https://arxiv.org/abs/2305.18290", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model", "cites": "170", "abstract": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.", "no": 38}, {"url": "https://arxiv.org/abs/2301.13867", "title": "Mathematical Capabilities of ChatGPT", "cites": "169", "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT\n(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on\npublicly available datasets, as well as hand-crafted ones, using a novel\nmethodology. In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, either cover\nonly elementary mathematics or are very small. We address this by publicly\nreleasing two new datasets: GHOSTS and miniGHOSTS. These are the first\nnatural-language datasets curated by working researchers in mathematics that\n(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of\nthe mathematical capabilities of language models, and (3) distinguish multiple\ndimensions of mathematical reasoning. These datasets also test whether ChatGPT\nand GPT-4 can be helpful assistants to professional mathematicians by emulating\nuse cases that arise in the daily professional activities of mathematicians. We\nbenchmark the models on a range of fine-grained performance metrics. For\nadvanced mathematics, this is the most detailed evaluation effort to date. We\nfind that ChatGPT can be used most successfully as a mathematical assistant for\nquerying facts, acting as a mathematical search engine and knowledge base\ninterface. GPT-4 can additionally be used for undergraduate-level mathematics\nbut fails on graduate-level difficulty. Contrary to many positive reports in\nthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of\nselection bias), their overall mathematical performance is well below the level\nof a graduate student. Hence, if your goal is to use ChatGPT to pass a\ngraduate-level math exam, you would be better off copying from your average\npeer!", "no": 39}, {"url": "https://arxiv.org/abs/2308.12950", "title": "Code Llama: Open Foundation Models for Code", "cites": "169", "abstract": "We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\non sequences of 16k tokens and show improvements on inputs with up to 100k\ntokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\ninfilling based on surrounding content. Code Llama reaches state-of-the-art\nperformance among open models on several code benchmarks, with scores of up to\n53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\nevery other publicly available model on MultiPL-E. We release Code Llama under\na permissive license that allows for both research and commercial use.", "no": 40}, {"url": "https://arxiv.org/abs/2305.06161", "title": "StarCoder: may the source be with you!", "cites": "166", "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.", "no": 41}, {"url": "https://arxiv.org/abs/2301.07093", "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "cites": "164", "abstract": "Large-scale text-to-image diffusion models have made amazing advances.\nHowever, the status quo is to use text input alone, which can impede\ncontrollability. In this work, we propose GLIGEN, Grounded-Language-to-Image\nGeneration, a novel approach that builds upon and extends the functionality of\nexisting pre-trained text-to-image diffusion models by enabling them to also be\nconditioned on grounding inputs. To preserve the vast concept knowledge of the\npre-trained model, we freeze all of its weights and inject the grounding\ninformation into new trainable layers via a gated mechanism. Our model achieves\nopen-world grounded text2img generation with caption and bounding box condition\ninputs, and the grounding ability generalizes well to novel spatial\nconfigurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS\noutperforms that of existing supervised layout-to-image baselines by a large\nmargin.", "no": 42}, {"url": "https://arxiv.org/abs/2304.15010", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", "cites": "163", "abstract": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.", "no": 43}, {"url": "https://arxiv.org/abs/2301.10226", "title": "A Watermark for Large Language Models", "cites": "156", "abstract": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.", "no": 44}, {"url": "https://arxiv.org/abs/2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "cites": "156", "abstract": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.", "no": 45}, {"url": "https://arxiv.org/abs/2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "cites": "154", "abstract": "While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nthe performance of language models on two tasks that require identifying\nrelevant information in their input contexts: multi-document question answering\nand key-value retrieval. We find that performance can degrade significantly\nwhen changing the position of relevant information, indicating that current\nlanguage models do not robustly make use of information in long input contexts.\nIn particular, we observe that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts, even for explicitly long-context models. Our analysis\nprovides a better understanding of how language models use their input context\nand provides new evaluation protocols for future long-context language models.", "no": 46}, {"url": "https://arxiv.org/abs/2301.02111", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "cites": "153", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS).\nSpecifically, we train a neural codec language model (called Vall-E) using\ndiscrete codes derived from an off-the-shelf neural audio codec model, and\nregard TTS as a conditional language modeling task rather than continuous\nsignal regression as in previous work. During the pre-training stage, we scale\nup the TTS training data to 60K hours of English speech which is hundreds of\ntimes larger than existing systems. Vall-E emerges in-context learning\ncapabilities and can be used to synthesize high-quality personalized speech\nwith only a 3-second enrolled recording of an unseen speaker as an acoustic\nprompt. Experiment results show that Vall-E significantly outperforms the\nstate-of-the-art zero-shot TTS system in terms of speech naturalness and\nspeaker similarity. In addition, we find Vall-E could preserve the speaker's\nemotion and acoustic environment of the acoustic prompt in synthesis. See\nhttps://aka.ms/valle for demos of our work.", "no": 47}, {"url": "https://arxiv.org/abs/2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "cites": "153", "abstract": "Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.", "no": 48}, {"url": "https://arxiv.org/abs/2307.15818", "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic\n  Control", "cites": "153", "abstract": "We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).", "no": 49}, {"url": "https://arxiv.org/abs/2301.12652", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "cites": "147", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.", "no": 50}, {"url": "https://arxiv.org/abs/2302.12813", "title": "Check Your Facts and Try Again: Improving Large Language Models with\n  External Knowledge and Automated Feedback", "cites": "145", "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.", "no": 51}, {"url": "https://arxiv.org/abs/2303.11381", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "cites": "144", "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/", "no": 52}, {"url": "https://arxiv.org/abs/2304.05128", "title": "Teaching Large Language Models to Self-Debug", "cites": "144", "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.", "no": 53}, {"url": "https://arxiv.org/abs/2306.17582", "title": "ChatGPT for Robotics: Design Principles and Model Abilities", "cites": "144", "abstract": "This paper presents an experimental study regarding the use of OpenAI's\nChatGPT for robotics applications. We outline a strategy that combines design\nprinciples for prompt engineering and the creation of a high-level function\nlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,\nand form factors. We focus our evaluations on the effectiveness of different\nprompt engineering techniques and dialog strategies towards the execution of\nvarious types of robotics tasks. We explore ChatGPT's ability to use free-form\ndialog, parse XML tags, and to synthesize code, in addition to the use of\ntask-specific prompting functions and closed-loop reasoning through dialogues.\nOur study encompasses a range of tasks within the robotics domain, from basic\nlogical, geometrical, and mathematical reasoning all the way to complex domains\nsuch as aerial navigation, manipulation, and embodied agents. We show that\nChatGPT can be effective at solving several of such tasks, while allowing users\nto interact with it primarily via natural language instructions. In addition to\nthese studies, we introduce an open-sourced research tool called PromptCraft,\nwhich contains a platform where researchers can collaboratively upload and vote\non examples of good prompting schemes for robotics applications, as well as a\nsample robotics simulator with ChatGPT integration, making it easier for users\nto get started with using ChatGPT for robotics.", "no": 54}, {"url": "https://arxiv.org/abs/2307.15043", "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models", "cites": "144", "abstract": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.", "no": 55}, {"url": "https://arxiv.org/abs/2303.04226", "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of\n  Generative AI from GAN to ChatGPT", "cites": "143", "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\nattention from society. As a result, many individuals have become interested in\nrelated resources and are seeking to uncover the background and secrets behind\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\ntechniques belong to the category of Artificial Intelligence Generated Content\n(AIGC), which involves the creation of digital content, such as images, music,\nand natural language, through AI models. The goal of AIGC is to make the\ncontent creation process more efficient and accessible, allowing for the\nproduction of high-quality content at a faster pace. AIGC is achieved by\nextracting and understanding intent information from instructions provided by\nhuman, and generating the content according to its knowledge and the intent\ninformation. In recent years, large-scale models have become increasingly\nimportant in AIGC as they provide better intent extraction and thus, improved\ngeneration results. With the growth of data and the size of the models, the\ndistribution that the model can learn becomes more comprehensive and closer to\nreality, leading to more realistic and high-quality content generation. This\nsurvey provides a comprehensive review on the history of generative models, and\nbasic components, recent advances in AIGC from unimodal interaction and\nmultimodal interaction. From the perspective of unimodality, we introduce the\ngeneration tasks and relative models of text and image. From the perspective of\nmultimodality, we introduce the cross-application between the modalities\nmentioned above. Finally, we discuss the existing open problems and future\nchallenges in AIGC.", "no": 56}, {"url": "https://arxiv.org/abs/2303.13367", "title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing", "cites": "139", "abstract": "This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.", "no": 57}, {"url": "https://arxiv.org/abs/2301.13826", "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image\n  Diffusion Models", "cites": "131", "abstract": "Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.", "no": 58}, {"url": "https://arxiv.org/abs/2302.09210", "title": "How Good Are GPT Models at Machine Translation? A Comprehensive\n  Evaluation", "cites": "130", "abstract": "Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.", "no": 59}, {"url": "https://arxiv.org/abs/2301.06627", "title": "Dissociating language and thought in large language models", "cites": "129", "abstract": "Large language models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence--knowledge of linguistic rules and patterns--and\nfunctional linguistic competence--understanding and using language in the\nworld. We ground this distinction in human neuroscience, showing that formal\nand functional competence rely on different neural mechanisms. Although LLMs\nare surprisingly good at formal competence, their performance on functional\ncompetence tasks remains spotty and often requires specialized fine-tuning\nand/or coupling with external modules. In short, LLMs are good models of\nlanguage but incomplete models of human thought.", "no": 60}, {"url": "https://arxiv.org/abs/2310.03744", "title": "Improved Baselines with Visual Instruction Tuning", "cites": "129", "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.", "no": 61}, {"url": "https://arxiv.org/abs/2305.09617", "title": "Towards Expert-Level Medical Question Answering with Large Language\n  Models", "cites": "128", "abstract": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.", "no": 62}, {"url": "https://arxiv.org/abs/2302.00923", "title": "Multimodal Chain-of-Thought Reasoning in Language Models", "cites": "127", "abstract": "Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have focused on the language modality. We propose\nMultimodal-CoT that incorporates language (text) and vision (images) modalities\ninto a two-stage framework that separates rationale generation and answer\ninference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. With Multimodal-CoT, our\nmodel under 1 billion parameters outperforms the previous state-of-the-art LLM\n(GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA\nbenchmark and even surpasses human performance. Code is publicly available\navailable at https://github.com/amazon-science/mm-cot.", "no": 63}, {"url": "https://arxiv.org/abs/2304.01196", "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data", "cites": "125", "abstract": "Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.", "no": 64}, {"url": "https://arxiv.org/abs/2302.10724", "title": "ChatGPT: Jack of all trades, master of none", "cites": "124", "abstract": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.", "no": 65}, {"url": "https://arxiv.org/abs/2306.14824", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "cites": "123", "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.", "no": 66}, {"url": "https://arxiv.org/abs/2303.03846", "title": "Larger language models do in-context learning differently", "cites": "122", "abstract": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.", "no": 67}, {"url": "https://arxiv.org/abs/2303.04048", "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study", "cites": "122", "abstract": "Recently, the emergence of ChatGPT has attracted wide attention from the\ncomputational linguistics community. Many prior studies have shown that ChatGPT\nachieves remarkable performance on various NLP tasks in terms of automatic\nevaluation metrics. However, the ability of ChatGPT to serve as an evaluation\nmetric is still underexplored. Considering assessing the quality of natural\nlanguage generation (NLG) models is an arduous task and NLG metrics notoriously\nshow their poor correlation with human judgments, we wonder whether ChatGPT is\na good NLG evaluation metric. In this report, we provide a preliminary\nmeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,\nwe regard ChatGPT as a human evaluator and give task-specific (e.g.,\nsummarization) and aspect-specific (e.g., relevance) instruction to prompt\nChatGPT to evaluate the generated results of NLG models. We conduct experiments\non five NLG meta-evaluation datasets (including summarization, story generation\nand data-to-text tasks). Experimental results show that compared with previous\nautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlation\nwith human judgments in most cases. In addition, we find that the effectiveness\nof the ChatGPT evaluator might be influenced by the creation method of the\nmeta-evaluation datasets. For the meta-evaluation datasets which are created\ngreatly depending on the reference and thus are biased, the ChatGPT evaluator\nmight lose its effectiveness. We hope our preliminary study could prompt the\nemergence of a general-purposed reliable NLG metric.", "no": 68}, {"url": "https://arxiv.org/abs/2305.17926", "title": "Large Language Models are not Fair Evaluators", "cites": "121", "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of\nadopting large language models~(LLMs), e.g., GPT-4, as a referee to score and\ncompare the quality of responses generated by candidate models. We find that\nthe quality ranking of candidate responses can be easily hacked by simply\naltering their order of appearance in the context. This manipulation allows us\nto skew the evaluation result, making one model appear considerably superior to\nthe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries\nwith ChatGPT as an evaluator. To address this issue, we propose a calibration\nframework with three simple yet effective strategies: 1) Multiple Evidence\nCalibration, which requires the evaluator model to generate multiple evaluation\nevidence before assigning ratings; 2) Balanced Position Calibration, which\naggregates results across various orders to determine the final score; 3)\nHuman-in-the-Loop Calibration, which introduces a balanced position diversity\nentropy to measure the difficulty of each example and seeks human assistance\nwhen needed. We also manually annotate the \"win/tie/lose\" outcomes of responses\nfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and\nextensive experiments demonstrate that our approach successfully mitigates\nevaluation bias, resulting in closer alignment with human judgments. We release\nour code and human annotation at \\url{https://github.com/i-Eval/FairEval} to\nfacilitate future research.", "no": 69}, {"url": "https://arxiv.org/abs/2303.11156", "title": "Can AI-Generated Text be Reliably Detected?", "cites": "117", "abstract": "In this paper, both empirically and theoretically, we show that several\nAI-text detectors are not reliable in practical scenarios. Empirically, we show\nthat paraphrasing attacks, where a light paraphraser is applied on top of a\nlarge language model (LLM), can break a whole range of detectors, including\nones using watermarking schemes as well as neural network-based detectors and\nzero-shot classifiers. Our experiments demonstrate that retrieval-based\ndetectors, designed to evade paraphrasing attacks, are still vulnerable to\nrecursive paraphrasing. We then provide a theoretical impossibility result\nindicating that as language models become more sophisticated and better at\nemulating human text, the performance of even the best-possible detector\ndecreases. For a sufficiently advanced language model seeking to imitate human\ntext, even the best-possible detector may only perform marginally better than a\nrandom classifier. Our result is general enough to capture specific scenarios\nsuch as particular writing styles, clever prompt design, or text paraphrasing.\nWe also extend the impossibility result to include the case where pseudorandom\nnumber generators are used for AI-text generation instead of true randomness.\nWe show that the same result holds with a negligible correction term for all\npolynomial-time computable detectors. Finally, we show that even LLMs protected\nby watermarking schemes can be vulnerable against spoofing attacks where\nadversarial humans can infer hidden LLM text signatures and add them to\nhuman-generated text to be detected as text generated by the LLMs, potentially\ncausing reputational damage to their developers. We believe these results can\nopen an honest conversation in the community regarding the ethical and reliable\nuse of AI-generated text.", "no": 70}, {"url": "https://arxiv.org/abs/2304.13712", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "cites": "115", "abstract": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.", "no": 71}, {"url": "https://arxiv.org/abs/2306.12014", "title": "3HAN: A Deep Neural Network for Fake News Detection", "cites": "115", "abstract": "The rapid spread of fake news is a serious problem calling for AI solutions.\nWe employ a deep learning based automated detector through a three level\nhierarchical attention network (3HAN) for fast, accurate detection of fake\nnews. 3HAN has three levels, one each for words, sentences, and the headline,\nand constructs a news vector: an effective representation of an input news\narticle, by processing an article in an hierarchical bottom-up manner. The\nheadline is known to be a distinguishing feature of fake news, and furthermore,\nrelatively few words and sentences in an article are more important than the\nrest. 3HAN gives a differential importance to parts of an article, on account\nof its three layers of attention. By experiments on a large real-world data\nset, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike\nsome other deep learning models, 3HAN provides an understandable output through\nthe attention weights given to different parts of an article, which can be\nvisualized through a heatmap to enable further manual fact checking.", "no": 72}, {"url": "https://arxiv.org/abs/2304.06364", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "cites": "114", "abstract": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.", "no": 73}, {"url": "https://arxiv.org/abs/2302.04166", "title": "GPTScore: Evaluate as You Desire", "cites": "113", "abstract": "Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation--how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.", "no": 74}, {"url": "https://arxiv.org/abs/2302.00093", "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "cites": "112", "abstract": "Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.", "no": 75}, {"url": "https://arxiv.org/abs/2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "cites": "112", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM", "no": 76}, {"url": "https://arxiv.org/abs/2307.03109", "title": "A Survey on Evaluation of Large Language Models", "cites": "112", "abstract": "Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.", "no": 77}, {"url": "https://arxiv.org/abs/2306.02858", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding", "cites": "110", "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.", "no": 78}, {"url": "https://arxiv.org/abs/2301.13848", "title": "Benchmarking Large Language Models for News Summarization", "cites": "109", "abstract": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.", "no": 79}, {"url": "https://arxiv.org/abs/2303.08896", "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "cites": "109", "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.", "no": 80}, {"url": "https://arxiv.org/abs/2302.06675", "title": "Symbolic Discovery of Optimization Algorithms", "cites": "108", "abstract": "We present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training.\nWe leverage efficient search techniques to explore an infinite and sparse\nprogram space. To bridge the large generalization gap between proxy and target\ntasks, we also introduce program selection and simplification strategies. Our\nmethod discovers a simple and effective optimization algorithm, $\\textbf{Lion}$\n($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and\n91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best\nresults by 2% and 0.1%, respectively. On diffusion models, Lion outperforms\nAdam by achieving a better FID score and reducing the training compute by up to\n2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion\nexhibits a similar or better performance compared to Adam. Our analysis of Lion\nreveals that its performance gain grows with the training batch size. It also\nrequires a smaller learning rate than Adam due to the larger norm of the update\nproduced by the sign function. Additionally, we examine the limitations of Lion\nand identify scenarios where its improvements are small or not statistically\nsignificant. Lion is also successfully deployed in production systems such as\nGoogle search ads CTR model.", "no": 81}, {"url": "https://arxiv.org/abs/2304.09842", "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language\n  Models", "cites": "108", "abstract": "Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner. The project is available at\nhttps://chameleon-llm.github.io.", "no": 82}, {"url": "https://arxiv.org/abs/2305.04790", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", "cites": "105", "abstract": "We present a vision and language model named MultiModal-GPT to conduct\nmulti-round dialogue with humans. MultiModal-GPT can follow various\ninstructions from humans, such as generating a detailed caption, counting the\nnumber of interested objects, and answering general questions from users.\nMultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with\nLow-rank Adapter (LoRA) added both in the cross-attention part and the\nself-attention part of the language model. We first construct instruction\ntemplates with vision and language data for multi-modality instruction tuning\nto make the model understand and follow human instructions. We find the quality\nof training data is vital for the dialogue performance, where few data\ncontaining short answers can lead the model to respond shortly to any\ninstructions. To further enhance the ability to chat with humans of the\nMultiModal-GPT, we utilize language-only instruction-following data to train\nthe MultiModal-GPT jointly. The joint training of language-only and\nvisual-language instructions with the \\emph{same} instruction template\neffectively improves dialogue performance. Various demos show the ability of\ncontinuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are\nat https://github.com/open-mmlab/Multimodal-GPT", "no": 83}, {"url": "https://arxiv.org/abs/2305.14325", "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate", "cites": "103", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.", "no": 84}, {"url": "https://arxiv.org/abs/2305.20050", "title": "Let's Verify Step by Step", "cites": "103", "abstract": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.", "no": 85}, {"url": "https://arxiv.org/abs/2305.08322", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models", "cites": "102", "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.", "no": 86}, {"url": "https://arxiv.org/abs/2301.08745", "title": "Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine", "cites": "101", "abstract": "This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well\nwith minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. As for the\ntranslation robustness, ChatGPT does not perform as well as the commercial\nsystems on biomedical abstracts or Reddit comments but exhibits good results on\nspoken language. Further, we explore an interesting strategy named\n$\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to\ntranslate the source sentence into a high-resource pivot language before into\nthe target language, improving the translation performance noticeably. With the\nlaunch of the GPT-4 engine, the translation performance of ChatGPT is\nsignificantly boosted, becoming comparable to commercial translation products,\neven for distant languages. Human analysis on Google Translate and ChatGPT\nsuggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and\nmis-translation errors while that with GPT-4 makes the least errors. In other\nwords, ChatGPT has already become a good translator. Please refer to our Github\nproject for more details:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator", "no": 87}, {"url": "https://arxiv.org/abs/2305.14387", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human\n  Feedback", "cites": "101", "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their ability to follow user instructions well. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following process\nfaces three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 45x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.", "no": 88}, {"url": "https://arxiv.org/abs/2307.09009", "title": "How is ChatGPT's behavior changing over time?", "cites": "100", "abstract": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nseveral diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)\nopinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating\ncode, 6) US Medical License tests, and 7) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was reasonable at identifying prime vs.\ncomposite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same\nquestions (51% accuracy). This is partly explained by a drop in GPT-4's amenity\nto follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in\nJune than in March in this task. GPT-4 became less willing to answer sensitive\nquestions and opinion survey questions in June than in March. GPT-4 performed\nbetter at multi-hop questions in June than in March, while GPT-3.5's\nperformance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting\nmistakes in code generation in June than in March. We provide evidence that\nGPT-4's ability to follow user instructions has decreased over time, which is\none common factor behind the many behavior drifts. Overall, our findings show\nthat the behavior of the \"same\" LLM service can change substantially in a\nrelatively short amount of time, highlighting the need for continuous\nmonitoring of LLMs.", "no": 89}, {"url": "https://arxiv.org/abs/2303.17760", "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language\n  Model Society", "cites": "99", "abstract": "The rapid advancement of chat-based language models has led to remarkable\nprogress in complex task-solving. However, their success heavily relies on\nhuman input to guide the conversation, which can be challenging and\ntime-consuming. This paper explores the potential of building scalable\ntechniques to facilitate autonomous cooperation among communicative agents, and\nprovides insight into their \"cognitive\" processes. To address the challenges of\nachieving autonomous cooperation, we propose a novel communicative agent\nframework named role-playing. Our approach involves using inception prompting\nto guide chat agents toward task completion while maintaining consistency with\nhuman intentions. We showcase how role-playing can be used to generate\nconversational data for studying the behaviors and capabilities of a society of\nagents, providing a valuable resource for investigating conversational language\nmodels. In particular, we conduct comprehensive studies on\ninstruction-following cooperation in multi-agent settings. Our contributions\ninclude introducing a novel communicative agent framework, offering a scalable\napproach for studying the cooperative behaviors and capabilities of multi-agent\nsystems, and open-sourcing our library to support research on communicative\nagents and beyond: https://github.com/camel-ai/camel.", "no": 90}, {"url": "https://arxiv.org/abs/2302.14520", "title": "Large Language Models Are State-of-the-Art Evaluators of Translation\n  Quality", "cites": "98", "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality,\nwhich works both with a reference translation and without. In our evaluation,\nwe focus on zero-shot prompting, comparing four prompt variants in two modes,\nbased on the availability of the reference. We investigate nine versions of GPT\nmodels, including ChatGPT and GPT-4. We show that our method for translation\nquality assessment only works with GPT~3.5 and larger models. Comparing to\nresults from WMT22's Metrics shared task, our method achieves state-of-the-art\naccuracy in both modes when compared to MQM-based human labels. Our results are\nvalid on the system level for all three WMT22 Metrics shared task language\npairs, namely English into German, English into Russian, and Chinese into\nEnglish. This provides a first glimpse into the usefulness of pre-trained,\ngenerative large language models for quality assessment of translations. We\npublicly release all our code and prompt templates used for the experiments\ndescribed in this work, as well as all corresponding scoring results, to allow\nfor external validation and reproducibility.", "no": 91}, {"url": "https://arxiv.org/abs/2304.05197", "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT", "cites": "97", "abstract": "With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.", "no": 92}, {"url": "https://arxiv.org/abs/2302.12095", "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective", "cites": "96", "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.", "no": 93}, {"url": "https://arxiv.org/abs/2305.06355", "title": "VideoChat: Chat-Centric Video Understanding", "cites": "96", "abstract": "In this study, we initiate an exploration into video understanding by\nintroducing VideoChat, an end-to-end chat-centric video understanding system.\nIt integrates video foundation models and large language models via a learnable\nneural interface, excelling in spatiotemporal reasoning, event localization,\nand causal relationship inference. To instructively tune this system, we\npropose a video-centric instruction dataset, composed of thousands of videos\nmatched with detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and causal relationships, providing a valuable asset\nfor training chat-centric video understanding systems. Preliminary qualitative\nexperiments reveal our system's potential across a broad spectrum of video\napplications and set the standard for future research. Access our code and data\nat https://github.com/OpenGVLab/Ask-Anything", "no": 94}, {"url": "https://arxiv.org/abs/2305.15717", "title": "The False Promise of Imitating Proprietary LLMs", "cites": "94", "abstract": "An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.", "no": 95}, {"url": "https://arxiv.org/abs/2306.08302", "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "cites": "94", "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.", "no": 96}, {"url": "https://arxiv.org/abs/2308.11432", "title": "A Survey on Large Language Model based Autonomous Agents", "cites": "94", "abstract": "Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.", "no": 97}, {"url": "https://arxiv.org/abs/2302.00083", "title": "In-Context Retrieval-Augmented Language Models", "cites": "93", "abstract": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.", "no": 98}, {"url": "https://arxiv.org/abs/2302.10198", "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and\n  Fine-tuned BERT", "cites": "93", "abstract": "Recently, ChatGPT has attracted great attention, as it can generate fluent\nand high-quality responses to human inquiries. Several prior studies have shown\nthat ChatGPT attains remarkable generation ability compared with existing\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\nhas been given little attention. In this report, we explore the understanding\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\nachieves comparable performance compared with BERT on sentiment analysis and\nquestion-answering tasks. Additionally, by combining some advanced prompting\nstrategies, we show that the understanding ability of ChatGPT can be further\nimproved.", "no": 99}, {"url": "https://arxiv.org/abs/2302.10205", "title": "Zero-Shot Information Extraction via Chatting with ChatGPT", "cites": "92", "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the\nunannotated text. It is challenging due to involving little human intervention.\nChallenging but worthwhile, zero-shot IE reduces the time and effort that data\nlabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,\nChatGPT) show promising performance on zero-shot settings, thus inspiring us to\nexplore prompt-based methods. In this work, we ask whether strong IE models can\nbe constructed by directly prompting LLMs. Specifically, we transform the\nzero-shot IE task into a multi-turn question-answering problem with a two-stage\nframework (ChatIE). With the power of ChatGPT, we extensively evaluate our\nframework on three IE tasks: entity-relation triple extract, named entity\nrecognition, and event extraction. Empirical results on six datasets across two\nlanguages show that ChatIE achieves impressive performance and even surpasses\nsome full-shot models on several datasets (e.g., NYT11-HRL). We believe that\nour work could shed light on building IE models with limited resources.", "no": 100}]