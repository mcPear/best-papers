[{"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": 2798}, {"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": 1749}, {"url": "https://arxiv.org/abs/2303.08774", "title": "GPT-4 Technical Report", "cites": 1308}, {"url": "https://arxiv.org/abs/2303.12712", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "cites": 676}, {"url": "https://arxiv.org/abs/2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "cites": 505}, {"url": "https://arxiv.org/abs/2303.18223", "title": "A Survey of Large Language Models", "cites": 402}, {"url": "https://arxiv.org/abs/2308.02976", "title": "Spanish Pre-trained BERT Model and Evaluation Data", "cites": 371}, {"url": "https://arxiv.org/abs/2302.04023", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on\n  Reasoning, Hallucination, and Interactivity", "cites": 341}, {"url": "https://arxiv.org/abs/2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "cites": 303}, {"url": "https://arxiv.org/abs/2305.10403", "title": "PaLM 2 Technical Report", "cites": 238}, {"url": "https://arxiv.org/abs/2308.11995", "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "cites": 237}, {"url": "https://arxiv.org/abs/2304.08485", "title": "Visual Instruction Tuning", "cites": 229}, {"url": "https://arxiv.org/abs/2303.17580", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "cites": 221}, {"url": "https://arxiv.org/abs/2302.06476", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "cites": 200}, {"url": "https://arxiv.org/abs/2306.05685", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "cites": 188}, {"url": "https://arxiv.org/abs/2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "cites": 185}, {"url": "https://arxiv.org/abs/2303.13375", "title": "Capabilities of GPT-4 on Medical Challenge Problems", "cites": 166}, {"url": "https://arxiv.org/abs/2304.03277", "title": "Instruction Tuning with GPT-4", "cites": 163}, {"url": "https://arxiv.org/abs/2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "cites": 158}, {"url": "https://arxiv.org/abs/2301.07597", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,\n  and Detection", "cites": 151}, {"url": "https://arxiv.org/abs/2301.13688", "title": "The Flan Collection: Designing Data and Methods for Effective\n  Instruction Tuning", "cites": 148}, {"url": "https://arxiv.org/abs/2302.03494", "title": "A Categorical Archive of ChatGPT Failures", "cites": 148}, {"url": "https://arxiv.org/abs/2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "cites": 141}, {"url": "https://arxiv.org/abs/2304.01373", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "cites": 138}, {"url": "https://arxiv.org/abs/2303.15056", "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "cites": 133}, {"url": "https://arxiv.org/abs/2301.13867", "title": "Mathematical Capabilities of ChatGPT", "cites": 128}, {"url": "https://arxiv.org/abs/2302.07842", "title": "Augmented Language Models: a Survey", "cites": 126}, {"url": "https://arxiv.org/abs/2302.09419", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT", "cites": 125}, {"url": "https://arxiv.org/abs/2301.11305", "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability\n  Curvature", "cites": 121}, {"url": "https://arxiv.org/abs/2301.10226", "title": "A Watermark for Large Language Models", "cites": 119}, {"url": "https://arxiv.org/abs/2303.16199", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "cites": 117}, {"url": "https://arxiv.org/abs/2303.16634", "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "cites": 115}, {"url": "https://arxiv.org/abs/2306.12014", "title": "3HAN: A Deep Neural Network for Fake News Detection", "cites": 112}, {"url": "https://arxiv.org/abs/2302.14624", "title": "The 2022 NIST Language Recognition Evaluation", "cites": 111}, {"url": "https://arxiv.org/abs/2303.17564", "title": "BloombergGPT: A Large Language Model for Finance", "cites": 111}, {"url": "https://arxiv.org/abs/2301.02111", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "cites": 110}, {"url": "https://arxiv.org/abs/2305.06161", "title": "StarCoder: may the source be with you!", "cites": 105}, {"url": "https://arxiv.org/abs/2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "cites": 104}, {"url": "https://arxiv.org/abs/2304.12244", "title": "WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions", "cites": 102}, {"url": "https://arxiv.org/abs/2302.12813", "title": "Check Your Facts and Try Again: Improving Large Language Models with\n  External Knowledge and Automated Feedback", "cites": 96}, {"url": "https://arxiv.org/abs/2301.06627", "title": "Dissociating language and thought in large language models: a cognitive\n  perspective", "cites": 91}, {"url": "https://arxiv.org/abs/2303.11381", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "cites": 91}, {"url": "https://arxiv.org/abs/2306.01116", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "cites": 91}, {"url": "https://arxiv.org/abs/2301.07093", "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "cites": 90}, {"url": "https://arxiv.org/abs/2305.11206", "title": "LIMA: Less Is More for Alignment", "cites": 89}, {"url": "https://arxiv.org/abs/2306.17582", "title": "ChatGPT for Robotics: Design Principles and Model Abilities", "cites": 88}, {"url": "https://arxiv.org/abs/2301.12652", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "cites": 86}, {"url": "https://arxiv.org/abs/2303.04226", "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of\n  Generative AI from GAN to ChatGPT", "cites": 84}, {"url": "https://arxiv.org/abs/2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "cites": 84}, {"url": "https://arxiv.org/abs/2302.04166", "title": "GPTScore: Evaluate as You Desire", "cites": 83}, {"url": "https://arxiv.org/abs/2304.05128", "title": "Teaching Large Language Models to Self-Debug", "cites": 83}, {"url": "https://arxiv.org/abs/2302.09210", "title": "How Good Are GPT Models at Machine Translation? A Comprehensive\n  Evaluation", "cites": 81}, {"url": "https://arxiv.org/abs/2303.13367", "title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing", "cites": 80}, {"url": "https://arxiv.org/abs/2304.01852", "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future\n  of Large Language Models", "cites": 80}, {"url": "https://arxiv.org/abs/2302.00923", "title": "Multimodal Chain-of-Thought Reasoning in Language Models", "cites": 79}, {"url": "https://arxiv.org/abs/2304.15010", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", "cites": 79}, {"url": "https://arxiv.org/abs/2304.01196", "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data", "cites": 76}, {"url": "https://arxiv.org/abs/2303.04048", "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study", "cites": 75}, {"url": "https://arxiv.org/abs/2302.02083", "title": "Theory of Mind Might Have Spontaneously Emerged in Large Language Models", "cites": 73}, {"url": "https://arxiv.org/abs/2302.06675", "title": "Symbolic Discovery of Optimization Algorithms", "cites": 73}, {"url": "https://arxiv.org/abs/2303.11156", "title": "Can AI-Generated Text be Reliably Detected?", "cites": 73}, {"url": "https://arxiv.org/abs/2303.03846", "title": "Larger language models do in-context learning differently", "cites": 72}, {"url": "https://arxiv.org/abs/2301.13826", "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image\n  Diffusion Models", "cites": 71}, {"url": "https://arxiv.org/abs/2304.13712", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "cites": 71}, {"url": "https://arxiv.org/abs/2303.08896", "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "cites": 70}, {"url": "https://arxiv.org/abs/2304.06364", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "cites": 70}, {"url": "https://arxiv.org/abs/2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "cites": 70}, {"url": "https://arxiv.org/abs/2302.10724", "title": "ChatGPT: Jack of all trades, master of none", "cites": 67}, {"url": "https://arxiv.org/abs/2304.09842", "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language\n  Models", "cites": 67}, {"url": "https://arxiv.org/abs/2301.13848", "title": "Benchmarking Large Language Models for News Summarization", "cites": 65}, {"url": "https://arxiv.org/abs/2305.17926", "title": "Large Language Models are not Fair Evaluators", "cites": 65}, {"url": "https://arxiv.org/abs/2301.08745", "title": "Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine", "cites": 64}, {"url": "https://arxiv.org/abs/2303.03915", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "cites": 64}, {"url": "https://arxiv.org/abs/2302.00093", "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "cites": 62}, {"url": "https://arxiv.org/abs/2302.10198", "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and\n  Fine-tuned BERT", "cites": 61}, {"url": "https://arxiv.org/abs/2302.12095", "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective", "cites": 61}, {"url": "https://arxiv.org/abs/2302.14520", "title": "Large Language Models Are State-of-the-Art Evaluators of Translation\n  Quality", "cites": 61}, {"url": "https://arxiv.org/abs/2305.18290", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model", "cites": 61}, {"url": "https://arxiv.org/abs/2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "cites": 61}, {"url": "https://arxiv.org/abs/2305.08322", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models", "cites": 60}, {"url": "https://arxiv.org/abs/2307.15043", "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models", "cites": 60}, {"url": "https://arxiv.org/abs/2303.04222", "title": "SemEval-2023 Task 10: Explainable Detection of Online Sexism", "cites": 59}, {"url": "https://arxiv.org/abs/2302.10205", "title": "Zero-Shot Information Extraction via Chatting with ChatGPT", "cites": 58}, {"url": "https://arxiv.org/abs/2303.16434", "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with\n  Millions of APIs", "cites": 58}, {"url": "https://arxiv.org/abs/2305.04790", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", "cites": 58}, {"url": "https://arxiv.org/abs/2302.00083", "title": "In-Context Retrieval-Augmented Language Models", "cites": 57}, {"url": "https://arxiv.org/abs/2303.17491", "title": "Language Models can Solve Computer Tasks", "cites": 57}, {"url": "https://arxiv.org/abs/2305.09617", "title": "Towards Expert-Level Medical Question Answering with Large Language\n  Models", "cites": 57}, {"url": "https://arxiv.org/abs/2307.09009", "title": "How is ChatGPT's behavior changing over time?", "cites": 57}, {"url": "https://arxiv.org/abs/2303.17760", "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale\n  Language Model Society", "cites": 56}, {"url": "https://arxiv.org/abs/2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "cites": 55}, {"url": "https://arxiv.org/abs/2304.03439", "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4", "cites": 55}, {"url": "https://arxiv.org/abs/2302.07736", "title": "Is ChatGPT better than Human Annotators? Potential and Limitations of\n  ChatGPT in Explaining Implicit Hate Speech", "cites": 54}, {"url": "https://arxiv.org/abs/2304.05197", "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT", "cites": 53}, {"url": "https://arxiv.org/abs/2305.04388", "title": "Language Models Don't Always Say What They Think: Unfaithful\n  Explanations in Chain-of-Thought Prompting", "cites": 53}, {"url": "https://arxiv.org/abs/2305.02301", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less\n  Training Data and Smaller Model Sizes", "cites": 52}, {"url": "https://arxiv.org/abs/2302.08081", "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text\n  Summarization", "cites": 51}, {"url": "https://arxiv.org/abs/2302.08399", "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks", "cites": 51}, {"url": "https://arxiv.org/abs/2304.08466", "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification", "cites": 51}, {"url": "https://arxiv.org/abs/2305.20050", "title": "Let's Verify Step by Step", "cites": 51}]