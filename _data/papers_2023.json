[{"url": "https://arxiv.org/abs/2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "cites": "6 586", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.", "no": 1}, {"url": "https://arxiv.org/abs/2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "cites": "5 608", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "no": 2}, {"url": "https://arxiv.org/abs/2303.08774", "title": "GPT-4 Technical Report", "cites": "5 412", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.", "no": 3}, {"url": "https://arxiv.org/abs/2310.16713", "title": "SkyMath: Technical Report", "cites": "5 412", "abstract": "Large language models (LLMs) have shown great potential to solve varieties of\nnatural language processing (NLP) tasks, including mathematical reasoning. In\nthis work, we present SkyMath, a large language model for mathematics with 13\nbillion parameters. By applying self-compare fine-tuning, we have enhanced\nmathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,\nSkyMath outperforms all known open-source models of similar size and has\nestablished a new SOTA performance.", "no": 4}, {"url": "https://arxiv.org/abs/2301.04856", "title": "Multimodal Deep Learning", "cites": "2 953", "abstract": "This book is the result of a seminar in which we reviewed multimodal\napproaches and attempted to create a solid overview of the field, starting with\nthe current state-of-the-art approaches in the two subfields of Deep Learning\nindividually. Further, modeling frameworks are discussed where one modality is\ntransformed into the other, as well as models in which one modality is utilized\nto enhance representation learning for the other. To conclude the second part,\narchitectures with a focus on handling both modalities simultaneously are\nintroduced. Finally, we also cover other modalities as well as general-purpose\nmulti-modal models, which are able to handle different tasks on different\nmodalities within one unified architecture. One interesting application\n(Generative Art) eventually caps off this booklet.", "no": 5}, {"url": "https://arxiv.org/abs/2303.12712", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "cites": "1 949", "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.", "no": 6}, {"url": "https://arxiv.org/abs/2304.08485", "title": "Visual Instruction Tuning", "cites": "1 598", "abstract": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.", "no": 7}, {"url": "https://arxiv.org/abs/2306.05685", "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "cites": "1 597", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "no": 8}, {"url": "https://arxiv.org/abs/2303.18223", "title": "A Survey of Large Language Models", "cites": "1 259", "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.", "no": 9}, {"url": "https://arxiv.org/abs/2305.18290", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model", "cites": "1 123", "abstract": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.", "no": 10}, {"url": "https://arxiv.org/abs/2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "cites": "952", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.", "no": 11}, {"url": "https://arxiv.org/abs/2302.04023", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on\n  Reasoning, Hallucination, and Interactivity", "cites": "902", "abstract": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.", "no": 12}, {"url": "https://arxiv.org/abs/2308.12950", "title": "Code Llama: Open Foundation Models for Code", "cites": "861", "abstract": "We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are\ntrained on sequences of 16k tokens and show improvements on inputs with up to\n100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants\nsupport infilling based on surrounding content. Code Llama reaches\nstate-of-the-art performance among open models on several code benchmarks, with\nscores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code\nLlama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our\nmodels outperform every other publicly available model on MultiPL-E. We release\nCode Llama under a permissive license that allows for both research and\ncommercial use.", "no": 13}, {"url": "https://arxiv.org/abs/2310.03744", "title": "Improved Baselines with Visual Instruction Tuning", "cites": "840", "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.", "no": 14}, {"url": "https://arxiv.org/abs/2305.10403", "title": "PaLM 2 Technical Report", "cites": "772", "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.", "no": 15}, {"url": "https://arxiv.org/abs/2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "cites": "770", "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.", "no": 16}, {"url": "https://arxiv.org/abs/2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "cites": "680", "abstract": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.", "no": 17}, {"url": "https://arxiv.org/abs/2310.06825", "title": "Mistral 7B", "cites": "678", "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.", "no": 18}, {"url": "https://arxiv.org/abs/2304.01373", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "cites": "652", "abstract": "How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.", "no": 19}, {"url": "https://arxiv.org/abs/2307.15043", "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models", "cites": "586", "abstract": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.", "no": 20}, {"url": "https://arxiv.org/abs/2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "cites": "582", "abstract": "While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nthe performance of language models on two tasks that require identifying\nrelevant information in their input contexts: multi-document question answering\nand key-value retrieval. We find that performance can degrade significantly\nwhen changing the position of relevant information, indicating that current\nlanguage models do not robustly make use of information in long input contexts.\nIn particular, we observe that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts, even for explicitly long-context models. Our analysis\nprovides a better understanding of how language models use their input context\nand provides new evaluation protocols for future long-context language models.", "no": 21}, {"url": "https://arxiv.org/abs/2304.12244", "title": "WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions", "cites": "554", "abstract": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM", "no": 22}, {"url": "https://arxiv.org/abs/2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "cites": "545", "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "no": 23}, {"url": "https://arxiv.org/abs/2308.02976", "title": "Spanish Pre-trained BERT Model and Evaluation Data", "cites": "533", "abstract": "The Spanish language is one of the top 5 spoken languages in the world.\nNevertheless, finding resources to train or evaluate Spanish language models is\nnot an easy task. In this paper we help bridge this gap by presenting a\nBERT-based language model pre-trained exclusively on Spanish data. As a second\ncontribution, we also compiled several tasks specifically for the Spanish\nlanguage in a single repository much in the spirit of the GLUE benchmark. By\nfine-tuning our pre-trained Spanish model, we obtain better results compared to\nother BERT-based models pre-trained on multilingual corpora for most of the\ntasks, even achieving a new state-of-the-art on some of them. We have publicly\nreleased our model, the pre-training data, and the compilation of the Spanish\nbenchmarks.", "no": 24}, {"url": "https://arxiv.org/abs/2303.17580", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "cites": "531", "abstract": "Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.", "no": 25}, {"url": "https://arxiv.org/abs/2309.16609", "title": "Qwen Technical Report", "cites": "515", "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, enabling natural language processing tasks that were previously\nthought to be exclusive to humans. In this work, we introduce Qwen, the first\ninstallment of our large language model series. Qwen is a comprehensive\nlanguage model series that encompasses distinct models with varying parameter\ncounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,\nthe chat models finetuned with human alignment techniques. The base language\nmodels consistently demonstrate superior performance across a multitude of\ndownstream tasks, and the chat models, particularly those trained using\nReinforcement Learning from Human Feedback (RLHF), are highly competitive. The\nchat models possess advanced tool-use and planning capabilities for creating\nagent applications, showcasing impressive performance even when compared to\nbigger models on complex tasks like utilizing a code interpreter. Furthermore,\nwe have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as\nwell as mathematics-focused models, Math-Qwen-Chat, which are built upon base\nlanguage models. These models demonstrate significantly improved performance in\ncomparison with open-source models, and slightly fall behind the proprietary\nmodels.", "no": 26}, {"url": "https://arxiv.org/abs/2303.13375", "title": "Capabilities of GPT-4 on Medical Challenge Problems", "cites": "502", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.", "no": 27}, {"url": "https://arxiv.org/abs/2306.01116", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "cites": "502", "abstract": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "no": 28}, {"url": "https://arxiv.org/abs/2303.15056", "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "cites": "499", "abstract": "Many NLP applications require manual data annotations for a variety of tasks,\nnotably to train classifiers or evaluate the performance of unsupervised\nmodels. Depending on the size and degree of complexity, the tasks may be\nconducted by crowd-workers on platforms such as MTurk as well as trained\nannotators, such as research assistants. Using a sample of 2,382 tweets, we\ndemonstrate that ChatGPT outperforms crowd-workers for several annotation\ntasks, including relevance, stance, topics, and frames detection. Specifically,\nthe zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\nfive tasks, while ChatGPT's intercoder agreement exceeds that of both\ncrowd-workers and trained annotators for all tasks. Moreover, the\nper-annotation cost of ChatGPT is less than $0.003 -- about twenty times\ncheaper than MTurk. These results show the potential of large language models\nto drastically increase the efficiency of text classification.", "no": 29}, {"url": "https://arxiv.org/abs/2303.16634", "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "cites": "499", "abstract": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval", "no": 30}, {"url": "https://arxiv.org/abs/2302.06476", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "cites": "492", "abstract": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.", "no": 31}, {"url": "https://arxiv.org/abs/2307.03109", "title": "A Survey on Evaluation of Large Language Models", "cites": "478", "abstract": "Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.", "no": 32}, {"url": "https://arxiv.org/abs/2303.16199", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "cites": "476", "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.", "no": 33}, {"url": "https://arxiv.org/abs/2307.15818", "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic\n  Control", "cites": "456", "abstract": "We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).", "no": 34}, {"url": "https://arxiv.org/abs/2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "cites": "450", "abstract": "Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.", "no": 35}, {"url": "https://arxiv.org/abs/2305.11206", "title": "LIMA: Less Is More for Alignment", "cites": "424", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.", "no": 36}, {"url": "https://arxiv.org/abs/2303.17564", "title": "BloombergGPT: A Large Language Model for Finance", "cites": "417", "abstract": "The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.", "no": 37}, {"url": "https://arxiv.org/abs/2308.11432", "title": "A Survey on Large Language Model based Autonomous Agents", "cites": "407", "abstract": "Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.", "no": 38}, {"url": "https://arxiv.org/abs/2301.07597", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,\n  and Detection", "cites": "400", "abstract": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.", "no": 39}, {"url": "https://arxiv.org/abs/2306.02858", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding", "cites": "384", "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.", "no": 40}, {"url": "https://arxiv.org/abs/2309.07864", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey", "cites": "384", "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.", "no": 41}, {"url": "https://arxiv.org/abs/2301.13688", "title": "The Flan Collection: Designing Data and Methods for Effective\n  Instruction Tuning", "cites": "381", "abstract": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.", "no": 42}, {"url": "https://arxiv.org/abs/2309.10305", "title": "Baichuan 2: Open Large-scale Language Models", "cites": "377", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.", "no": 43}, {"url": "https://arxiv.org/abs/2305.06161", "title": "StarCoder: may the source be with you!", "cites": "376", "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.", "no": 44}, {"url": "https://arxiv.org/abs/2304.03277", "title": "Instruction Tuning with GPT-4", "cites": "368", "abstract": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.", "no": 45}, {"url": "https://arxiv.org/abs/2306.14824", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "cites": "358", "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.", "no": 46}, {"url": "https://arxiv.org/abs/2304.15010", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", "cites": "355", "abstract": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.", "no": 47}, {"url": "https://arxiv.org/abs/2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "cites": "351", "abstract": "Large language models (LLMs) have demonstrated significant universal\ncapabilities as few/zero-shot learners in various tasks due to their\npre-training on vast amounts of text data, as exemplified by GPT-3, which\nboosted to InstrctGPT and ChatGPT, effectively following natural language\ninstructions to accomplish real-world tasks. In this paper, we propose to\nintroduce instruction tuning into multi-modal models, motivated by the Flamingo\nmodel's upstream interleaved format pretraining dataset. We adopt a similar\napproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)\ndataset. We then introduce Otter, a multi-modal model based on OpenFlamingo\n(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and\nshowcasing improved instruction-following ability and in-context learning. We\nalso optimize OpenFlamingo's implementation for researchers, democratizing the\nrequired training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs,\nand integrate both OpenFlamingo and Otter into Huggingface Transformers for\nmore researchers to incorporate the models into their customized training and\ninference pipelines.", "no": 48}, {"url": "https://arxiv.org/abs/2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "cites": "349", "abstract": "A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.", "no": 49}, {"url": "https://arxiv.org/abs/2304.05128", "title": "Teaching Large Language Models to Self-Debug", "cites": "349", "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.", "no": 50}, {"url": "https://arxiv.org/abs/2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "cites": "348", "abstract": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.", "no": 51}, {"url": "https://arxiv.org/abs/2301.11305", "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability\n  Curvature", "cites": "347", "abstract": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.", "no": 52}, {"url": "https://arxiv.org/abs/2301.12652", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "cites": "340", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.", "no": 53}, {"url": "https://arxiv.org/abs/2301.02111", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "cites": "331", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS).\nSpecifically, we train a neural codec language model (called Vall-E) using\ndiscrete codes derived from an off-the-shelf neural audio codec model, and\nregard TTS as a conditional language modeling task rather than continuous\nsignal regression as in previous work. During the pre-training stage, we scale\nup the TTS training data to 60K hours of English speech which is hundreds of\ntimes larger than existing systems. Vall-E emerges in-context learning\ncapabilities and can be used to synthesize high-quality personalized speech\nwith only a 3-second enrolled recording of an unseen speaker as an acoustic\nprompt. Experiment results show that Vall-E significantly outperforms the\nstate-of-the-art zero-shot TTS system in terms of speech naturalness and\nspeaker similarity. In addition, we find Vall-E could preserve the speaker's\nemotion and acoustic environment of the acoustic prompt in synthesis. See\nhttps://aka.ms/valle for demos of our work.", "no": 54}, {"url": "https://arxiv.org/abs/2301.07093", "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "cites": "324", "abstract": "Large-scale text-to-image diffusion models have made amazing advances.\nHowever, the status quo is to use text input alone, which can impede\ncontrollability. In this work, we propose GLIGEN, Grounded-Language-to-Image\nGeneration, a novel approach that builds upon and extends the functionality of\nexisting pre-trained text-to-image diffusion models by enabling them to also be\nconditioned on grounding inputs. To preserve the vast concept knowledge of the\npre-trained model, we freeze all of its weights and inject the grounding\ninformation into new trainable layers via a gated mechanism. Our model achieves\nopen-world grounded text2img generation with caption and bounding box condition\ninputs, and the grounding ability generalizes well to novel spatial\nconfigurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS\noutperforms that of existing supervised layout-to-image baselines by a large\nmargin.", "no": 55}, {"url": "https://arxiv.org/abs/2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "cites": "323", "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.", "no": 56}, {"url": "https://arxiv.org/abs/2309.17421", "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)", "cites": "322", "abstract": "Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf", "no": 57}, {"url": "https://arxiv.org/abs/2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "cites": "314", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM", "no": 58}, {"url": "https://arxiv.org/abs/2302.09419", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT", "cites": "311", "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.", "no": 59}, {"url": "https://arxiv.org/abs/2305.09617", "title": "Towards Expert-Level Medical Question Answering with Large Language\n  Models", "cites": "303", "abstract": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.", "no": 60}, {"url": "https://arxiv.org/abs/2305.14387", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human\n  Feedback", "cites": "301", "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their strong instruction-following abilities. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following requires\ntackling three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 50x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.", "no": 61}, {"url": "https://arxiv.org/abs/2307.06281", "title": "MMBench: Is Your Multi-modal Model an All-around Player?", "cites": "299", "abstract": "Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.", "no": 62}, {"url": "https://arxiv.org/abs/2305.01210", "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of\n  Large Language Models for Code Generation", "cites": "294", "abstract": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.", "no": 63}, {"url": "https://arxiv.org/abs/2308.11995", "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "cites": "294", "abstract": "Building socialbots that can have deep, engaging open-domain conversations\nwith humans is one of the grand challenges of artificial intelligence (AI). To\nthis end, bots need to be able to leverage world knowledge spanning several\ndomains effectively when conversing with humans who have their own world\nknowledge. Existing knowledge-grounded conversation datasets are primarily\nstylized with explicit roles for conversation partners. These datasets also do\nnot explore depth or breadth of topical coverage with transitions in\nconversations. We introduce Topical-Chat, a knowledge-grounded human-human\nconversation dataset where the underlying knowledge spans 8 broad topics and\nconversation partners don't have explicitly defined roles, to help further\nresearch in open-domain conversational AI. We also train several\nstate-of-the-art encoder-decoder conversational models on Topical-Chat and\nperform automated and human evaluation for benchmarking.", "no": 64}, {"url": "https://arxiv.org/abs/2303.04226", "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of\n  Generative AI from GAN to ChatGPT", "cites": "291", "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\nattention from society. As a result, many individuals have become interested in\nrelated resources and are seeking to uncover the background and secrets behind\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\ntechniques belong to the category of Artificial Intelligence Generated Content\n(AIGC), which involves the creation of digital content, such as images, music,\nand natural language, through AI models. The goal of AIGC is to make the\ncontent creation process more efficient and accessible, allowing for the\nproduction of high-quality content at a faster pace. AIGC is achieved by\nextracting and understanding intent information from instructions provided by\nhuman, and generating the content according to its knowledge and the intent\ninformation. In recent years, large-scale models have become increasingly\nimportant in AIGC as they provide better intent extraction and thus, improved\ngeneration results. With the growth of data and the size of the models, the\ndistribution that the model can learn becomes more comprehensive and closer to\nreality, leading to more realistic and high-quality content generation. This\nsurvey provides a comprehensive review on the history of generative models, and\nbasic components, recent advances in AIGC from unimodal interaction and\nmultimodal interaction. From the perspective of unimodality, we introduce the\ngeneration tasks and relative models of text and image. From the perspective of\nmultimodality, we introduce the cross-application between the modalities\nmentioned above. Finally, we discuss the existing open problems and future\nchallenges in AIGC.", "no": 65}, {"url": "https://arxiv.org/abs/2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs", "cites": "289", "abstract": "Despite the advancements of open-source large language models (LLMs), e.g.,\nLLaMA, they remain significantly limited in tool-use capabilities, i.e., using\nexternal tools (APIs) to fulfill human instructions. The reason is that current\ninstruction tuning largely focuses on basic language tasks but ignores the\ntool-use domain. This is in contrast to the excellent tool-use capabilities of\nstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,\nwe introduce ToolLLM, a general tool-use framework encompassing data\nconstruction, model training, and evaluation. We first present ToolBench, an\ninstruction-tuning dataset for tool use, which is constructed automatically\nusing ChatGPT. Specifically, the construction can be divided into three stages:\n(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49\ncategories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to\ngenerate diverse instructions involving these APIs, covering both single-tool\nand multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to\nsearch for a valid solution path (chain of API calls) for each instruction. To\nenhance the reasoning capabilities of LLMs, we develop a novel depth-first\nsearch-based decision tree algorithm. It enables LLMs to evaluate multiple\nreasoning traces and expand the search space. Moreover, to evaluate the\ntool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.\nBased on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it\nwith a neural API retriever to recommend appropriate APIs for each instruction.\nExperiments show that ToolLLaMA demonstrates a remarkable ability to execute\ncomplex instructions and generalize to unseen APIs, and exhibits comparable\nperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot\ngeneralization ability in an out-of-distribution tool-use dataset: APIBench.", "no": 66}, {"url": "https://arxiv.org/abs/2306.08302", "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "cites": "287", "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.", "no": 67}, {"url": "https://arxiv.org/abs/2305.01937", "title": "Can Large Language Models Be an Alternative to Human Evaluations?", "cites": "285", "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.", "no": 68}, {"url": "https://arxiv.org/abs/2301.13826", "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image\n  Diffusion Models", "cites": "283", "abstract": "Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.", "no": 69}, {"url": "https://arxiv.org/abs/2306.17582", "title": "ChatGPT for Robotics: Design Principles and Model Abilities", "cites": "282", "abstract": "This paper presents an experimental study regarding the use of OpenAI's\nChatGPT for robotics applications. We outline a strategy that combines design\nprinciples for prompt engineering and the creation of a high-level function\nlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,\nand form factors. We focus our evaluations on the effectiveness of different\nprompt engineering techniques and dialog strategies towards the execution of\nvarious types of robotics tasks. We explore ChatGPT's ability to use free-form\ndialog, parse XML tags, and to synthesize code, in addition to the use of\ntask-specific prompting functions and closed-loop reasoning through dialogues.\nOur study encompasses a range of tasks within the robotics domain, from basic\nlogical, geometrical, and mathematical reasoning all the way to complex domains\nsuch as aerial navigation, manipulation, and embodied agents. We show that\nChatGPT can be effective at solving several of such tasks, while allowing users\nto interact with it primarily via natural language instructions. In addition to\nthese studies, we introduce an open-sourced research tool called PromptCraft,\nwhich contains a platform where researchers can collaboratively upload and vote\non examples of good prompting schemes for robotics applications, as well as a\nsample robotics simulator with ChatGPT integration, making it easier for users\nto get started with using ChatGPT for robotics.", "no": 70}, {"url": "https://arxiv.org/abs/2302.00093", "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "cites": "278", "abstract": "Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.", "no": 71}, {"url": "https://arxiv.org/abs/2302.03494", "title": "A Categorical Archive of ChatGPT Failures", "cites": "278", "abstract": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.", "no": 72}, {"url": "https://arxiv.org/abs/2305.08322", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models", "cites": "276", "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.", "no": 73}, {"url": "https://arxiv.org/abs/2305.14325", "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate", "cites": "276", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.", "no": 74}, {"url": "https://arxiv.org/abs/2302.10724", "title": "ChatGPT: Jack of all trades, master of none", "cites": "275", "abstract": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.", "no": 75}, {"url": "https://arxiv.org/abs/2305.14251", "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long\n  Form Text Generation", "cites": "273", "abstract": "Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI -- and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via `pip install factscore`.", "no": 76}, {"url": "https://arxiv.org/abs/2303.13367", "title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing", "cites": "272", "abstract": "This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.", "no": 77}, {"url": "https://arxiv.org/abs/2302.07842", "title": "Augmented Language Models: a Survey", "cites": "270", "abstract": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.", "no": 78}, {"url": "https://arxiv.org/abs/2305.17926", "title": "Large Language Models are not Fair Evaluators", "cites": "270", "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of\nadopting large language models~(LLMs), e.g., GPT-4, as a referee to score and\ncompare the quality of responses generated by candidate models. We find that\nthe quality ranking of candidate responses can be easily hacked by simply\naltering their order of appearance in the context. This manipulation allows us\nto skew the evaluation result, making one model appear considerably superior to\nthe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries\nwith ChatGPT as an evaluator. To address this issue, we propose a calibration\nframework with three simple yet effective strategies: 1) Multiple Evidence\nCalibration, which requires the evaluator model to generate multiple evaluation\nevidence before assigning ratings; 2) Balanced Position Calibration, which\naggregates results across various orders to determine the final score; 3)\nHuman-in-the-Loop Calibration, which introduces a balanced position diversity\nentropy to measure the difficulty of each example and seeks human assistance\nwhen needed. We also manually annotate the \"win/tie/lose\" outcomes of responses\nfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and\nextensive experiments demonstrate that our approach successfully mitigates\nevaluation bias, resulting in closer alignment with human judgments. We release\nour code and human annotation at \\url{https://github.com/i-Eval/FairEval} to\nfacilitate future research.", "no": 79}, {"url": "https://arxiv.org/abs/2305.20050", "title": "Let's Verify Step by Step", "cites": "269", "abstract": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.", "no": 80}, {"url": "https://arxiv.org/abs/2305.10355", "title": "Evaluating Object Hallucination in Large Vision-Language Models", "cites": "268", "abstract": "Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.", "no": 81}, {"url": "https://arxiv.org/abs/2302.00083", "title": "In-Context Retrieval-Augmented Language Models", "cites": "267", "abstract": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.", "no": 82}, {"url": "https://arxiv.org/abs/2309.00267", "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI\n  Feedback", "cites": "267", "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences. However,\ngathering high-quality human preference labels can be a time-consuming and\nexpensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al.,\noffers a promising alternative that leverages a powerful off-the-shelf LLM to\ngenerate preferences in lieu of human annotators. Across the tasks of\nsummarization, helpful dialogue generation, and harmless dialogue generation,\nRLAIF achieves comparable or superior performance to RLHF, as rated by human\nevaluators. Furthermore, RLAIF demonstrates the ability to outperform a\nsupervised fine-tuned baseline even when the LLM preference labeler is the same\nsize as the policy. In another experiment, directly prompting the LLM for\nreward scores achieves superior performance to the canonical RLAIF setup, where\nLLM preference labels are first distilled into a reward model. Finally, we\nconduct extensive studies on techniques for generating aligned AI preferences.\nOur results suggest that RLAIF can achieve human-level performance, offering a\npotential solution to the scalability limitations of RLHF.", "no": 83}, {"url": "https://arxiv.org/abs/2303.04048", "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study", "cites": "264", "abstract": "Recently, the emergence of ChatGPT has attracted wide attention from the\ncomputational linguistics community. Many prior studies have shown that ChatGPT\nachieves remarkable performance on various NLP tasks in terms of automatic\nevaluation metrics. However, the ability of ChatGPT to serve as an evaluation\nmetric is still underexplored. Considering assessing the quality of natural\nlanguage generation (NLG) models is an arduous task and NLG metrics notoriously\nshow their poor correlation with human judgments, we wonder whether ChatGPT is\na good NLG evaluation metric. In this report, we provide a preliminary\nmeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,\nwe regard ChatGPT as a human evaluator and give task-specific (e.g.,\nsummarization) and aspect-specific (e.g., relevance) instruction to prompt\nChatGPT to evaluate the generated results of NLG models. We conduct experiments\non five NLG meta-evaluation datasets (including summarization, story generation\nand data-to-text tasks). Experimental results show that compared with previous\nautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlation\nwith human judgments in most cases. In addition, we find that the effectiveness\nof the ChatGPT evaluator might be influenced by the creation method of the\nmeta-evaluation datasets. For the meta-evaluation datasets which are created\ngreatly depending on the reference and thus are biased, the ChatGPT evaluator\nmight lose its effectiveness. We hope our preliminary study could prompt the\nemergence of a general-purposed reliable NLG metric.", "no": 84}, {"url": "https://arxiv.org/abs/2304.06364", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "cites": "263", "abstract": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.", "no": 85}, {"url": "https://arxiv.org/abs/2304.13712", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "cites": "263", "abstract": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.", "no": 86}, {"url": "https://arxiv.org/abs/2301.13867", "title": "Mathematical Capabilities of ChatGPT", "cites": "261", "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT\n(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on\npublicly available datasets, as well as hand-crafted ones, using a novel\nmethodology. In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, either cover\nonly elementary mathematics or are very small. We address this by publicly\nreleasing two new datasets: GHOSTS and miniGHOSTS. These are the first\nnatural-language datasets curated by working researchers in mathematics that\n(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of\nthe mathematical capabilities of language models, and (3) distinguish multiple\ndimensions of mathematical reasoning. These datasets also test whether ChatGPT\nand GPT-4 can be helpful assistants to professional mathematicians by emulating\nuse cases that arise in the daily professional activities of mathematicians. We\nbenchmark the models on a range of fine-grained performance metrics. For\nadvanced mathematics, this is the most detailed evaluation effort to date. We\nfind that ChatGPT can be used most successfully as a mathematical assistant for\nquerying facts, acting as a mathematical search engine and knowledge base\ninterface. GPT-4 can additionally be used for undergraduate-level mathematics\nbut fails on graduate-level difficulty. Contrary to many positive reports in\nthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of\nselection bias), their overall mathematical performance is well below the level\nof a graduate student. Hence, if your goal is to use ChatGPT to pass a\ngraduate-level math exam, you would be better off copying from your average\npeer!", "no": 87}, {"url": "https://arxiv.org/abs/2305.13860", "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study", "cites": "261", "abstract": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.", "no": 88}, {"url": "https://arxiv.org/abs/2305.02301", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less\n  Training Data and Smaller Model Sizes", "cites": "259", "abstract": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .", "no": 89}, {"url": "https://arxiv.org/abs/2302.12813", "title": "Check Your Facts and Try Again: Improving Large Language Models with\n  External Knowledge and Automated Feedback", "cites": "258", "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.", "no": 90}, {"url": "https://arxiv.org/abs/2302.09210", "title": "How Good Are GPT Models at Machine Translation? A Comprehensive\n  Evaluation", "cites": "257", "abstract": "Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.", "no": 91}, {"url": "https://arxiv.org/abs/2306.15595", "title": "Extending Context Window of Large Language Models via Positional\n  Interpolation", "cites": "256", "abstract": "We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.", "no": 92}, {"url": "https://arxiv.org/abs/2301.10226", "title": "A Watermark for Large Language Models", "cites": "254", "abstract": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.", "no": 93}, {"url": "https://arxiv.org/abs/2303.11381", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "cites": "254", "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/", "no": 94}, {"url": "https://arxiv.org/abs/2305.15334", "title": "Gorilla: Large Language Model Connected with Massive APIs", "cites": "254", "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu", "no": 95}, {"url": "https://arxiv.org/abs/2301.13848", "title": "Benchmarking Large Language Models for News Summarization", "cites": "250", "abstract": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.", "no": 96}, {"url": "https://arxiv.org/abs/2308.09687", "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "cites": "249", "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (\"LLM thoughts\") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.", "no": 97}, {"url": "https://arxiv.org/abs/2303.11156", "title": "Can AI-Generated Text be Reliably Detected?", "cites": "241", "abstract": "The unregulated use of LLMs can potentially lead to malicious consequences\nsuch as plagiarism, generating fake news, spamming, etc. Therefore, reliable\ndetection of AI-generated text can be critical to ensure the responsible use of\nLLMs. Recent works attempt to tackle this problem either using certain model\nsignatures present in the generated text outputs or by applying watermarking\ntechniques that imprint specific patterns onto them. In this paper, we show\nthat these detectors are not reliable in practical scenarios. In particular, we\ndevelop a recursive paraphrasing attack to apply on AI text, which can break a\nwhole range of detectors, including the ones using the watermarking schemes as\nwell as neural network-based detectors, zero-shot classifiers, and\nretrieval-based detectors. Our experiments include passages around 300 tokens\nin length, showing the sensitivity of the detectors even in the case of\nrelatively long passages. We also observe that our recursive paraphrasing only\ndegrades text quality slightly, measured via human studies, and metrics such as\nperplexity scores and accuracy on text benchmarks. Additionally, we show that\neven LLMs protected by watermarking schemes can be vulnerable against spoofing\nattacks aimed to mislead detectors to classify human-written text as\nAI-generated, potentially causing reputational damages to the developers. In\nparticular, we show that an adversary can infer hidden AI text signatures of\nthe LLM outputs without having white-box access to the detection method.\nFinally, we provide a theoretical connection between the AUROC of the best\npossible detector and the Total Variation distance between human and AI text\ndistributions that can be used to study the fundamental hardness of the\nreliable detection problem for advanced language models. Our code is publicly\navailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.", "no": 98}, {"url": "https://arxiv.org/abs/2305.06355", "title": "VideoChat: Chat-Centric Video Understanding", "cites": "240", "abstract": "In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything", "no": 99}, {"url": "https://arxiv.org/abs/2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large\n  Language Models", "cites": "240", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.", "no": 100}]