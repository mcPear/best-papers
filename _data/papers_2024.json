[{"url": "https://arxiv.org/abs/2401.04088", "title": "Mixtral of Experts", "cites": "353", "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.", "no": 1}, {"url": "https://arxiv.org/abs/2403.05530", "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "cites": "214", "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.", "no": 2}, {"url": "https://arxiv.org/abs/2403.04652", "title": "Yi: Open Foundation Models by 01.AI", "cites": "171", "abstract": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.", "no": 3}, {"url": "https://arxiv.org/abs/2401.14196", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The\n  Rise of Code Intelligence", "cites": "164", "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.", "no": 4}, {"url": "https://arxiv.org/abs/2407.03993", "title": "A Survey on Natural Language Counterfactual Generation", "cites": "149", "abstract": "Natural Language Counterfactual generation aims to minimally modify a given\ntext such that the modified text will be classified into a different class. The\ngenerated counterfactuals provide insight into the reasoning behind a model's\npredictions by highlighting which words significantly influence the outcomes.\nAdditionally, they can be used to detect model fairness issues or augment the\ntraining data to enhance the model's robustness. A substantial amount of\nresearch has been conducted to generate counterfactuals for various NLP tasks,\nemploying different models and methodologies. With the rapid growth of studies\nin this field, a systematic review is crucial to guide future researchers and\ndevelopers. To bridge this gap, this survey comprehensively overview textual\ncounterfactual generation methods, particularly including those based on Large\nLanguage Models. We propose a new taxonomy that categorizes the generation\nmethods into four groups and systematically summarize the metrics for\nevaluating the generation quality. Finally, we discuss ongoing research\nchallenges and outline promising directions for future work.", "no": 5}, {"url": "https://arxiv.org/abs/2401.02385", "title": "TinyLlama: An Open-Source Small Language Model", "cites": "126", "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention and Lit-GPT), achieving better\ncomputational efficiency. Despite its relatively small size, TinyLlama\ndemonstrates remarkable performance in a series of downstream tasks. It\nsignificantly outperforms existing open-source language models with comparable\nsizes. Our model checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.", "no": 6}, {"url": "https://arxiv.org/abs/2401.10020", "title": "Self-Rewarding Language Models", "cites": "118", "abstract": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes.", "no": 7}, {"url": "https://arxiv.org/abs/2405.18308", "title": "Joint Lemmatization and Morphological Tagging with LEMMING", "cites": "115", "abstract": "We present LEMMING, a modular log-linear model that jointly models\nlemmatization and tagging and supports the integration of arbitrary global\nfeatures. It is trainable on corpora annotated with gold standard tags and\nlemmata and does not rely on morphological dictionaries or analyzers. LEMMING\nsets the new state of the art in token-based statistical lemmatization on six\nlanguages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05\nto 1.58. We also give empirical evidence that jointly modeling morphological\ntags and lemmata is mutually beneficial.", "no": 8}, {"url": "https://arxiv.org/abs/2401.01335", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models", "cites": "109", "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents. Codes\nare available at https://github.com/uclaml/SPIN.", "no": 9}, {"url": "https://arxiv.org/abs/2404.14219", "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone", "cites": "109", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used for phi-2,\ncomposed of heavily filtered publicly available web data and synthetic data.\nThe model is also further aligned for robustness, safety, and chat format. We\nalso provide some initial parameter-scaling results with a 7B and 14B models\ntrained for 4.8T tokens, called phi-3-small and phi-3-medium, both\nsignificantly more capable than phi-3-mini (e.g., respectively 75% and 78% on\nMMLU, and 8.7 and 8.9 on MT-bench). Moreover, we also introduce phi-3-vision, a\n4.2 billion parameter model based on phi-3-mini with strong reasoning\ncapabilities for image and text prompts.", "no": 10}, {"url": "https://arxiv.org/abs/2405.09711", "title": "STAR: A Benchmark for Situated Reasoning in Real-World Videos", "cites": "97", "abstract": "Reasoning in the real world is not divorced from situations. How to capture\nthe present knowledge from surrounding situations and perform reasoning\naccordingly is crucial and challenging for machine intelligence. This paper\nintroduces a new benchmark that evaluates the situated reasoning ability via\nsituation abstraction and logic-grounded question answering for real-world\nvideos, called Situated Reasoning in Real-World Videos (STAR Benchmark). This\nbenchmark is built upon the real-world videos associated with human actions or\ninteractions, which are naturally dynamic, compositional, and logical. The\ndataset includes four types of questions, including interaction, sequence,\nprediction, and feasibility. We represent the situations in real-world videos\nby hyper-graphs connecting extracted atomic entities and relations (e.g.,\nactions, persons, objects, and relationships). Besides visual perception,\nsituated reasoning also requires structured situation comprehension and logical\nreasoning. Questions and answers are procedurally generated. The answering\nlogic of each question is represented by a functional program based on a\nsituation hyper-graph. We compare various existing video reasoning models and\nfind that they all struggle on this challenging situated reasoning task. We\nfurther propose a diagnostic neuro-symbolic model that can disentangle visual\nperception, situation abstraction, language understanding, and functional\nreasoning to understand the challenges of this benchmark.", "no": 11}, {"url": "https://arxiv.org/abs/2401.02954", "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "cites": "96", "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.", "no": 12}, {"url": "https://arxiv.org/abs/2403.08295", "title": "Gemma: Open Models Based on Gemini Research and Technology", "cites": "93", "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.", "no": 13}, {"url": "https://arxiv.org/abs/2407.03563", "title": "Learning Video Temporal Dynamics with Cross-Modal Attention for Robust\n  Audio-Visual Speech Recognition", "cites": "88", "abstract": "Audio-visual speech recognition (AVSR) aims to transcribe human speech using\nboth audio and video modalities. In practical environments with noise-corrupted\naudio, the role of video information becomes crucial. However, prior works have\nprimarily focused on enhancing audio features in AVSR, overlooking the\nimportance of video features. In this study, we strengthen the video features\nby learning three temporal dynamics in video data: context order, playback\ndirection, and the speed of video frames. Cross-modal attention modules are\nintroduced to enrich video features with audio information so that speech\nvariability can be taken into account when training on the video temporal\ndynamics. Based on our approach, we achieve the state-of-the-art performance on\nthe LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach\nexcels in scenarios especially for babble and speech noise, indicating the\nability to distinguish the speech signal that should be recognized from lip\nmovements in the video modality. We support the validity of our methodology by\noffering the ablation experiments for the temporal dynamics losses and the\ncross-modal attention architecture design.", "no": 14}, {"url": "https://arxiv.org/abs/2401.06373", "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to\n  Challenge AI Safety by Humanizing LLMs", "cites": "84", "abstract": "Most traditional AI safety research has approached AI models as machines and\ncentered on algorithm-focused attacks developed by security experts. As large\nlanguage models (LLMs) become increasingly common and competent, non-expert\nusers can also impose risks during daily interactions. This paper introduces a\nnew perspective to jailbreak LLMs as human-like communicators, to explore this\noverlooked intersection between everyday language interaction and AI safety.\nSpecifically, we study how to persuade LLMs to jailbreak them. First, we\npropose a persuasion taxonomy derived from decades of social science research.\nThen, we apply the taxonomy to automatically generate interpretable persuasive\nadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion\nsignificantly increases the jailbreak performance across all risk categories:\nPAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b\nChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused\nattacks. On the defense side, we explore various mechanisms against PAP and,\nfound a significant gap in existing defenses, and advocate for more fundamental\nmitigation for highly interactive LLMs", "no": 15}, {"url": "https://arxiv.org/abs/2402.00838", "title": "OLMo: Accelerating the Science of Language Models", "cites": "82", "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in\ncommercial product offerings. As their commercial importance has surged, the\nmost powerful models have become closed off, gated behind proprietary\ninterfaces, with important details of their training data, architectures, and\ndevelopment undisclosed. Given the importance of these details in\nscientifically studying these models, including their biases and potential\nrisks, we believe it is essential for the research community to have access to\npowerful, truly open LMs. To this end, we have built OLMo, a competitive, truly\nOpen Language Model, to enable the scientific study of language models. Unlike\nmost prior efforts that have only released model weights and inference code, we\nrelease OLMo alongside open training data and training and evaluation code. We\nhope this release will empower the open research community and inspire a new\nwave of innovation.", "no": 16}, {"url": "https://arxiv.org/abs/2401.16420", "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and\n  Comprehension in Vision-Language Large Model", "cites": "77", "abstract": "We introduce InternLM-XComposer2, a cutting-edge vision-language model\nexcelling in free-form text-image composition and comprehension. This model\ngoes beyond conventional vision-language understanding, adeptly crafting\ninterleaved text-image content from diverse inputs like outlines, detailed\ntextual specifications, and reference images, enabling highly customizable\ncontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach\nthat applies additional LoRA parameters exclusively to image tokens to preserve\nthe integrity of pre-trained language knowledge, striking a balance between\nprecise vision understanding and text composition with literary talent.\nExperimental results demonstrate the superiority of InternLM-XComposer2 based\non InternLM2-7B in producing high-quality long-text multi-modal content and its\nexceptional vision-language understanding performance across various\nbenchmarks, where it not only significantly outperforms existing multimodal\nmodels but also matches or even surpasses GPT-4V and Gemini Pro in certain\nassessments. This highlights its remarkable proficiency in the realm of\nmultimodal understanding. The InternLM-XComposer2 model series with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.", "no": 17}, {"url": "https://arxiv.org/abs/2404.01439", "title": "Creating emoji lexica from unsupervised sentiment analysis of their\n  descriptions", "cites": "73", "abstract": "Online media, such as blogs and social networking sites, generate massive\nvolumes of unstructured data of great interest to analyze the opinions and\nsentiments of individuals and organizations. Novel approaches beyond Natural\nLanguage Processing are necessary to quantify these opinions with polarity\nmetrics. So far, the sentiment expressed by emojis has received little\nattention. The use of symbols, however, has boomed in the past four years.\nAbout twenty billion are typed in Twitter nowadays, and new emojis keep\nappearing in each new Unicode version, making them increasingly relevant to\nsentiment analysis tasks. This has motivated us to propose a novel approach to\npredict the sentiments expressed by emojis in online textual messages, such as\ntweets, that does not require human effort to manually annotate data and saves\nvaluable time for other analysis tasks. For this purpose, we automatically\nconstructed a novel emoji sentiment lexicon using an unsupervised sentiment\nanalysis system based on the definitions given by emoji creators in Emojipedia.\nAdditionally, we automatically created lexicon variants by also considering the\nsentiment distribution of the informal texts accompanying emojis. All these\nlexica are evaluated and compared regarding the improvement obtained by\nincluding them in sentiment analysis of the annotated datasets provided by\nKralj Novak et al. (2015). The results confirm the competitiveness of our\napproach.", "no": 18}, {"url": "https://arxiv.org/abs/2402.00159", "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model\n  Pretraining Research", "cites": "71", "abstract": "Information about pretraining corpora used to train the current\nbest-performing language models is seldom discussed: commercial models rarely\ndetail their data, and even open models are often released without accompanying\ntraining data or recipes to reproduce them. As a result, it is challenging to\nconduct and advance scientific research on language modeling, such as\nunderstanding how training data impacts model capabilities and limitations. To\nfacilitate scientific research on language model pretraining, we curate and\nrelease Dolma, a three-trillion-token English corpus, built from a diverse\nmixture of web content, scientific papers, code, public-domain books, social\nmedia, and encyclopedic materials. We extensively document Dolma, including its\ndesign principles, details about its construction, and a summary of its\ncontents. We present analyses and experimental results on intermediate states\nof Dolma to share what we have learned about important data curation practices.\nFinally, we open-source our data curation toolkit to enable reproduction of our\nwork as well as support further research in large-scale data curation.", "no": 19}, {"url": "https://arxiv.org/abs/2401.10774", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding Heads", "cites": "68", "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires\nsequential computation, with each step reliant on the previous one's output.\nThis creates a bottleneck as each step necessitates moving the full model\nparameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods such as speculative decoding have been suggested to address this issue,\ntheir implementation is impeded by the challenges associated with acquiring and\nmaintaining a separate draft model. In this paper, we present Medusa, an\nefficient method that augments LLM inference by adding extra decoding heads to\npredict multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism, Medusa constructs multiple candidate continuations and verifies them\nsimultaneously in each decoding step. By leveraging parallel processing, Medusa\nsubstantially reduces the number of decoding steps required. We present two\nlevels of fine-tuning procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM, enabling better prediction accuracy of Medusa\nheads and higher speedup but needing a special training recipe that preserves\nthe backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.", "no": 20}, {"url": "https://arxiv.org/abs/2402.04249", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal", "cites": "65", "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.", "no": 21}, {"url": "https://arxiv.org/abs/2401.05566", "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety\n  Training", "cites": "64", "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoor behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoor behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.", "no": 22}, {"url": "https://arxiv.org/abs/2403.04132", "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", "cites": "64", "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications;\nhowever, evaluating the alignment with human preferences still poses\nsignificant challenges. To address this issue, we introduce Chatbot Arena, an\nopen platform for evaluating LLMs based on human preferences. Our methodology\nemploys a pairwise comparison approach and leverages input from a diverse user\nbase through crowdsourcing. The platform has been operational for several\nmonths, amassing over 240K votes. This paper describes the platform, analyzes\nthe data we have collected so far, and explains the tried-and-true statistical\nmethods we are using for efficient and accurate evaluation and ranking of\nmodels. We confirm that the crowdsourced questions are sufficiently diverse and\ndiscriminating and that the crowdsourced human votes are in good agreement with\nthose of expert raters. These analyses collectively establish a robust\nfoundation for the credibility of Chatbot Arena. Because of its unique value\nand openness, Chatbot Arena has emerged as one of the most referenced LLM\nleaderboards, widely cited by leading LLM developers and companies. Our demo is\npublicly available at \\url{https://chat.lmsys.org}.", "no": 23}, {"url": "https://arxiv.org/abs/2402.09353", "title": "DoRA: Weight-Decomposed Low-Rank Adaptation", "cites": "63", "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.", "no": 24}, {"url": "https://arxiv.org/abs/2401.05561", "title": "TrustLLM: Trustworthiness in Large Language Models", "cites": "62", "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.", "no": 25}, {"url": "https://arxiv.org/abs/2402.03300", "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\n  Language Models", "cites": "62", "abstract": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.", "no": 26}, {"url": "https://arxiv.org/abs/2401.01614", "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded", "cites": "60", "abstract": "The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.", "no": 27}, {"url": "https://arxiv.org/abs/2402.07827", "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language\n  Model", "cites": "56", "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a\nhandful of data-rich languages. What does it take to broaden access to\nbreakthroughs beyond first-class citizen languages? Our work introduces Aya, a\nmassively multilingual generative language model that follows instructions in\n101 languages of which over 50% are considered as lower-resourced. Aya\noutperforms mT0 and BLOOMZ on the majority of tasks while covering double the\nnumber of languages. We introduce extensive new evaluation suites that broaden\nthe state-of-art for multilingual eval across 99 languages -- including\ndiscriminative and generative tasks, human evaluation, and simulated win rates\nthat cover both held-out tasks and in-distribution performance. Furthermore, we\nconduct detailed investigations on the optimal finetuning mixture composition,\ndata pruning, as well as the toxicity, bias, and safety of our models. We\nopen-source our instruction datasets and our model at\nhttps://hf.co/CohereForAI/aya-101", "no": 28}, {"url": "https://arxiv.org/abs/2403.09611", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", "cites": "56", "abstract": "In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.", "no": 29}, {"url": "https://arxiv.org/abs/2404.04475", "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators", "cites": "55", "abstract": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce complex biases that\nare hard to remove. Even simple, known confounders such as preference for\nlonger outputs remain in existing automated evaluation metrics. We propose a\nsimple regression analysis approach for controlling biases in auto-evaluations.\nAs a real case study, we focus on reducing the length bias of AlpacaEval, a\nfast and affordable benchmark for chat LLMs that uses LLMs to estimate response\nquality. Despite being highly correlated with human preferences, AlpacaEval is\nknown to favor models that generate longer outputs. We introduce a\nlength-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\". To achieve this, we first fit a generalized linear model to predict\nthe biased output of interest (auto-annotator preferences) based on the\nmediators we want to control for (length difference) and other relevant\nfeatures. We then obtain length-controlled preferences by predicting\npreferences while conditioning the GLM with a zero difference in lengths.\nLength-controlling not only improves the robustness of the metric to\nmanipulations in model verbosity, we also find that it increases the Spearman\ncorrelation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release the code\nand leaderboard at https://tatsu-lab.github.io/alpaca_eval/ .", "no": 30}, {"url": "https://arxiv.org/abs/2404.09383", "title": "Low-Resource Named Entity Recognition with Cross-Lingual,\n  Character-Level Neural Conditional Random Fields", "cites": "53", "abstract": "Low-resource named entity recognition is still an open problem in NLP. Most\nstate-of-the-art systems require tens of thousands of annotated sentences in\norder to obtain high performance. However, for most of the world's languages,\nit is unfeasible to obtain such annotation. In this paper, we present a\ntransfer learning scheme, whereby we train character-level neural CRFs to\npredict named entities for both high-resource languages and low resource\nlanguages jointly. Learning character representations for multiple related\nlanguages allows transfer among the languages, improving F1 by up to 9.8 points\nover a loglinear CRF baseline.", "no": 31}, {"url": "https://arxiv.org/abs/2401.01313", "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models", "cites": "52", "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.", "no": 32}, {"url": "https://arxiv.org/abs/2403.18814", "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models", "cites": "52", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.", "no": 33}, {"url": "https://arxiv.org/abs/2402.10373", "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models\n  for Medical Domains", "cites": "51", "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.", "no": 34}, {"url": "https://arxiv.org/abs/2401.06066", "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in\n  Mixture-of-Experts Language Models", "cites": "50", "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.", "no": 35}, {"url": "https://arxiv.org/abs/2402.03927", "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in\n  Closed-Source LLMs", "cites": "50", "abstract": "Natural Language Processing (NLP) research is increasingly focusing on the\nuse of Large Language Models (LLMs), with some of the most popular ones being\neither fully or partially closed-source. The lack of access to model details,\nespecially regarding training data, has repeatedly raised concerns about data\ncontamination among researchers. Several attempts have been made to address\nthis issue, but they are limited to anecdotal evidence and trial and error.\nAdditionally, they overlook the problem of \\emph{indirect} data leaking, where\nmodels are iteratively improved by using data coming from users. In this work,\nwe conduct the first systematic analysis of work using OpenAI's GPT-3.5 and\nGPT-4, the most prominently used LLMs today, in the context of data\ncontamination. By analysing 255 papers and considering OpenAI's data usage\npolicy, we extensively document the amount of data leaked to these models\nduring the first year after the model's release. We report that these models\nhave been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the\nsame time, we document a number of evaluation malpractices emerging in the\nreviewed papers, such as unfair or missing baseline comparisons and\nreproducibility issues. We release our results as a collaborative project on\nhttps://leak-llm.github.io/, where other researchers can contribute to our\nefforts.", "no": 36}, {"url": "https://arxiv.org/abs/2404.08997", "title": "Labeled Morphological Segmentation with Semi-Markov Models", "cites": "47", "abstract": "We present labeled morphological segmentation, an alternative view of\nmorphological processing that unifies several tasks. From an annotation\nstandpoint, we additionally introduce a new hierarchy of morphotactic tagsets.\nFinally, we develop \\modelname, a discriminative morphological segmentation\nsystem that, contrary to previous work, explicitly models morphotactics. We\nshow that \\textsc{chipmunk} yields improved performance on three tasks for all\nsix languages: (i) morphological segmentation, (ii) stemming and (iii)\nmorphological tag classification. On morphological segmentation, our method\nshows absolute improvements of 2--6 points $F_1$ over the baseline.", "no": 37}, {"url": "https://arxiv.org/abs/2402.17764", "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits", "cites": "46", "abstract": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.", "no": 38}, {"url": "https://arxiv.org/abs/2401.08417", "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM\n  Performance in Machine Translation", "cites": "45", "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B\nparameters -- exhibit promising machine translation (MT) performance. However,\neven the top-performing 13B LLM-based translation models, like ALMA, does not\nmatch the performance of state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such as GPT-4. In this study, we bridge\nthis performance gap. We first assess the shortcomings of supervised\nfine-tuning for LLMs in the MT task, emphasizing the quality issues present in\nthe reference data, despite being human-generated. Then, in contrast to SFT\nwhich mimics reference translations, we introduce Contrastive Preference\nOptimization (CPO), a novel approach that trains models to avoid generating\nadequate but not perfect translations. Applying CPO to ALMA models with only\n22K parallel sentences and 12M parameters yields significant improvements. The\nresulting model, called ALMA-R, can match or exceed the performance of the WMT\ncompetition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.", "no": 39}, {"url": "https://arxiv.org/abs/2401.11817", "title": "Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models", "cites": "45", "abstract": "Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all of the computable functions and will\ntherefore always hallucinate. Since the formal world is a part of the real\nworld which is much more complicated, hallucinations are also inevitable for\nreal world LLMs. Furthermore, for real world LLMs constrained by provable time\ncomplexity, we describe the hallucination-prone tasks and empirically validate\nour claims. Finally, using the formal world framework, we discuss the possible\nmechanisms and efficacies of existing hallucination mitigators as well as the\npractical implications on the safe deployment of LLMs.", "no": 40}, {"url": "https://arxiv.org/abs/2404.06395", "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable\n  Training Strategies", "cites": "44", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to\ntrillion parameters has been met with concerns regarding resource efficiency\nand practical expense, particularly given the immense cost of experimentation.\nThis scenario underscores the importance of exploring the potential of Small\nLanguage Models (SLMs) as a resource-efficient alternative. In this context, we\nintroduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter\nvariants, not only excel in their respective categories but also demonstrate\ncapabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach\nexhibits scalability in both model and data dimensions for future LLM research.\nRegarding model scaling, we employ extensive model wind tunnel experiments for\nstable and optimal scaling. For data scaling, we introduce a\nWarmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to\ncontinuous training and domain adaptation. We present an in-depth analysis of\nthe intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we\nare now able to efficiently study data-model scaling law without extensive\nretraining experiments on both axes of model and data, from which we derive the\nmuch higher compute optimal data-model ratio than Chinchilla Optimal.\nAdditionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE\nand MiniCPM-128K, whose excellent performance further cementing MiniCPM's\nfoundation in diverse SLM applications. MiniCPM models are available publicly\nat https://github.com/OpenBMB/MiniCPM .", "no": 41}, {"url": "https://arxiv.org/abs/2402.02057", "title": "Break the Sequential Dependency of LLM Inference Using Lookahead\n  Decoding", "cites": "42", "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth\nbounded, resulting in high latency and significant wastes of the parallel\nprocessing power of modern accelerators. Existing methods for accelerating LLM\ndecoding often require a draft model (e.g., speculative decoding), which is\nnontrivial to obtain and unable to generalize. In this paper, we introduce\nLookahead decoding, an exact, parallel decoding algorithm that accelerates LLM\ndecoding without needing auxiliary models or data stores. It allows trading\nper-step log(FLOPs) to reduce the number of total decoding steps, is more\nparallelizable on single or multiple modern accelerators, and is compatible\nwith concurrent memory-efficient attention (e.g., FlashAttention). Our\nimplementation of Lookahead decoding can speed up autoregressive decoding by up\nto 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code\ncompletion tasks. Our code is avialable at\nhttps://github.com/hao-ai-lab/LookaheadDecoding", "no": 42}, {"url": "https://arxiv.org/abs/2402.10171", "title": "Data Engineering for Scaling Language Models to 128K Context", "cites": "42", "abstract": "We study the continual pretraining recipe for scaling language models'\ncontext lengths to 128K, with a focus on data engineering. We hypothesize that\nlong context modeling, in particular \\textit{the ability to utilize information\nat arbitrary input locations}, is a capability that is mostly already acquired\nthrough large-scale pretraining, and that this capability can be readily\nextended to contexts substantially longer than seen during training~(e.g., 4K\nto 128K) through lightweight continual pretraining on appropriate data mixture.\nWe investigate the \\textit{quantity} and \\textit{quality} of the data for\ncontinual pretraining: (1) for quantity, we show that 500 million to 5 billion\ntokens are enough to enable the model to retrieve information anywhere within\nthe 128K context; (2) for quality, our results equally emphasize \\textit{domain\nbalance} and \\textit{length upsampling}. Concretely, we find that naively\nupsampling longer data on certain domains like books, a common practice of\nexisting work, gives suboptimal performance, and that a balanced domain mixture\nis important. We demonstrate that continual pretraining of the full model on\n1B-5B tokens of such data is an effective and affordable strategy for scaling\nthe context length of language models to 128K. Our recipe outperforms strong\nopen-source long-context models and closes the gap to frontier models like\nGPT-4 128K.", "no": 43}, {"url": "https://arxiv.org/abs/2407.03495", "title": "Codec-ASR: Training Performant Automatic Speech Recognition Systems with\n  Discrete Speech Representations", "cites": "42", "abstract": "Discrete speech representations have garnered recent attention for their\nefficacy in training transformer-based models for various speech-related tasks\nsuch as automatic speech recognition (ASR), translation, speaker verification,\nand joint speech-text foundational models. In this work, we present a\ncomprehensive analysis on building ASR systems with discrete codes. We\ninvestigate different methods for codec training such as quantization schemes\nand time-domain vs spectral feature encodings. We further explore ASR training\ntechniques aimed at enhancing performance, training efficiency, and noise\nrobustness. Drawing upon our findings, we introduce a codec ASR pipeline that\noutperforms Encodec at similar bit-rate. Remarkably, it also surpasses the\nstate-of-the-art results achieved by strong self-supervised models on the 143\nlanguages ML-SUPERB benchmark despite being smaller in size and pretrained on\nsignificantly less data.", "no": 44}, {"url": "https://arxiv.org/abs/2407.03974", "title": "LLM Roleplay: Simulating Human-Chatbot Interaction", "cites": "42", "abstract": "The development of chatbots requires collecting a large number of\nhuman-chatbot dialogues to reflect the breadth of users' sociodemographic\nbackgrounds and conversational goals. However, the resource requirements to\nconduct the respective user studies can be prohibitively high and often only\nallow for a narrow analysis of specific dialogue goals and participant\ndemographics. In this paper, we propose LLM-Roleplay: a goal-oriented,\npersona-based method to automatically generate diverse multi-turn dialogues\nsimulating human-chatbot interaction. LLM-Roleplay can be applied to generate\ndialogues with any type of chatbot and uses large language models (LLMs) to\nplay the role of textually described personas. To validate our method we\ncollect natural human-chatbot dialogues from different sociodemographic groups\nand conduct a human evaluation to compare real human-chatbot dialogues with our\ngenerated dialogues. We compare the abilities of state-of-the-art LLMs in\nembodying personas and holding a conversation and find that our method can\nsimulate human-chatbot dialogues with a high indistinguishability rate.", "no": 45}, {"url": "https://arxiv.org/abs/2401.12187", "title": "WARM: On the Benefits of Weight Averaged Reward Models", "cites": "39", "abstract": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.", "no": 46}, {"url": "https://arxiv.org/abs/2402.19427", "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for\n  Efficient Language Models", "cites": "39", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on\nlong sequences, but they are difficult to train and hard to scale. We propose\nHawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that\nmixes gated linear recurrences with local attention. Hawk exceeds the reported\nperformance of Mamba on downstream tasks, while Griffin matches the performance\nof Llama-2 despite being trained on over 6 times fewer tokens. We also show\nthat Griffin can extrapolate on sequences significantly longer than those seen\nduring training. Our models match the hardware efficiency of Transformers\nduring training, and during inference they have lower latency and significantly\nhigher throughput. We scale Griffin up to 14B parameters, and explain how to\nshard our models for efficient distributed training.", "no": 47}, {"url": "https://arxiv.org/abs/2405.04434", "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language Model", "cites": "39", "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model\ncharacterized by economical training and efficient inference. It comprises 236B\ntotal parameters, of which 21B are activated for each token, and supports a\ncontext length of 128K tokens. DeepSeek-V2 adopts innovative architectures\nincluding Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees\nefficient inference through significantly compressing the Key-Value (KV) cache\ninto a latent vector, while DeepSeekMoE enables training strong models at an\neconomical cost through sparse computation. Compared with DeepSeek 67B,\nDeepSeek-V2 achieves significantly stronger performance, and meanwhile saves\n42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum\ngeneration throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality\nand multi-source corpus consisting of 8.1T tokens, and further perform\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock\nits potential. Evaluation results show that, even with only 21B activated\nparameters, DeepSeek-V2 and its chat versions still achieve top-tier\nperformance among open-source models.", "no": 48}, {"url": "https://arxiv.org/abs/2401.01286", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models", "cites": "37", "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.", "no": 49}, {"url": "https://arxiv.org/abs/2401.13601", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models", "cites": "37", "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone\nsubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or\noutputs via cost-effective training strategies. The resulting models not only\npreserve the inherent reasoning and decision-making capabilities of LLMs but\nalso empower a diverse range of MM tasks. In this paper, we provide a\ncomprehensive survey aimed at facilitating further research of MM-LLMs.\nInitially, we outline general design formulations for model architecture and\ntraining pipeline. Subsequently, we introduce a taxonomy encompassing 126\nMM-LLMs, each characterized by its specific formulations. Furthermore, we\nreview the performance of selected MM-LLMs on mainstream benchmarks and\nsummarize key training recipes to enhance the potency of MM-LLMs. Finally, we\nexplore promising directions for MM-LLMs while concurrently maintaining a\nreal-time tracking website for the latest developments in the field. We hope\nthat this survey contributes to the ongoing advancement of the MM-LLMs domain.", "no": 50}, {"url": "https://arxiv.org/abs/2401.14887", "title": "The Power of Noise: Redefining Retrieval for RAG Systems", "cites": "37", "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to\nextend beyond the pre-trained knowledge of Large Language Models by augmenting\nthe original prompt with relevant passages or documents retrieved by an\nInformation Retrieval (IR) system. RAG has become increasingly important for\nGenerative AI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized in the LLM. We\nargue here that the retrieval component of RAG systems, be it dense or sparse,\ndeserves increased attention from the research community, and accordingly, we\nconduct the first comprehensive and systematic examination of the retrieval\nstrategy of RAG systems. We focus, in particular, on the type of passages IR\nsystems within a RAG solution should retrieve. Our analysis considers multiple\nfactors, such as the relevance of the passages included in the prompt context,\ntheir position, and their number. One counter-intuitive finding of this work is\nthat the retriever's highest-scoring documents that are not directly relevant\nto the query (e.g., do not contain the answer) negatively impact the\neffectiveness of the LLM. Even more surprising, we discovered that adding\nrandom documents in the prompt improves the LLM accuracy by up to 35%. These\nresults highlight the need to investigate the appropriate strategies when\nintegrating retrieval with LLMs, thereby laying the groundwork for future\nresearch in this area.", "no": 51}, {"url": "https://arxiv.org/abs/2402.01622", "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents", "cites": "37", "abstract": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.", "no": 52}, {"url": "https://arxiv.org/abs/2402.06196", "title": "Large Language Models: A Survey", "cites": "37", "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.", "no": 53}, {"url": "https://arxiv.org/abs/2402.08638", "title": "SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 13\n  Languages", "cites": "37", "abstract": "Exploring and quantifying semantic relatedness is central to representing\nlanguage and holds significant implications across various NLP tasks. While\nearlier NLP research primarily focused on semantic similarity, often within the\nEnglish language context, we instead investigate the broader phenomenon of\nsemantic relatedness. In this paper, we present \\textit{SemRel}, a new semantic\nrelatedness dataset collection annotated by native speakers across 13\nlanguages: \\textit{Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi,\nIndonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic,\nSpanish,} and \\textit{Telugu}. These languages originate from five distinct\nlanguage families and are predominantly spoken in Africa and Asia -- regions\ncharacterised by a relatively limited availability of NLP resources. Each\ninstance in the SemRel datasets is a sentence pair associated with a score that\nrepresents the degree of semantic textual relatedness between the two\nsentences. The scores are obtained using a comparative annotation framework. We\ndescribe the data collection and annotation processes, challenges when building\nthe datasets, baseline experiments, and their impact and utility in NLP.", "no": 54}, {"url": "https://arxiv.org/abs/2402.03216", "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation", "cites": "36", "abstract": "In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.", "no": 55}, {"url": "https://arxiv.org/abs/2403.17297", "title": "InternLM2 Technical Report", "cites": "36", "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.", "no": 56}, {"url": "https://arxiv.org/abs/2404.03715", "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with\n  General Preferences", "cites": "36", "abstract": "This paper studies post-training large language models (LLMs) using\npreference feedback from a powerful oracle to help a model iteratively improve\nover itself. The typical approach for post-training LLMs involves Reinforcement\nLearning from Human Feedback (RLHF), which traditionally separates reward\nlearning and subsequent policy optimization. However, such a reward\nmaximization approach is limited by the nature of \"point-wise\" rewards (such as\nBradley-Terry model), which fails to express complex intransitive or cyclic\npreference relations. While advances on RLHF show reward learning and policy\noptimization can be merged into a single contrastive objective for stability,\nthey yet still remain tethered to the reward maximization framework. Recently,\na new wave of research sidesteps the reward maximization presumptions in favor\nof directly optimizing over \"pair-wise\" or general preferences. In this paper,\nwe introduce Direct Nash Optimization (DNO), a provable and scalable algorithm\nthat marries the simplicity and stability of contrastive learning with\ntheoretical generality from optimizing general preferences. Because DNO is a\nbatched on-policy algorithm using a regression-based objective, its\nimplementation is straightforward and efficient. Moreover, DNO enjoys monotonic\nimprovement across iterations that help it improve even over a strong teacher\n(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model\naligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of\n33% on AlpacaEval 2.0 (even after controlling for response length), an absolute\ngain of 26% (7% to 33%) over the initializing model. It outperforms models with\nfar more parameters, including Mistral Large, Self-Rewarding LM (70B\nparameters), and older versions of GPT-4.", "no": 57}, {"url": "https://arxiv.org/abs/2401.06121", "title": "TOFU: A Task of Fictitious Unlearning for LLMs", "cites": "35", "abstract": "Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.", "no": 58}, {"url": "https://arxiv.org/abs/2402.04792", "title": "Direct Language Model Alignment from Online AI Feedback", "cites": "35", "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently\nemerged as efficient alternatives to reinforcement learning from human feedback\n(RLHF), that do not require a separate reward model. However, the preference\ndatasets used in DAP methods are usually collected ahead of training and never\nupdated, thus the feedback is purely offline. Moreover, responses in these\ndatasets are often sampled from a language model distinct from the one being\naligned, and since the model evolves over training, the alignment phase is\ninevitably off-policy. In this study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as\nannotator: on each training iteration, we sample two responses from the current\nmodel and prompt the LLM annotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity, we demonstrate via human\nevaluation in several tasks that OAIF outperforms both offline DAP and RLHF\nmethods. We further show that the feedback leveraged in OAIF is easily\ncontrollable, via instruction prompts to the LLM annotator.", "no": 59}, {"url": "https://arxiv.org/abs/2402.05935", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models", "cites": "35", "abstract": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory", "no": 60}, {"url": "https://arxiv.org/abs/2401.13649", "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web\n  Tasks", "cites": "34", "abstract": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.", "no": 61}, {"url": "https://arxiv.org/abs/2401.17263", "title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks", "cites": "34", "abstract": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo", "no": 62}, {"url": "https://arxiv.org/abs/2402.13228", "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive", "cites": "34", "abstract": "Direct Preference Optimisation (DPO) is effective at significantly improving\nthe performance of large language models (LLMs) on downstream tasks such as\nreasoning, summarisation, and alignment. Using pairs of preferred and\ndispreferred data, DPO models the relative probability of picking one response\nover another. In this work, first we show theoretically that the standard DPO\nloss can lead to a reduction of the model's likelihood of the preferred\nexamples, as long as the relative probability between the preferred and\ndispreferred classes increases. We then show empirically that this phenomenon\noccurs when fine-tuning LLMs on common datasets, especially datasets in which\nthe edit distance between pairs of completions is low. Using these insights, we\ndesign DPO-Positive (DPOP), a new loss function and training procedure which\navoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and\nother fine-tuning procedures across a wide variety of datasets and downstream\ntasks, including datasets with high edit distances between completions.\nFurthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model\n(all else equal) on benchmarks independent of the fine-tuning data, such as\nMT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and\nSmaug-72B, with the latter becoming the first open-source LLM to surpass an\naverage accuracy of 80% on the HuggingFace Open LLM Leaderboard.", "no": 63}, {"url": "https://arxiv.org/abs/2403.19887", "title": "Jamba: A Hybrid Transformer-Mamba Language Model", "cites": "34", "abstract": "We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.", "no": 64}, {"url": "https://arxiv.org/abs/2402.11684", "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language\n  Models", "cites": "33", "abstract": "Large vision-language models (LVLMs) have shown premise in a broad range of\nvision-language tasks with their strong reasoning and generalization\ncapabilities. However, they require considerable computational resources for\ntraining and deployment. This study aims to bridge the performance gap between\ntraditional-scale LVLMs and resource-friendly lite versions by adopting\nhigh-quality training data. To this end, we propose a comprehensive pipeline\nfor generating a synthetic dataset. The key idea is to leverage strong\nproprietary models to generate (i) fine-grained image annotations for\nvision-language alignment and (ii) complex reasoning visual question-answering\npairs for visual instruction fine-tuning, yielding 1.3M samples in total. We\ntrain a series of lite VLMs on the synthetic dataset and experimental results\ndemonstrate the effectiveness of the proposed scheme, where they achieve\ncompetitive performance on 17 benchmarks among 4B LVLMs, and even perform on\npar with 7B/13B-scale models on various benchmarks. This work highlights the\nfeasibility of adopting high-quality data in crafting more efficient LVLMs. We\nname our dataset \\textit{ALLaVA}, and open-source it to research community for\ndeveloping better resource-efficient LVLMs for wider usage.", "no": 65}, {"url": "https://arxiv.org/abs/2401.00812", "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents", "cites": "32", "abstract": "The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.", "no": 66}, {"url": "https://arxiv.org/abs/2401.02330", "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model", "cites": "32", "abstract": "In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.", "no": 67}, {"url": "https://arxiv.org/abs/2402.01680", "title": "Large Language Model based Multi-Agents: A Survey of Progress and\n  Challenges", "cites": "32", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\narray of tasks. Due to the impressive planning and reasoning abilities of LLMs,\nthey have been used as autonomous agents to do many tasks automatically.\nRecently, based on the development of using one LLM as a single planning or\ndecision-making agent, LLM-based multi-agent systems have achieved considerable\nprogress in complex problem-solving and world simulation. To provide the\ncommunity with an overview of this dynamic field, we present this survey to\noffer an in-depth discussion on the essential aspects of multi-agent systems\nbased on LLMs, as well as the challenges. Our goal is for readers to gain\nsubstantial insights on the following questions: What domains and environments\ndo LLM-based multi-agents simulate? How are these agents profiled and how do\nthey communicate? What mechanisms contribute to the growth of agents'\ncapacities? For those interested in delving into this field of study, we also\nsummarize the commonly used datasets or benchmarks for them to have convenient\naccess. To keep researchers updated on the latest studies, we maintain an\nopen-source GitHub repository, dedicated to outlining the research on LLM-based\nmulti-agent systems.", "no": 68}, {"url": "https://arxiv.org/abs/2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "cites": "32", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is\nnow a standard paradigm. When using these LLMs for many downstream\napplications, it is common to additionally bake in new knowledge (e.g.,\ntime-critical news, or private domain knowledge) into the pretrained model\neither through RAG-based-prompting, or fine-tuning. However, the optimal\nmethodology for the model to gain such new knowledge remains an open question.\nIn this paper, we present Retrieval Augmented FineTuning (RAFT), a training\nrecipe that improves the model's ability to answer questions in a \"open-book\"\nin-domain settings. In RAFT, given a question, and a set of retrieved\ndocuments, we train the model to ignore those documents that don't help in\nanswering the question, which we call, distractor documents. RAFT accomplishes\nthis by citing verbatim the right sequence from the relevant document that\nwould help answer the question. This coupled with RAFT's chain-of-thought-style\nresponse helps improve the model's ability to reason. In domain-specific RAG,\nRAFT consistently improves the model's performance across PubMed, HotpotQA, and\nGorilla datasets, presenting a post-training recipe to improve pre-trained LLMs\nto in-domain RAG. RAFT's code and demo are open-sourced at\ngithub.com/ShishirPatil/gorilla.", "no": 69}, {"url": "https://arxiv.org/abs/2402.01364", "title": "Continual Learning for Large Language Models: A Survey", "cites": "31", "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to\nhigh training costs arising from their massive scale. However, updates are\nnecessary to endow LLMs with new skills and keep them up-to-date with rapidly\nevolving human knowledge. This paper surveys recent works on continual learning\nfor LLMs. Due to the unique nature of LLMs, we catalog continue learning\ntechniques in a novel multi-staged categorization scheme, involving continual\npretraining, instruction tuning, and alignment. We contrast continual learning\nfor LLMs with simpler adaptation methods used in smaller models, as well as\nwith other enhancement strategies like retrieval-augmented generation and model\nediting. Moreover, informed by a discussion of benchmarks and evaluation, we\nidentify several challenges and future work directions for this crucial task.", "no": 70}, {"url": "https://arxiv.org/abs/2402.06619", "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction\n  Tuning", "cites": "31", "abstract": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.", "no": 71}, {"url": "https://arxiv.org/abs/2402.07927", "title": "A Systematic Survey of Prompt Engineering in Large Language Models:\n  Techniques and Applications", "cites": "31", "abstract": "Prompt engineering has emerged as an indispensable technique for extending\nthe capabilities of large language models (LLMs) and vision-language models\n(VLMs). This approach leverages task-specific instructions, known as prompts,\nto enhance model efficacy without modifying the core model parameters. Rather\nthan updating the model parameters, prompts allow seamless integration of\npre-trained models into downstream tasks by eliciting desired model behaviors\nsolely based on the given prompt. Prompts can be natural language instructions\nthat provide context to guide the model or learned vector representations that\nactivate relevant knowledge. This burgeoning field has enabled success across\nvarious applications, from question-answering to commonsense reasoning.\nHowever, there remains a lack of systematic organization and understanding of\nthe diverse prompt engineering methods and techniques. This survey paper\naddresses the gap by providing a structured overview of recent advancements in\nprompt engineering, categorized by application area. For each prompting\napproach, we provide a summary detailing the prompting methodology, its\napplications, the models involved, and the datasets utilized. We also delve\ninto the strengths and limitations of each approach and include a taxonomy\ndiagram and table summarizing datasets, models, and critical points of each\nprompting technique. This systematic analysis enables a better understanding of\nthis rapidly developing field and facilitates future research by illuminating\nopen challenges and opportunities for prompt engineering.", "no": 72}, {"url": "https://arxiv.org/abs/2401.12168", "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning\n  Capabilities", "cites": "30", "abstract": "Understanding and reasoning about spatial relationships is a fundamental\ncapability for Visual Question Answering (VQA) and robotics. While Vision\nLanguage Models (VLM) have demonstrated remarkable performance in certain VQA\nbenchmarks, they still lack capabilities in 3D spatial reasoning, such as\nrecognizing quantitative relationships of physical objects like distances or\nsize differences. We hypothesize that VLMs' limited spatial reasoning\ncapability is due to the lack of 3D spatial knowledge in training data and aim\nto solve this problem by training VLMs with Internet-scale spatial reasoning\ndata. To this end, we present a system to facilitate this approach. We first\ndevelop an automatic 3D spatial VQA data generation framework that scales up to\n2 billion VQA examples on 10 million real-world images. We then investigate\nvarious factors in the training recipe, including data quality, training\npipeline, and VLM architecture. Our work features the first internet-scale 3D\nspatial reasoning dataset in metric space. By training a VLM on such data, we\nsignificantly enhance its ability on both qualitative and quantitative spatial\nVQA. Finally, we demonstrate that this VLM unlocks novel downstream\napplications in chain-of-thought spatial reasoning and robotics due to its\nquantitative estimation capability. Project website:\nhttps://spatial-vlm.github.io/", "no": 73}, {"url": "https://arxiv.org/abs/2401.15024", "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "cites": "30", "abstract": "Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression", "no": 74}, {"url": "https://arxiv.org/abs/2403.14624", "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?", "cites": "30", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io", "no": 75}, {"url": "https://arxiv.org/abs/2401.01055", "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer", "cites": "29", "abstract": "In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.", "no": 76}, {"url": "https://arxiv.org/abs/2401.01325", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", "cites": "29", "abstract": "It is well known that LLMs cannot generalize well to long contexts whose\nlengths are larger than the training sequence length. This poses challenges\nwhen employing LLMs for processing long input sequences during inference. In\nthis work, we argue that LLMs themselves have inherent capabilities to handle\nlong contexts without fine-tuning. To achieve this goal, we propose SelfExtend\nto extend the context window of LLMs by constructing bi-level attention\ninformation: the grouped attention and the neighbor attention. The grouped\nattention captures the dependencies among tokens that are far apart, while\nneighbor attention captures dependencies among adjacent tokens within a\nspecified range. The two-level attentions are computed based on the original\nmodel's self-attention mechanism during inference. With minor code\nmodification, our SelfExtend can effortlessly extend existing LLMs' context\nwindow without any fine-tuning. We conduct comprehensive experiments on\nmultiple benchmarks and the results show that our SelfExtend can effectively\nextend existing LLMs' context window length. The code can be found at\n\\url{https://github.com/datamllab/LongLM}.", "no": 77}, {"url": "https://arxiv.org/abs/2401.01967", "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO\n  and Toxicity", "cites": "29", "abstract": "While alignment algorithms are now commonly used to tune pre-trained language\nmodels towards a user's preferences, we lack explanations for the underlying\nmechanisms in which models become ``aligned'', thus making it difficult to\nexplain phenomena like jailbreaks. In this work we study a popular algorithm,\ndirect preference optimization (DPO), and the mechanisms by which it reduces\ntoxicity. Namely, we first study how toxicity is represented and elicited in a\npre-trained language model, GPT2-medium. We then apply DPO with a carefully\ncrafted pairwise dataset to reduce toxicity. We examine how the resulting model\naverts toxic outputs, and find that capabilities learned from pre-training are\nnot removed, but rather bypassed. We use this insight to demonstrate a simple\nmethod to un-align the model, reverting it back to its toxic behavior.", "no": 78}, {"url": "https://arxiv.org/abs/2401.15077", "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty", "cites": "29", "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.", "no": 79}, {"url": "https://arxiv.org/abs/2402.00157", "title": "Large Language Models for Mathematical Reasoning: Progresses and\n  Challenges", "cites": "29", "abstract": "Mathematical reasoning serves as a cornerstone for assessing the fundamental\ncognitive capabilities of human intelligence. In recent times, there has been a\nnotable surge in the development of Large Language Models (LLMs) geared towards\nthe automated resolution of mathematical problems. However, the landscape of\nmathematical problem types is vast and varied, with LLM-oriented techniques\nundergoing evaluation across diverse datasets and settings. This diversity\nmakes it challenging to discern the true advancements and obstacles within this\nburgeoning field. This survey endeavors to address four pivotal dimensions: i)\na comprehensive exploration of the various mathematical problems and their\ncorresponding datasets that have been investigated; ii) an examination of the\nspectrum of LLM-oriented techniques that have been proposed for mathematical\nproblem-solving; iii) an overview of factors and concerns affecting LLMs in\nsolving math; and iv) an elucidation of the persisting challenges within this\ndomain. To the best of our knowledge, this survey stands as one of the first\nextensive examinations of the landscape of LLMs in the realm of mathematics,\nproviding a holistic perspective on the current state, accomplishments, and\nfuture challenges in this rapidly evolving field.", "no": 80}, {"url": "https://arxiv.org/abs/2402.04333", "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning", "cites": "29", "abstract": "Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.", "no": 81}, {"url": "https://arxiv.org/abs/2402.05162", "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications", "cites": "29", "abstract": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.", "no": 82}, {"url": "https://arxiv.org/abs/2404.04963", "title": "SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for\n  Clinical Trials", "cites": "29", "abstract": "Large Language Models (LLMs) are at the forefront of NLP achievements but\nfall short in dealing with shortcut learning, factual inconsistency, and\nvulnerability to adversarial inputs.These shortcomings are especially critical\nin medical contexts, where they can misrepresent actual model capabilities.\nAddressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural\nLanguage Inference for ClinicalTrials. Our contributions include the refined\nNLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials -\nPerturbed), designed to challenge LLMs with interventional and causal reasoning\ntasks, along with a comprehensive evaluation of methods and results for\nparticipant submissions. A total of 106 participants registered for the task\ncontributing to over 1200 individual submissions and 25 system overview papers.\nThis initiative aims to advance the robustness and applicability of NLI models\nin healthcare, ensuring safer and more dependable AI assistance in clinical\ndecision-making. We anticipate that the dataset, models, and outcomes of this\ntask can support future research in the field of biomedical NLI. The dataset,\ncompetition leaderboard, and website are publicly available.", "no": 83}, {"url": "https://arxiv.org/abs/2404.06512", "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model\n  Handling Resolutions from 336 Pixels to 4K HD", "cites": "29", "abstract": "The Large Vision-Language Model (LVLM) field has seen significant\nadvancements, yet its progression has been hindered by challenges in\ncomprehending fine-grained visual content due to limited resolution. Recent\nefforts have aimed to enhance the high-resolution understanding capabilities of\nLVLMs, yet they remain capped at approximately 1500 x 1500 pixels and\nconstrained to a relatively narrow resolution range. This paper represents\nInternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM\nresolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently,\nconsidering the ultra-high resolution may not be necessary in all scenarios, it\nsupports a wide range of diverse resolutions from 336 pixels to 4K standard,\nsignificantly broadening its scope of applicability. Specifically, this\nresearch advances the patch division paradigm by introducing a novel extension:\ndynamic resolution with automatic patch configuration. It maintains the\ntraining image aspect ratios while automatically varying patch counts and\nconfiguring layouts based on a pre-trained Vision Transformer (ViT) (336 x\n336), leading to dynamic training resolution from 336 pixels to 4K standard.\nOur research demonstrates that scaling training resolution up to 4K HD leads to\nconsistent performance enhancements without hitting the ceiling of potential\nimprovements. InternLM-XComposer2-4KHD shows superb capability that matches or\neven surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. The\nInternLM-XComposer2-4KHD model series with 7B parameters are publicly available\nat https://github.com/InternLM/InternLM-XComposer.", "no": 84}, {"url": "https://arxiv.org/abs/2401.04398", "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding", "cites": "28", "abstract": "Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.", "no": 85}, {"url": "https://arxiv.org/abs/2402.13753", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens", "cites": "28", "abstract": "Large context window is a desirable feature in large language models (LLMs).\nHowever, due to high fine-tuning costs, scarcity of long texts, and\ncatastrophic values introduced by new token positions, current extended context\nwindows are limited to around 128k tokens. This paper introduces LongRoPE that,\nfor the first time, extends the context window of pre-trained LLMs to an\nimpressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k\ntraining lengths, while maintaining performance at the original short context\nwindow. This is achieved by three key innovations: (i) we identify and exploit\ntwo forms of non-uniformities in positional interpolation through an efficient\nsearch, providing a better initialization for fine-tuning and enabling an 8x\nextension in non-fine-tuning scenarios; (ii) we introduce a progressive\nextension strategy that first fine-tunes a 256k length LLM and then conducts a\nsecond positional interpolation on the fine-tuned extended LLM to achieve a\n2048k context window; (iii) we readjust LongRoPE on 8k length to recover the\nshort context window performance. Extensive experiments on LLaMA2 and Mistral\nacross various tasks demonstrate the effectiveness of our method. Models\nextended via LongRoPE retain the original architecture with minor modifications\nto the positional embedding, and can reuse most pre-existing optimizations.", "no": 86}, {"url": "https://arxiv.org/abs/2403.13372", "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models", "cites": "28", "abstract": "Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and\n3,000 forks.", "no": 87}, {"url": "https://arxiv.org/abs/2404.07143", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with\n  Infini-attention", "cites": "28", "abstract": "This work introduces an efficient method to scale Transformer-based Large\nLanguage Models (LLMs) to infinitely long inputs with bounded memory and\ncomputation. A key component in our proposed approach is a new attention\ntechnique dubbed Infini-attention. The Infini-attention incorporates a\ncompressive memory into the vanilla attention mechanism and builds in both\nmasked local attention and long-term linear attention mechanisms in a single\nTransformer block. We demonstrate the effectiveness of our approach on\nlong-context language modeling benchmarks, 1M sequence length passkey context\nblock retrieval and 500K length book summarization tasks with 1B and 8B LLMs.\nOur approach introduces minimal bounded memory parameters and enables fast\nstreaming inference for LLMs.", "no": 88}, {"url": "https://arxiv.org/abs/2404.16068", "title": "SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", "cites": "28", "abstract": "While vertical thinking relies on logical and commonsense reasoning, lateral\nthinking requires systems to defy commonsense associations and overwrite them\nthrough unconventional thinking. Lateral thinking has been shown to be\nchallenging for current models but has received little attention. A recent\nbenchmark, BRAINTEASER, aims to evaluate current models' lateral thinking\nability in a zero-shot setting. In this paper, we split the original benchmark\nto also support fine-tuning setting and present SemEval Task 9:\nBRAIN-TEASER(S), the first task at this competition designed to test the\nsystem's reasoning and lateral thinking ability. As a popular task,\nBRAINTEASER(S)'s two subtasks receive 483 team submissions from 182\nparticipants during the competition. This paper provides a fine-grained system\nanalysis of the competition results, together with a reflection on what this\nmeans for the ability of the systems to reason laterally. We hope that the\nBRAINTEASER(S) subtasks and findings in this paper can stimulate future work on\nlateral thinking and robust reasoning by computational models.", "no": 89}, {"url": "https://arxiv.org/abs/2407.04047", "title": "Improving Accented Speech Recognition using Data Augmentation based on\n  Unsupervised Text-to-Speech Synthesis", "cites": "28", "abstract": "This paper investigates the use of unsupervised text-to-speech synthesis\n(TTS) as a data augmentation method to improve accented speech recognition. TTS\nsystems are trained with a small amount of accented speech training data and\ntheir pseudo-labels rather than manual transcriptions, and hence unsupervised.\nThis approach enables the use of accented speech data without manual\ntranscriptions to perform data augmentation for accented speech recognition.\nSynthetic accented speech data, generated from text prompts by using the TTS\nsystems, are then combined with available non-accented speech data to train\nautomatic speech recognition (ASR) systems. ASR experiments are performed in a\nself-supervised learning framework using a Wav2vec2.0 model which was\npre-trained on large amount of unsupervised accented speech data. The accented\nspeech data for training the unsupervised TTS are read speech, selected from\nL2-ARCTIC and British Isles corpora, while spontaneous conversational speech\nfrom the Edinburgh international accents of English corpus are used as the\nevaluation data. Experimental results show that Wav2vec2.0 models which are\nfine-tuned to downstream ASR task with synthetic accented speech data,\ngenerated by the unsupervised TTS, yield up to 6.1% relative word error rate\nreductions compared to a Wav2vec2.0 baseline which is fine-tuned with the\nnon-accented speech data from Librispeech corpus.", "no": 90}, {"url": "https://arxiv.org/abs/2401.05654", "title": "Towards Conversational Diagnostic AI", "cites": "27", "abstract": "At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.", "no": 91}, {"url": "https://arxiv.org/abs/2401.10529", "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model\n  Reasoning over Image Sequences", "cites": "27", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.", "no": 92}, {"url": "https://arxiv.org/abs/2401.13919", "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal\n  Models", "cites": "27", "abstract": "The rapid advancement of large language models (LLMs) has led to a new era\nmarked by the development of autonomous applications in real-world scenarios,\nwhich drives innovation in creating advanced web agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe establish a new benchmark by compiling real-world tasks from 15 popular\nwebsites and introduce an automatic evaluation protocol leveraging multimodal\nunderstanding abilities of GPT-4V to evaluate open-ended web agents. We show\nthat WebVoyager achieves a 59.1% task success rate on our benchmark,\nsignificantly surpassing the performance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring the exceptional capability of\nWebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement\nwith human judgment, indicating its effectiveness in providing reliable and\naccurate assessments of web agents.", "no": 93}, {"url": "https://arxiv.org/abs/2402.12354", "title": "LoRA+: Efficient Low Rank Adaptation of Large Models", "cites": "27", "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.", "no": 94}, {"url": "https://arxiv.org/abs/2402.14289", "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models", "cites": "27", "abstract": "We present the TinyLLaVA framework that provides a unified perspective in\ndesigning and analyzing the small-scale Large Multimodal Models (LMMs). We\nempirically study the effects of different vision encoders, connection modules,\nlanguage models, training data and training recipes. Our extensive experiments\nshowed that better quality of data combined with better training recipes,\nsmaller LMMs can consistently achieve on-par performances compared to bigger\nLMMs. Under our framework, we train a family of small-scale LMMs. Our best\nmodel, TinyLLaVA-3.1B, achieves better overall performance against existing 7B\nmodels such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as\nbaselines for future research in terms of data scaling, training setups and\nmodel selections. Our model weights and codes will be made public.", "no": 95}, {"url": "https://arxiv.org/abs/2403.07691", "title": "ORPO: Monolithic Preference Optimization without Reference Model", "cites": "27", "abstract": "While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).", "no": 96}, {"url": "https://arxiv.org/abs/2404.10719", "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study", "cites": "27", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions.", "no": 97}, {"url": "https://arxiv.org/abs/2401.03462", "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon", "cites": "26", "abstract": "The utilization of long contexts poses a big challenge for LLMs due to their\nlimited context window size. Although the context window can be extended\nthrough fine-tuning, it will result in a considerable cost at both training and\ninference time, and exert an unfavorable impact to the LLM's original\ncapabilities. In this work, we propose a new method called Activation Beacon,\nwhich condenses LLM's raw activations into compact forms such that the LLM can\nperceive a longer context with a limited context window. Activation Beacon is\nintroduced as a plug-in module, which fully preserves the LLM's original\ncapability in short contexts. It works with the sliding window to streamingly\nprocess the long context, which leads to a competitive memory and time\nefficiency in both training and inference. Activation Beacon is trained with\nshort-sequence data of diversified condensing ratios. Thanks to such a\ntreatment, it can be effectively learned to support different context lengths\nwith a small training cost. Our experiment verifies Activation Beacon's\neffectiveness of context extension: it can remarkably accomplish high-quality\nextension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K);\nmeanwhile, it can also achieve superior performances across a variety of\nlong-context language modeling and understanding tasks. The source code and\nmodel checkpoint are available at\n\\url{https://github.com/FlagOpen/FlagEmbedding}.", "no": 98}, {"url": "https://arxiv.org/abs/2401.07851", "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive\n  Survey of Speculative Decoding", "cites": "26", "abstract": "To mitigate the high inference latency stemming from autoregressive decoding\nin Large Language Models (LLMs), Speculative Decoding has emerged as a novel\ndecoding paradigm for LLM inference. In each decoding step, this method first\ndrafts several future tokens efficiently and then verifies them in parallel.\nUnlike autoregressive decoding, Speculative Decoding facilitates the\nsimultaneous decoding of multiple tokens per step, thereby accelerating\ninference. This paper presents a comprehensive overview and analysis of this\npromising decoding paradigm. We begin by providing a formal definition and\nformulation of Speculative Decoding. Then, we organize in-depth discussions on\nits key facets, such as drafter selection and verification strategies.\nFurthermore, we present a comparative analysis of leading methods under\nthird-party testing environments. We aim for this work to serve as a catalyst\nfor further research on Speculative Decoding, ultimately contributing to more\nefficient LLM inference.", "no": 99}, {"url": "https://arxiv.org/abs/2402.00253", "title": "A Survey on Hallucination in Large Vision-Language Models", "cites": "26", "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.", "no": 100}]