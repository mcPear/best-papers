[{"url": "https://arxiv.org/abs/1706.03762", "title": "Attention Is All You Need", "cites": "112 066", "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "no": 1}, {"url": "https://arxiv.org/abs/1704.05426", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through\n  Inference", "cites": "4 181", "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)\ncorpus, a dataset designed for use in the development and evaluation of machine\nlearning models for sentence understanding. In addition to being one of the\nlargest corpora available for the task of NLI, at 433k examples, this corpus\nimproves upon available resources in its coverage: it offers data from ten\ndistinct genres of written and spoken English--making it possible to evaluate\nsystems on nearly the full complexity of the language--and it offers an\nexplicit setting for the evaluation of cross-genre domain adaptation.", "no": 2}, {"url": "https://arxiv.org/abs/1704.04368", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "cites": "3 852", "abstract": "Neural sequence-to-sequence models have provided a viable new approach for\nabstractive text summarization (meaning they are not restricted to simply\nselecting and rearranging passages from the original text). However, these\nmodels have two shortcomings: they are liable to reproduce factual details\ninaccurately, and they tend to repeat themselves. In this work we propose a\nnovel architecture that augments the standard sequence-to-sequence attentional\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\nthat can copy words from the source text via pointing, which aids accurate\nreproduction of information, while retaining the ability to produce novel words\nthrough the generator. Second, we use coverage to keep track of what has been\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\nMail summarization task, outperforming the current abstractive state-of-the-art\nby at least 2 ROUGE points.", "no": 3}, {"url": "https://arxiv.org/abs/1705.03122", "title": "Convolutional Sequence to Sequence Learning", "cites": "3 158", "abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT'14 English-German and WMT'14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.", "no": 4}, {"url": "https://arxiv.org/abs/1708.02709", "title": "Recent Trends in Deep Learning Based Natural Language Processing", "cites": "2 739", "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical\nrepresentations of data and have produced state-of-the-art results in many\ndomains. Recently, a variety of model designs and methods have blossomed in the\ncontext of natural language processing (NLP). In this paper, we review\nsignificant deep learning related models and methods that have been employed\nfor numerous NLP tasks and provide a walk-through of their evolution. We also\nsummarize, compare and contrast the various models and put forward a detailed\nunderstanding of the past, present and future of deep learning in NLP.", "no": 5}, {"url": "https://arxiv.org/abs/1712.05884", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram\n  Predictions", "cites": "2 521", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech\nsynthesis directly from text. The system is composed of a recurrent\nsequence-to-sequence feature prediction network that maps character embeddings\nto mel-scale spectrograms, followed by a modified WaveNet model acting as a\nvocoder to synthesize timedomain waveforms from those spectrograms. Our model\nachieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for\nprofessionally recorded speech. To validate our design choices, we present\nablation studies of key components of our system and evaluate the impact of\nusing mel spectrograms as the input to WaveNet instead of linguistic, duration,\nand $F_0$ features. We further demonstrate that using a compact acoustic\nintermediate representation enables significant simplification of the WaveNet\narchitecture.", "no": 6}, {"url": "https://arxiv.org/abs/1703.04009", "title": "Automated Hate Speech Detection and the Problem of Offensive Language", "cites": "2 452", "abstract": "A key challenge for automatic hate-speech detection on social media is the\nseparation of hate speech from other instances of offensive language. Lexical\ndetection methods tend to have low precision because they classify all messages\ncontaining particular terms as hate speech and previous work using supervised\nlearning has failed to distinguish between the two categories. We used a\ncrowd-sourced hate speech lexicon to collect tweets containing hate speech\nkeywords. We use crowd-sourcing to label a sample of these tweets into three\ncategories: those containing hate speech, only offensive language, and those\nwith neither. We train a multi-class classifier to distinguish between these\ndifferent categories. Close analysis of the predictions and the errors shows\nwhen we can reliably separate hate speech from other offensive language and\nwhen this differentiation is more difficult. We find that racist and homophobic\ntweets are more likely to be classified as hate speech but that sexist tweets\nare generally classified as offensive. Tweets without explicit hate keywords\nare also more difficult to classify.", "no": 7}, {"url": "https://arxiv.org/abs/1703.04247", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "cites": "2 451", "abstract": "Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods seem to have a strong bias towards low- or high-order interactions, or\nrequire expertise feature engineering. In this paper, we show that it is\npossible to derive an end-to-end learning model that emphasizes both low- and\nhigh-order feature interactions. The proposed model, DeepFM, combines the power\nof factorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide \\&\nDeep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\"\nparts, with no need of feature engineering besides raw features. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and efficiency of\nDeepFM over the existing models for CTR prediction, on both benchmark data and\ncommercial data.", "no": 8}, {"url": "https://arxiv.org/abs/1705.02315", "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "cites": "2 217", "abstract": "The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC", "no": 9}, {"url": "https://arxiv.org/abs/1705.03551", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for\n  Reading Comprehension", "cites": "2 126", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing\nover 650K question-answer-evidence triples. TriviaQA includes 95K\nquestion-answer pairs authored by trivia enthusiasts and independently gathered\nevidence documents, six per question on average, that provide high quality\ndistant supervision for answering the questions. We show that, in comparison to\nother recently introduced large-scale datasets, TriviaQA (1) has relatively\ncomplex, compositional questions, (2) has considerable syntactic and lexical\nvariability between questions and corresponding answer-evidence sentences, and\n(3) requires more cross sentence reasoning to find answers. We also present two\nbaseline algorithms: a feature-based classifier and a state-of-the-art neural\nnetwork, that performs well on SQuAD reading comprehension. Neither approach\ncomes close to human performance (23% and 40% vs. 80%), suggesting that\nTriviaQA is a challenging testbed that is worth significant future study. Data\nand code available at -- http://nlp.cs.washington.edu/triviaqa/", "no": 10}, {"url": "https://arxiv.org/abs/1701.06538", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "cites": "2 110", "abstract": "The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.", "no": 11}, {"url": "https://arxiv.org/abs/1703.03130", "title": "A Structured Self-attentive Sentence Embedding", "cites": "2 065", "abstract": "This paper proposes a new model for extracting an interpretable sentence\nembedding by introducing self-attention. Instead of using a vector, we use a\n2-D matrix to represent the embedding, with each row of the matrix attending on\na different part of the sentence. We also propose a self-attention mechanism\nand a special regularization term for the model. As a side effect, the\nembedding comes with an easy way of visualizing what specific parts of the\nsentence are encoded into the embedding. We evaluate our model on 3 different\ntasks: author profiling, sentiment classification, and textual entailment.\nResults show that our model yields a significant performance gain compared to\nother sentence embedding methods in all of the 3 tasks.", "no": 12}, {"url": "https://arxiv.org/abs/1705.02364", "title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data", "cites": "2 050", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted. In this paper, we show how universal sentence representations trained\nusing the supervised data of the Stanford Natural Language Inference datasets\ncan consistently outperform unsupervised methods like SkipThought vectors on a\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\nobtain features, which can then be transferred to other tasks, our work tends\nto indicate the suitability of natural language inference for transfer learning\nto other NLP tasks. Our encoder is publicly available.", "no": 13}, {"url": "https://arxiv.org/abs/1709.07871", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "cites": "1 913", "abstract": "We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.", "no": 14}, {"url": "https://arxiv.org/abs/1704.00051", "title": "Reading Wikipedia to Answer Open-Domain Questions", "cites": "1 904", "abstract": "This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.", "no": 15}, {"url": "https://arxiv.org/abs/1701.02810", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "cites": "1 882", "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques.", "no": 16}, {"url": "https://arxiv.org/abs/1709.03815", "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation", "cites": "1 882", "abstract": "We introduce an open-source toolkit for neural machine translation (NMT) to\nsupport research into model architectures, feature representations, and source\nmodalities, while maintaining competitive performance, modularity and\nreasonable training requirements.", "no": 17}, {"url": "https://arxiv.org/abs/1708.00055", "title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and\n  Cross-lingual Focused Evaluation", "cites": "1 774", "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of\nsentences. Applications include machine translation (MT), summarization,\ngeneration, question answering (QA), short answer grading, semantic search,\ndialog and conversational systems. The STS shared task is a venue for assessing\nthe current state-of-the-art. The 2017 task focuses on multilingual and\ncross-lingual pairs with one sub-track exploring MT quality estimation (MTQE)\ndata. The task obtained strong participation from 31 teams, with 17\nparticipating in all language tracks. We summarize performance and review a\nselection of well performing methods. Analysis highlights common errors,\nproviding insight into the limitations of existing models. To support ongoing\nwork on semantic representations, the STS Benchmark is introduced as a new\nshared training and evaluation set carefully selected from the corpus of\nEnglish STS shared task data (2012-2017).", "no": 18}, {"url": "https://arxiv.org/abs/1703.10135", "title": "Tacotron: Towards End-to-End Speech Synthesis", "cites": "1 725", "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given <text, audio> pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it's\nsubstantially faster than sample-level autoregressive methods.", "no": 19}, {"url": "https://arxiv.org/abs/1710.04087", "title": "Word Translation Without Parallel Data", "cites": "1 590", "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have\nrelied on bilingual dictionaries or parallel corpora. Recent studies showed\nthat the need for parallel data supervision can be alleviated with\ncharacter-level information. While these methods showed encouraging results,\nthey are not on par with their supervised counterparts and are limited to pairs\nof languages sharing a common alphabet. In this work, we show that we can build\na bilingual dictionary between two languages without using any parallel\ncorpora, by aligning monolingual word embedding spaces in an unsupervised way.\nWithout using any character information, our model even outperforms existing\nsupervised methods on cross-lingual tasks for some language pairs. Our\nexperiments demonstrate that our method works very well also for distant\nlanguage pairs, like English-Russian or English-Chinese. We finally describe\nexperiments on the English-Esperanto low-resource language pair, on which there\nonly exists a limited amount of parallel data, to show the potential impact of\nour method in fully unsupervised machine translation. Our code, embeddings and\ndictionaries are publicly available.", "no": 20}, {"url": "https://arxiv.org/abs/1706.01427", "title": "A simple neural network module for relational reasoning", "cites": "1 569", "abstract": "Relational reasoning is a central component of generally intelligent\nbehavior, but has proven difficult for neural networks to learn. In this paper\nwe describe how to use Relation Networks (RNs) as a simple plug-and-play module\nto solve problems that fundamentally hinge on relational reasoning. We tested\nRN-augmented networks on three tasks: visual question answering using a\nchallenging dataset called CLEVR, on which we achieve state-of-the-art,\nsuper-human performance; text-based question answering using the bAbI suite of\ntasks; and complex reasoning about dynamic physical systems. Then, using a\ncurated dataset called Sort-of-CLEVR we show that powerful convolutional\nnetworks do not have a general capacity to solve relational questions, but can\ngain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to\nreason about entities and their relations.", "no": 21}, {"url": "https://arxiv.org/abs/1707.07328", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "cites": "1 537", "abstract": "Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely.", "no": 22}, {"url": "https://arxiv.org/abs/1705.04304", "title": "A Deep Reinforced Model for Abstractive Summarization", "cites": "1 504", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel\nintra-attention that attends over the input and continuously generated output\nseparately, and a new training method that combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with supervised\nlearning often exhibit \"exposure bias\" - they assume ground truth is provided\nat each step during training. However, when standard word prediction is\ncombined with the global sequence prediction training of RL the resulting\nsummaries become more readable. We evaluate this model on the CNN/Daily Mail\nand New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the\nCNN/Daily Mail dataset, an improvement over previous state-of-the-art models.\nHuman evaluation also shows that our model produces higher quality summaries.", "no": 23}, {"url": "https://arxiv.org/abs/1708.07524", "title": "Supervised Speech Separation Based on Deep Learning: An Overview", "cites": "1 284", "abstract": "Speech separation is the task of separating target speech from background\ninterference. Traditionally, speech separation is studied as a signal\nprocessing problem. A more recent approach formulates speech separation as a\nsupervised learning problem, where the discriminative patterns of speech,\nspeakers, and background noise are learned from training data. Over the past\ndecade, many supervised separation algorithms have been put forward. In\nparticular, the recent introduction of deep learning to supervised speech\nseparation has dramatically accelerated progress and boosted separation\nperformance. This article provides a comprehensive overview of the research on\ndeep learning based supervised speech separation in the last several years. We\nfirst introduce the background of speech separation and the formulation of\nsupervised separation. Then we discuss three main components of supervised\nseparation: learning machines, training targets, and acoustic features. Much of\nthe overview is on separation algorithms where we review monaural methods,\nincluding speech enhancement (speech-nonspeech separation), speaker separation\n(multi-talker separation), and speech dereverberation, as well as\nmulti-microphone techniques. The important issue of generalization, unique to\nsupervised learning, is discussed. This overview provides a historical\nperspective on how advances are made. In addition, we discuss a number of\nconceptual issues, including what constitutes the target source.", "no": 24}, {"url": "https://arxiv.org/abs/1705.00648", "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News\n  Detection", "cites": "1 259", "abstract": "Automatic fake news detection is a challenging problem in deception\ndetection, and it has tremendous real-world political and social impacts.\nHowever, statistical approaches to combating fake news has been dramatically\nlimited by the lack of labeled benchmark datasets. In this paper, we present\nliar: a new, publicly available dataset for fake news detection. We collected a\ndecade-long, 12.8K manually labeled short statements in various contexts from\nPolitiFact.com, which provides detailed analysis report and links to source\ndocuments for each case. This dataset can be used for fact-checking research as\nwell. Notably, this new dataset is an order of magnitude larger than previously\nlargest public fake news datasets of similar type. Empirically, we investigate\nautomatic fake news detection based on surface-level linguistic patterns. We\nhave designed a novel, hybrid convolutional neural network to integrate\nmeta-data with text. We show that this hybrid approach can improve a text-only\ndeep learning model.", "no": 25}, {"url": "https://arxiv.org/abs/1704.04683", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "cites": "1 222", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students' ability in understanding and reasoning.\nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines.", "no": 26}, {"url": "https://arxiv.org/abs/1712.09405", "title": "Advances in Pre-Training Distributed Word Representations", "cites": "1 202", "abstract": "Many Natural Language Processing applications nowadays rely on pre-trained\nword representations estimated from large text corpora such as news\ncollections, Wikipedia and Web Crawl. In this paper, we show how to train\nhigh-quality word vector representations by using a combination of known tricks\nthat are however rarely used together. The main result of our work is the new\nset of publicly available pre-trained models that outperform the current state\nof the art by a large margin on a number of tasks.", "no": 27}, {"url": "https://arxiv.org/abs/1710.03957", "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset", "cites": "1 196", "abstract": "We develop a high-quality multi-turn dialog dataset, DailyDialog, which is\nintriguing in several aspects. The language is human-written and less noisy.\nThe dialogues in the dataset reflect our daily communication way and cover\nvarious topics about our daily life. We also manually label the developed\ndataset with communication intention and emotion information. Then, we evaluate\nexisting approaches on DailyDialog dataset and hope it benefit the research\nfield of dialog systems.", "no": 28}, {"url": "https://arxiv.org/abs/1711.07280", "title": "Vision-and-Language Navigation: Interpreting visually-grounded\n  navigation instructions in real environments", "cites": "1 185", "abstract": "A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator -- a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.", "no": 29}, {"url": "https://arxiv.org/abs/1706.03872", "title": "Six Challenges for Neural Machine Translation", "cites": "1 170", "abstract": "We explore six challenges for neural machine translation: domain mismatch,\namount of training data, rare words, long sentences, word alignment, and beam\nsearch. We show both deficiencies and improvements over the quality of\nphrase-based statistical machine translation.", "no": 30}, {"url": "https://arxiv.org/abs/1712.01769", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models", "cites": "1 127", "abstract": "Attention-based encoder-decoder architectures such as Listen, Attend, and\nSpell (LAS), subsume the acoustic, pronunciation and language model components\nof a traditional automatic speech recognition (ASR) system into a single neural\nnetwork. In previous work, we have shown that such architectures are comparable\nto state-of-theart ASR systems on dictation tasks, but it was not clear if such\narchitectures would be practical for more challenging tasks such as voice\nsearch. In this work, we explore a variety of structural and optimization\nimprovements to our LAS model which significantly improve performance. On the\nstructural side, we show that word piece models can be used instead of\ngraphemes. We also introduce a multi-head attention architecture, which offers\nimprovements over the commonly-used single-head attention. On the optimization\nside, we explore synchronous training, scheduled sampling, label smoothing, and\nminimum word error rate optimization, which are all shown to improve accuracy.\nWe present results with a unidirectional LSTM encoder for streaming\nrecognition. On a 12, 500 hour voice search task, we find that the proposed\nchanges improve the WER from 9.2% to 5.6%, while the best conventional system\nachieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to\n5% for the conventional system.", "no": 31}, {"url": "https://arxiv.org/abs/1707.07250", "title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "cites": "1 084", "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which\nextends the conventional language-based definition of sentiment analysis to a\nmultimodal setup where other relevant modalities accompany language. In this\npaper, we pose the problem of multimodal sentiment analysis as modeling\nintra-modality and inter-modality dynamics. We introduce a novel model, termed\nTensor Fusion Network, which learns both such dynamics end-to-end. The proposed\napproach is tailored for the volatile nature of spoken language in online\nvideos as well as accompanying gestures and voice. In the experiments, our\nmodel outperforms state-of-the-art approaches for both multimodal and unimodal\nsentiment analysis.", "no": 32}, {"url": "https://arxiv.org/abs/1709.00103", "title": "Seq2SQL: Generating Structured Queries from Natural Language using\n  Reinforcement Learning", "cites": "1 080", "abstract": "A significant amount of the world's knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.", "no": 33}, {"url": "https://arxiv.org/abs/1708.02182", "title": "Regularizing and Optimizing LSTM Language Models", "cites": "1 067", "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.", "no": 34}, {"url": "https://arxiv.org/abs/1711.00043", "title": "Unsupervised Machine Translation Using Monolingual Corpora Only", "cites": "1 064", "abstract": "Machine translation has recently achieved impressive performance thanks to\nrecent advances in deep learning and the availability of large-scale parallel\ncorpora. There have been numerous attempts to extend these successes to\nlow-resource language pairs, yet requiring tens of thousands of parallel\nsentences. In this work, we take this research direction to the extreme and\ninvestigate whether it is possible to learn to translate even without any\nparallel data. We propose a model that takes sentences from monolingual corpora\nin two different languages and maps them into the same latent space. By\nlearning to reconstruct in both languages from this shared feature space, the\nmodel effectively learns to translate without using any labeled data. We\ndemonstrate our model on two widely used datasets and two language pairs,\nreporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French\ndatasets, without using even a single parallel sentence at training time.", "no": 35}, {"url": "https://arxiv.org/abs/1706.00188", "title": "Deep Learning for Hate Speech Detection in Tweets", "cites": "1 063", "abstract": "Hate speech detection on Twitter is critical for applications like\ncontroversial event extraction, building AI chatterbots, content\nrecommendation, and sentiment analysis. We define this task as being able to\nclassify a tweet as racist, sexist or neither. The complexity of the natural\nlanguage constructs makes this task very challenging. We perform extensive\nexperiments with multiple deep learning architectures to learn semantic word\nembeddings to handle this complexity. Our experiments on a benchmark dataset of\n16K annotated tweets show that such deep learning methods outperform\nstate-of-the-art char/word n-gram methods by ~18 F1 points.", "no": 36}, {"url": "https://arxiv.org/abs/1703.00955", "title": "Toward Controlled Generation of Text", "cites": "958", "abstract": "Generic generation and manipulation of text is challenging and has limited\nsuccess compared to recent deep generative modeling in visual domain. This\npaper aims at generating plausible natural language sentences, whose attributes\nare dynamically controlled by learning disentangled latent representations with\ndesignated semantics. We propose a new neural generative model which combines\nvariational auto-encoders and holistic attribute discriminators for effective\nimposition of semantic structures. With differentiable approximation to\ndiscrete text samples, explicit constraints on independent attribute controls,\nand efficient collaborative learning of generator and discriminators, our model\nlearns highly interpretable representations from even only word annotations,\nand produces realistic sentences with desired attributes. Quantitative\nevaluation validates the accuracy of sentence and attribute generation.", "no": 37}, {"url": "https://arxiv.org/abs/1709.00893", "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification", "cites": "935", "abstract": "Aspect-level sentiment classification aims at identifying the sentiment\npolarity of specific target in its context. Previous approaches have realized\nthe importance of targets in sentiment classification and developed various\nmethods with the goal of precisely modeling their contexts via generating\ntarget-specific representations. However, these studies always ignore the\nseparate modeling of targets. In this paper, we argue that both targets and\ncontexts deserve special treatment and need to be learned their own\nrepresentations via interactive learning. Then, we propose the interactive\nattention networks (IAN) to interactively learn attentions in the contexts and\ntargets, and generate the representations for targets and contexts separately.\nWith this design, the IAN model can well represent a target and its collocative\ncontext, which is helpful to sentiment classification. Experimental results on\nSemEval 2014 Datasets demonstrate the effectiveness of our model.", "no": 38}, {"url": "https://arxiv.org/abs/1702.01923", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "cites": "934", "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language\nprocessing (NLP). Convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), the two main types of DNN architectures, are widely explored to\nhandle various NLP tasks. CNN is supposed to be good at extracting\nposition-invariant features and RNN at modeling units in sequence. The state of\nthe art on many NLP tasks often switches due to the battle between CNNs and\nRNNs. This work is the first systematic comparison of CNN and RNN on a wide\nrange of representative NLP tasks, aiming to give basic guidance for DNN\nselection.", "no": 39}, {"url": "https://arxiv.org/abs/1707.09457", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints", "cites": "915", "abstract": "Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively.", "no": 40}, {"url": "https://arxiv.org/abs/1708.00107", "title": "Learned in Translation: Contextualized Word Vectors", "cites": "890", "abstract": "Computer vision has benefited from initializing multiple deep layers with\nweights pretrained on large supervised training sets like ImageNet. Natural\nlanguage processing (NLP) typically sees initialization of only the lowest\nlayer of deep models with pretrained word vectors. In this paper, we use a deep\nLSTM encoder from an attentional sequence-to-sequence model trained for machine\ntranslation (MT) to contextualize word vectors. We show that adding these\ncontext vectors (CoVe) improves performance over using only unsupervised word\nand character vectors on a wide variety of common NLP tasks: sentiment analysis\n(SST, IMDb), question classification (TREC), entailment (SNLI), and question\nanswering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe\nimproves performance of our baseline models to the state of the art.", "no": 41}, {"url": "https://arxiv.org/abs/1711.08412", "title": "Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes", "cites": "887", "abstract": "Word embeddings use vectors to represent words such that the geometry between\nvectors captures semantic relationship between the words. In this paper, we\ndevelop a framework to demonstrate how the temporal dynamics of the embedding\ncan be leveraged to quantify changes in stereotypes and attitudes toward women\nand ethnic minorities in the 20th and 21st centuries in the United States. We\nintegrate word embeddings trained on 100 years of text data with the U.S.\nCensus to show that changes in the embedding track closely with demographic and\noccupation shifts over time. The embedding captures global social shifts --\ne.g., the women's movement in the 1960s and Asian immigration into the U.S --\nand also illuminates how specific adjectives and occupations became more\nclosely associated with certain populations over time. Our framework for\ntemporal analysis of word embedding opens up a powerful new intersection\nbetween machine learning and quantitative social science.", "no": 42}, {"url": "https://arxiv.org/abs/1701.06547", "title": "Adversarial Learning for Neural Dialogue Generation", "cites": "885", "abstract": "In this paper, drawing intuition from the Turing test, we propose using\nadversarial training for open-domain dialogue generation: the system is trained\nto produce sequences that are indistinguishable from human-generated dialogue\nutterances. We cast the task as a reinforcement learning (RL) problem where we\njointly train two systems, a generative model to produce response sequences,\nand a discriminator---analagous to the human evaluator in the Turing test--- to\ndistinguish between the human-generated dialogues and the machine-generated\nones. The outputs from the discriminator are then used as rewards for the\ngenerative model, pushing the system to generate dialogues that mostly resemble\nhuman dialogues.\n  In addition to adversarial training we describe a model for adversarial {\\em\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\nmetric, while avoiding a number of potential pitfalls. Experimental results on\nseveral metrics, including adversarial evaluation, demonstrate that the\nadversarially-trained system generates higher-quality responses than previous\nbaselines.", "no": 43}, {"url": "https://arxiv.org/abs/1707.07045", "title": "End-to-end Neural Coreference Resolution", "cites": "870", "abstract": "We introduce the first end-to-end coreference resolution model and show that\nit significantly outperforms all previous work without using a syntactic parser\nor hand-engineered mention detector. The key idea is to directly consider all\nspans in a document as potential mentions and learn distributions over possible\nantecedents for each. The model computes span embeddings that combine\ncontext-dependent boundary representations with a head-finding attention\nmechanism. It is trained to maximize the marginal likelihood of gold antecedent\nspans from coreference clusters and is factored to enable aggressive pruning of\npotential mentions. Experiments demonstrate state-of-the-art performance, with\na gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model\nensemble, despite the fact that this is the first approach to be successfully\ntrained with no external resources.", "no": 44}, {"url": "https://arxiv.org/abs/1710.10467", "title": "Generalized End-to-End Loss for Speaker Verification", "cites": "866", "abstract": "In this paper, we propose a new loss function called generalized end-to-end\n(GE2E) loss, which makes the training of speaker verification models more\nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike\nTE2E, the GE2E loss function updates the network in a way that emphasizes\nexamples that are difficult to verify at each step of the training process.\nAdditionally, the GE2E loss does not require an initial stage of example\nselection. With these properties, our model with the new loss function\ndecreases speaker verification EER by more than 10%, while reducing the\ntraining time by 60% at the same time. We also introduce the MultiReader\ntechnique, which allows us to do domain adaptation - training a more accurate\nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as\nwell as multiple dialects.", "no": 45}, {"url": "https://arxiv.org/abs/1703.04826", "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role\n  Labeling", "cites": "810", "abstract": "Semantic role labeling (SRL) is the task of identifying the\npredicate-argument structure of a sentence. It is typically regarded as an\nimportant step in the standard NLP pipeline. As the semantic representations\nare closely related to syntactic ones, we exploit syntactic information in our\nmodel. We propose a version of graph convolutional networks (GCNs), a recent\nclass of neural networks operating on graphs, suited to model syntactic\ndependency graphs. GCNs over syntactic dependency trees are used as sentence\nencoders, producing latent feature representations of words in a sentence. We\nobserve that GCN layers are complementary to LSTM ones: when we stack both GCN\nand LSTM layers, we obtain a substantial improvement over an already\nstate-of-the-art LSTM SRL model, resulting in the best reported scores on the\nstandard benchmark (CoNLL-2009) both for Chinese and English.", "no": 46}, {"url": "https://arxiv.org/abs/1702.03814", "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences", "cites": "778", "abstract": "Natural language sentence matching is a fundamental technology for a variety\nof tasks. Previous approaches either match sentences from a single direction or\nonly apply single granular (word-by-word or sentence-by-sentence) matching. In\nthis work, we propose a bilateral multi-perspective matching (BiMPM) model\nunder the \"matching-aggregation\" framework. Given two sentences $P$ and $Q$,\nour model first encodes them with a BiLSTM encoder. Next, we match the two\nencoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In\neach matching direction, each time step of one sentence is matched against all\ntime-steps of the other sentence from multiple perspectives. Then, another\nBiLSTM layer is utilized to aggregate the matching results into a fix-length\nmatching vector. Finally, based on the matching vector, the decision is made\nthrough a fully connected layer. We evaluate our model on three tasks:\nparaphrase identification, natural language inference and answer sentence\nselection. Experimental results on standard benchmark datasets show that our\nmodel achieves the state-of-the-art performance on all tasks.", "no": 47}, {"url": "https://arxiv.org/abs/1703.09902", "title": "Survey of the State of the Art in Natural Language Generation: Core\n  tasks, applications and evaluation", "cites": "778", "abstract": "This paper surveys the current state of the art in Natural Language\nGeneration (NLG), defined as the task of generating text or speech from\nnon-linguistic input. A survey of NLG is timely in view of the changes that the\nfield has undergone over the past decade or so, especially in relation to new\n(usually data-driven) methods, as well as new applications of NLG technology.\nThis survey therefore aims to (a) give an up-to-date synthesis of research on\nthe core tasks in NLG and the architectures adopted in which such tasks are\norganised; (b) highlight a number of relatively recent research topics that\nhave arisen partly as a result of growing synergies between NLG and other areas\nof artificial intelligence; (c) draw attention to the challenges in NLG\nevaluation, relating them to similar challenges faced in other areas of Natural\nLanguage Processing, with an emphasis on different evaluation methods and the\nrelationships between them.", "no": 48}, {"url": "https://arxiv.org/abs/1711.02281", "title": "Non-Autoregressive Neural Machine Translation", "cites": "766", "abstract": "Existing approaches to neural machine translation condition each output word\non previously generated outputs. We introduce a model that avoids this\nautoregressive property and produces its outputs in parallel, allowing an order\nof magnitude lower latency during inference. Through knowledge distillation,\nthe use of input token fertilities as a latent variable, and policy gradient\nfine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative\nto the autoregressive Transformer network used as a teacher. We demonstrate\nsubstantial cumulative improvements associated with each of the three aspects\nof our training strategy, and validate our approach on IWSLT 2016\nEnglish-German and two WMT language pairs. By sampling fertilities in parallel\nat inference time, our non-autoregressive model achieves near-state-of-the-art\nperformance of 29.8 BLEU on WMT 2016 English-Romanian.", "no": 49}, {"url": "https://arxiv.org/abs/1704.00656", "title": "Detection and Resolution of Rumours in Social Media: A Survey", "cites": "765", "abstract": "Despite the increasing use of social media platforms for information and news\ngathering, its unmoderated nature often leads to the emergence and spread of\nrumours, i.e. pieces of information that are unverified at the time of posting.\nAt the same time, the openness of social media platforms provides opportunities\nto study how users share and discuss rumours, and to explore how natural\nlanguage processing and data mining techniques may be used to find ways of\ndetermining their veracity. In this survey we introduce and discuss two types\nof rumours that circulate on social media; long-standing rumours that circulate\nfor long periods of time, and newly-emerging rumours spawned during fast-paced\nevents such as breaking news, where reports are released piecemeal and often\nwith an unverified status in their early stages. We provide an overview of\nresearch into social media rumours with the ultimate goal of developing a\nrumour classification system that consists of four components: rumour\ndetection, rumour tracking, rumour stance classification and rumour veracity\nclassification. We delve into the approaches presented in the scientific\nliterature for the development of each of these four components. We summarise\nthe efforts and achievements so far towards the development of rumour\nclassification systems and conclude with suggestions for avenues for future\nresearch in social media mining for detection and resolution of rumours.", "no": 50}, {"url": "https://arxiv.org/abs/1710.11041", "title": "Unsupervised Neural Machine Translation", "cites": "760", "abstract": "In spite of the recent success of neural machine translation (NMT) in\nstandard benchmarks, the lack of large parallel corpora poses a major practical\nproblem for many language pairs. There have been several proposals to alleviate\nthis issue with, for instance, triangulation and semi-supervised learning\ntechniques, but they still require a strong cross-lingual signal. In this work,\nwe completely remove the need of parallel data and propose a novel method to\ntrain an NMT system in a completely unsupervised manner, relying on nothing but\nmonolingual corpora. Our model builds upon the recent work on unsupervised\nembedding mappings, and consists of a slightly modified attentional\nencoder-decoder model that can be trained on monolingual corpora alone using a\ncombination of denoising and backtranslation. Despite the simplicity of the\napproach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014\nFrench-to-English and German-to-English translation. The model can also profit\nfrom small parallel corpora, and attains 21.81 and 15.24 points when combined\nwith 100,000 parallel sentences, respectively. Our implementation is released\nas an open source project.", "no": 51}, {"url": "https://arxiv.org/abs/1709.05522", "title": "AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech\n  Recognition Baseline", "cites": "753", "abstract": "An open-source Mandarin speech corpus called AISHELL-1 is released. It is by\nfar the largest corpus which is suitable for conducting the speech recognition\nresearch and building speech recognition systems for Mandarin. The recording\nprocedure, including audio capturing devices and environments are presented in\ndetails. The preparation of the related resources, including transcriptions and\nlexicon are described. The corpus is released with a Kaldi recipe. Experimental\nresults implies that the quality of audio recordings and transcriptions are\npromising.", "no": 52}, {"url": "https://arxiv.org/abs/1708.05148", "title": "Natural Language Processing: State of The Art, Current Trends and\n  Challenges", "cites": "750", "abstract": "Natural language processing (NLP) has recently gained much attention for\nrepresenting and analysing human language computationally. It has spread its\napplications in various fields such as machine translation, email spam\ndetection, information extraction, summarization, medical, and question\nanswering etc. The paper distinguishes four phases by discussing different\nlevels of NLP and components of Natural Language Generation (NLG) followed by\npresenting the history and evolution of NLP, state of the art presenting the\nvarious applications of NLP and current trends and challenges.", "no": 53}, {"url": "https://arxiv.org/abs/1705.09655", "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "cites": "747", "abstract": "This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order.", "no": 54}, {"url": "https://arxiv.org/abs/1708.07104", "title": "Automatic Detection of Fake News", "cites": "741", "abstract": "The proliferation of misleading information in everyday access media outlets\nsuch as social media feeds, news blogs, and online newspapers have made it\nchallenging to identify trustworthy news sources, thus increasing the need for\ncomputational tools able to provide insights into the reliability of online\ncontent. In this paper, we focus on the automatic identification of fake\ncontent in online news. Our contribution is twofold. First, we introduce two\nnovel datasets for the task of fake news detection, covering seven different\nnews domains. We describe the collection, annotation, and validation process in\ndetail and present several exploratory analysis on the identification of\nlinguistic differences in fake and legitimate news content. Second, we conduct\na set of learning experiments to build accurate fake news detectors. In\naddition, we provide comparative analyses of the automatic and manual\nidentification of fake news.", "no": 55}, {"url": "https://arxiv.org/abs/1703.10960", "title": "Learning Discourse-level Diversity for Neural Dialog Models using\n  Conditional Variational Autoencoders", "cites": "734", "abstract": "While recent neural encoder-decoder models have shown great promise in\nmodeling open-domain conversations, they often generate dull and generic\nresponses. Unlike past work that has focused on diversifying the output of the\ndecoder at word-level to alleviate this problem, we present a novel framework\nbased on conditional variational autoencoders that captures the discourse-level\ndiversity in the encoder. Our model uses latent variables to learn a\ndistribution over potential conversational intents and generates diverse\nresponses using only greedy decoders. We have further developed a novel variant\nthat is integrated with linguistic prior knowledge for better performance.\nFinally, the training procedure is improved by introducing a bag-of-word loss.\nOur proposed models have been validated to generate significantly more diverse\nresponses than baseline approaches and exhibit competence in discourse-level\ndecision-making.", "no": 56}, {"url": "https://arxiv.org/abs/1709.04696", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language\n  Understanding", "cites": "723", "abstract": "Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely\nused on NLP tasks to capture the long-term and local dependencies,\nrespectively. Attention mechanisms have recently attracted enormous interest\ndue to their highly parallelizable computation, significantly less training\ntime, and flexibility in modeling dependencies. We propose a novel attention\nmechanism in which the attention between elements from input sequence(s) is\ndirectional and multi-dimensional (i.e., feature-wise). A light-weight neural\nnet, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn\nsentence embedding, based solely on the proposed attention without any RNN/CNN\nstructure. DiSAN is only composed of a directional self-attention with temporal\norder encoded, followed by a multi-dimensional attention that compresses the\nsequence into a vector representation. Despite its simple form, DiSAN\noutperforms complicated RNN models on both prediction quality and time\nefficiency. It achieves the best test accuracy among all sentence encoding\nmethods and improves the most recent best result by 1.02% on the Stanford\nNatural Language Inference (SNLI) dataset, and shows state-of-the-art test\naccuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language\ninference (MultiNLI), Sentences Involving Compositional Knowledge (SICK),\nCustomer Review, MPQA, TREC question-type classification and Subjectivity\n(SUBJ) datasets.", "no": 57}, {"url": "https://arxiv.org/abs/1711.02173", "title": "Synthetic and Natural Noise Both Break Neural Machine Translation", "cites": "711", "abstract": "Character-based neural machine translation (NMT) models alleviate\nout-of-vocabulary issues, learn morphology, and move us closer to completely\nend-to-end translation systems. Unfortunately, they are also very brittle and\neasily falter when presented with noisy data. In this paper, we confront NMT\nmodels with synthetic and natural sources of noise. We find that\nstate-of-the-art models fail to translate even moderately noisy texts that\nhumans have no trouble comprehending. We explore two approaches to increase\nmodel robustness: structure-invariant word representations and robust training\non noisy texts. We find that a model based on a character convolutional neural\nnetwork is able to simultaneously learn representations robust to multiple\nkinds of noise.", "no": 58}, {"url": "https://arxiv.org/abs/1704.01074", "title": "Emotional Chatting Machine: Emotional Conversation Generation with\n  Internal and External Memory", "cites": "703", "abstract": "Perception and expression of emotion are key factors to the success of\ndialogue systems or conversational agents. However, this problem has not been\nstudied in large-scale conversation generation so far. In this paper, we\npropose Emotional Chatting Machine (ECM) that can generate appropriate\nresponses not only in content (relevant and grammatical) but also in emotion\n(emotionally consistent). To the best of our knowledge, this is the first work\nthat addresses the emotion factor in large-scale conversation generation. ECM\naddresses the factor using three new mechanisms that respectively (1) models\nthe high-level abstraction of emotion expressions by embedding emotion\ncategories, (2) captures the change of implicit internal emotion states, and\n(3) uses explicit emotion expressions with an external emotion vocabulary.\nExperiments show that the proposed model can generate responses appropriate not\nonly in content but also in emotion.", "no": 59}, {"url": "https://arxiv.org/abs/1707.05005", "title": "graph2vec: Learning Distributed Representations of Graphs", "cites": "677", "abstract": "Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels.", "no": 60}, {"url": "https://arxiv.org/abs/1704.05021", "title": "Sparse Communication for Distributed Gradient Descent", "cites": "676", "abstract": "We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU.", "no": 61}, {"url": "https://arxiv.org/abs/1703.02507", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram\n  Features", "cites": "673", "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude\nof applications raises the obvious question if similar methods could be derived\nto improve embeddings (i.e. semantic representations) of word sequences as\nwell. We present a simple but efficient unsupervised objective to train\ndistributed representations of sentences. Our method outperforms the\nstate-of-the-art unsupervised models on most benchmark tasks, highlighting the\nrobustness of the produced general-purpose sentence embeddings.", "no": 62}, {"url": "https://arxiv.org/abs/1703.04908", "title": "Emergence of Grounded Compositional Language in Multi-Agent Populations", "cites": "671", "abstract": "By capturing statistical patterns in large corpora, machine learning has\nenabled significant advances in natural language processing, including in\nmachine translation, question answering, and sentiment analysis. However, for\nagents to intelligently interact with humans, simply capturing the statistical\npatterns is insufficient. In this paper we investigate if, and how, grounded\ncompositional language can emerge as a means to achieve goals in multi-agent\npopulations. Towards this end, we propose a multi-agent learning environment\nand learning methods that bring about emergence of a basic compositional\nlanguage. This language is represented as streams of abstract discrete symbols\nuttered by agents over time, but nonetheless has a coherent structure that\npossesses a defined vocabulary and syntax. We also observe emergence of\nnon-verbal communication such as pointing and guiding when language\ncommunication is unavailable.", "no": 63}, {"url": "https://arxiv.org/abs/1712.07040", "title": "The NarrativeQA Reading Comprehension Challenge", "cites": "671", "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requires\nintegrating information and reasoning about events, entities, and their\nrelations across a full document. Question answering is conventionally used to\nassess RC ability, in both artificial agents and children learning to read.\nHowever, existing RC datasets and tasks are dominated by questions that can be\nsolved by selecting answers using superficial information (e.g., local context\nsimilarity or global term frequency); they thus fail to test for the essential\nintegrative aspect of RC. To encourage progress on deeper comprehension of\nlanguage, we present a new dataset and set of tasks in which the reader must\nanswer questions about stories by reading entire books or movie scripts. These\ntasks are designed so that successfully answering their questions requires\nunderstanding the underlying narrative rather than relying on shallow pattern\nmatching or salience. We show that although humans solve the tasks easily,\nstandard RC models struggle on the tasks presented here. We provide an analysis\nof the dataset and the challenges it presents.", "no": 64}, {"url": "https://arxiv.org/abs/1707.06690", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "cites": "669", "abstract": "We study the problem of learning to reason in large scale knowledge graphs\n(KGs). More specifically, we describe a novel reinforcement learning framework\nfor learning multi-hop relational paths: we use a policy-based agent with\ncontinuous states based on knowledge graph embeddings, which reasons in a KG\nvector space by sampling the most promising relation to extend its path. In\ncontrast to prior work, our approach includes a reward function that takes the\naccuracy, diversity, and efficiency into consideration. Experimentally, we show\nthat our proposed method outperforms a path-ranking based algorithm and\nknowledge graph embedding methods on Freebase and Never-Ending Language\nLearning datasets.", "no": 65}, {"url": "https://arxiv.org/abs/1711.01731", "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers", "cites": "666", "abstract": "Dialogue systems have attracted more and more attention. Recent advances on\ndialogue systems are overwhelmingly contributed by deep learning techniques,\nwhich have been employed to enhance a wide range of big data applications such\nas computer vision, natural language processing, and recommender systems. For\ndialogue systems, deep learning can leverage a massive amount of data to learn\nmeaningful feature representations and response generation strategies, while\nrequiring a minimum amount of hand-crafting. In this article, we give an\noverview to these recent advances on dialogue systems from various perspectives\nand discuss some possible research directions. In particular, we generally\ndivide existing dialogue systems into task-oriented and non-task-oriented\nmodels, then detail how deep learning techniques help them with representative\nalgorithms and finally discuss some appealing research directions that can\nbring the dialogue system research into a new frontier.", "no": 66}, {"url": "https://arxiv.org/abs/1712.02121", "title": "A Novel Embedding Model for Knowledge Base Completion Based on\n  Convolutional Neural Network", "cites": "638", "abstract": "In this paper, we propose a novel embedding model, named ConvKB, for\nknowledge base completion. Our model ConvKB advances state-of-the-art models by\nemploying a convolutional neural network, so that it can capture global\nrelationships and transitional characteristics between entities and relations\nin knowledge bases. In ConvKB, each triple (head entity, relation, tail entity)\nis represented as a 3-column matrix where each column vector represents a\ntriple element. This 3-column matrix is then fed to a convolution layer where\nmultiple filters are operated on the matrix to generate different feature maps.\nThese feature maps are then concatenated into a single feature vector\nrepresenting the input triple. The feature vector is multiplied with a weight\nvector via a dot product to return a score. This score is then used to predict\nwhether the triple is valid or not. Experiments show that ConvKB achieves\nbetter link prediction performance than previous state-of-the-art embedding\nmodels on two benchmark datasets WN18RR and FB15k-237.", "no": 67}, {"url": "https://arxiv.org/abs/1705.00106", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "cites": "632", "abstract": "We study automatic question generation for sentences from text passages in\nreading comprehension. We introduce an attention-based sequence learning model\nfor the task and investigate the effect of encoding sentence- vs.\nparagraph-level information. In contrast to all previous work, our model does\nnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead\ntrainable end-to-end via sequence-to-sequence learning. Automatic evaluation\nresults show that our system significantly outperforms the state-of-the-art\nrule-based system. In human evaluations, questions generated by our system are\nalso rated as being more natural (i.e., grammaticality, fluency) and as more\ndifficult to answer (in terms of syntactic and lexical divergence from the\noriginal text and reasoning needed to answer).", "no": 68}, {"url": "https://arxiv.org/abs/1705.00108", "title": "Semi-supervised sequence tagging with bidirectional language models", "cites": "620", "abstract": "Pre-trained word embeddings learned from unlabeled text have become a\nstandard component of neural network architectures for NLP tasks. However, in\nmost cases, the recurrent network that operates on word-level representations\nto produce context sensitive representations is trained on relatively little\nlabeled data. In this paper, we demonstrate a general semi-supervised approach\nfor adding pre- trained context embeddings from bidirectional language models\nto NLP systems and apply it to sequence labeling tasks. We evaluate our model\non two standard datasets for named entity recognition (NER) and chunking, and\nin both cases achieve state of the art results, surpassing previous systems\nthat use other forms of transfer or joint learning with additional labeled data\nand task specific gazetteers.", "no": 69}, {"url": "https://arxiv.org/abs/1706.04115", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "cites": "615", "abstract": "We show that relation extraction can be reduced to answering simple reading\ncomprehension questions, by associating one or more natural-language questions\nwith each relation slot. This reduction has several advantages: we can (1)\nlearn relation-extraction models by extending recent neural\nreading-comprehension techniques, (2) build very large training sets for those\nmodels by combining relation-specific crowd-sourced questions with distant\nsupervision, and even (3) do zero-shot learning by extracting new relation\ntypes that are only specified at test-time, for which we have no labeled\ntraining examples. Experiments on a Wikipedia slot-filling task demonstrate\nthat the approach can generalize to new questions for known relation types with\nhigh accuracy, and that zero-shot generalization to unseen relation types is\npossible, at lower accuracy levels, setting the bar for future work on this\ntask.", "no": 70}, {"url": "https://arxiv.org/abs/1711.11543", "title": "Embodied Question Answering", "cites": "609", "abstract": "We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (\"What color is the car?\"). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\n  This challenging task requires a range of AI skills -- active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA.", "no": 71}, {"url": "https://arxiv.org/abs/1702.07825", "title": "Deep Voice: Real-time Neural Text-to-Speech", "cites": "594", "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.", "no": 72}, {"url": "https://arxiv.org/abs/1705.04146", "title": "Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "cites": "591", "abstract": "Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.", "no": 73}, {"url": "https://arxiv.org/abs/1704.05742", "title": "Adversarial Multi-task Learning for Text Classification", "cites": "586", "abstract": "Neural network models have shown their promising opportunities for multi-task\nlearning, which focus on learning the shared layers to extract the common and\ntask-invariant features. However, in most existing approaches, the extracted\nshared features are prone to be contaminated by task-specific features or the\nnoise brought by other tasks. In this paper, we propose an adversarial\nmulti-task learning framework, alleviating the shared and private latent\nfeature spaces from interfering with each other. We conduct extensive\nexperiments on 16 different text classification tasks, which demonstrates the\nbenefits of our approach. Besides, we show that the shared knowledge learned by\nour proposed model can be regarded as off-the-shelf knowledge and easily\ntransferred to new tasks. The datasets of all 16 tasks are publicly available\nat \\url{http://nlp.fudan.edu.cn/data/}", "no": 74}, {"url": "https://arxiv.org/abs/1702.05638", "title": "A Stylometric Inquiry into Hyperpartisan and Fake News", "cites": "584", "abstract": "This paper reports on a writing style analysis of hyperpartisan (i.e.,\nextremely one-sided) news in connection to fake news. It presents a large\ncorpus of 1,627 articles that were manually fact-checked by professional\njournalists from BuzzFeed. The articles originated from 9 well-known political\npublishers, 3 each from the mainstream, the hyperpartisan left-wing, and the\nhyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of\nwhich originated from hyperpartisan publishers.\n  We propose and demonstrate a new way of assessing style similarity between\ntext categories via Unmasking---a meta-learning approach originally devised for\nauthorship verification---, revealing that the style of left-wing and\nright-wing news have a lot more in common than any of the two have with the\nmainstream. Furthermore, we show that hyperpartisan news can be discriminated\nwell by its style from the mainstream (F1=0.78), as can be satire from both\n(F1=0.81). Unsurprisingly, style-based fake news detection does not live up to\nscratch (F1=0.46). Nevertheless, the former results are important to implement\npre-screening for fake news detectors.", "no": 75}, {"url": "https://arxiv.org/abs/1710.11342", "title": "Generating Natural Adversarial Examples", "cites": "583", "abstract": "Due to their complex nature, it is hard to characterize the ways in which\nmachine learning models can misbehave or be exploited when deployed. Recent\nwork on adversarial examples, i.e. inputs with minor perturbations that result\nin substantially different model predictions, is helpful in evaluating the\nrobustness of these models by exposing the adversarial scenarios where they\nfail. However, these malicious perturbations are often unnatural, not\nsemantically meaningful, and not applicable to complicated domains such as\nlanguage. In this paper, we propose a framework to generate natural and legible\nadversarial examples that lie on the data manifold, by searching in semantic\nspace of dense and continuous data representation, utilizing the recent\nadvances in generative adversarial networks. We present generated adversaries\nto demonstrate the potential of the proposed approach for black-box classifiers\nfor a wide range of applications such as image classification, textual\nentailment, and machine translation. We include experiments to show that the\ngenerated adversaries are natural, legible to humans, and useful in evaluating\nand analyzing black-box classifiers.", "no": 76}, {"url": "https://arxiv.org/abs/1706.05075", "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging\n  Scheme", "cites": "582", "abstract": "Joint extraction of entities and relations is an important task in\ninformation extraction. To tackle this problem, we firstly propose a novel\ntagging scheme that can convert the joint extraction task to a tagging problem.\nThen, based on our tagging scheme, we study different end-to-end models to\nextract entities and their relations directly, without identifying entities and\nrelations separately. We conduct experiments on a public dataset produced by\ndistant supervision method and the experimental results show that the tagging\nbased methods are better than most of the existing pipelined and joint learning\nmethods. What's more, the end-to-end model proposed in this paper, achieves the\nbest results on the public dataset.", "no": 77}, {"url": "https://arxiv.org/abs/1707.08052", "title": "Challenges in Data-to-Document Generation", "cites": "567", "abstract": "Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements.", "no": 78}, {"url": "https://arxiv.org/abs/1702.01932", "title": "A Knowledge-Grounded Neural Conversation Model", "cites": "558", "abstract": "Neural network models are capable of generating extremely natural sounding\nconversational interactions. Nevertheless, these models have yet to demonstrate\nthat they can incorporate content in the form of factual information or\nentity-grounded opinion that would enable them to serve in more task-oriented\nconversational applications. This paper presents a novel, fully data-driven,\nand knowledge-grounded neural conversation model aimed at producing more\ncontentful responses without slot filling. We generalize the widely-used\nSeq2Seq approach by conditioning responses on both conversation history and\nexternal \"facts\", allowing the model to be versatile and applicable in an\nopen-domain setting. Our approach yields significant improvements over a\ncompetitive Seq2Seq baseline. Human judges found that our outputs are\nsignificantly more informative.", "no": 79}, {"url": "https://arxiv.org/abs/1703.09398", "title": "This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive\n  Content in Text Body, More Similar to Satire than Real News", "cites": "556", "abstract": "The problem of fake news has gained a lot of attention as it is claimed to\nhave had a significant impact on 2016 US Presidential Elections. Fake news is\nnot a new problem and its spread in social networks is well-studied. Often an\nunderlying assumption in fake news discussion is that it is written to look\nlike real news, fooling the reader who does not check for reliability of the\nsources or the arguments in its content. Through a unique study of three data\nsets and features that capture the style and the language of articles, we show\nthat this assumption is not true. Fake news in most cases is more similar to\nsatire than to real news, leading us to conclude that persuasion in fake news\nis achieved through heuristics rather than the strength of arguments. We show\noverall title structure and the use of proper nouns in titles are very\nsignificant in differentiating fake from real. This leads us to conclude that\nfake news is targeted for audiences who are not likely to read beyond titles\nand is aimed at creating mental associations between entities and claims.", "no": 80}, {"url": "https://arxiv.org/abs/1712.00377", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual\n  Question Answering", "cites": "554", "abstract": "A number of studies have found that today's Visual Question Answering (VQA)\nmodels are heavily driven by superficial correlations in the training data and\nlack sufficient image grounding. To encourage development of models geared\ntowards the latter, we propose a new setting for VQA where for every question\ntype, train and test sets have different prior distributions of answers.\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\nrespectively). First, we evaluate several existing VQA models under this new\nsetting and show that their performance degrades significantly compared to the\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\nAnswering model (GVQA) that contains inductive biases and restrictions in the\narchitecture specifically designed to prevent the model from 'cheating' by\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\ndisentangles the recognition of visual concepts present in the image from the\nidentification of plausible answer space for a given question, enabling the\nmodel to more robustly generalize across different distributions of answers.\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\nseveral cases. GVQA offers strengths complementary to SAN when trained and\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\ntransparent and interpretable than existing VQA models.", "no": 81}, {"url": "https://arxiv.org/abs/1706.06613", "title": "End-to-End Neural Ad-hoc Ranking with Kernel Pooling", "cites": "546", "abstract": "This paper proposes K-NRM, a kernel based neural model for document ranking.\nGiven a query and a set of documents, K-NRM uses a translation matrix that\nmodels word-level similarities via word embeddings, a new kernel-pooling\ntechnique that uses kernels to extract multi-level soft match features, and a\nlearning-to-rank layer that combines those features into the final ranking\nscore. The whole model is trained end-to-end. The ranking layer learns desired\nfeature patterns from the pairwise ranking loss. The kernels transfer the\nfeature patterns into soft-match targets at each similarity level and enforce\nthem on the translation matrix. The word embeddings are tuned accordingly so\nthat they can produce the desired soft matches. Experiments on a commercial\nsearch engine's query log demonstrate the improvements of K-NRM over prior\nfeature-based and neural-based states-of-the-art, and explain the source of\nK-NRM's advantage: Its kernel-guided embedding encodes a similarity metric\ntailored for matching query words to document words, and provides effective\nmulti-level soft matches.", "no": 82}, {"url": "https://arxiv.org/abs/1705.03633", "title": "Inferring and Executing Programs for Visual Reasoning", "cites": "527", "abstract": "Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.", "no": 83}, {"url": "https://arxiv.org/abs/1702.03859", "title": "Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax", "cites": "526", "abstract": "Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%.", "no": 84}, {"url": "https://arxiv.org/abs/1707.05589", "title": "On the State of the Art of Evaluation in Neural Language Models", "cites": "524", "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.", "no": 85}, {"url": "https://arxiv.org/abs/1707.02919", "title": "A Brief Survey of Text Mining: Classification, Clustering and Extraction\n  Techniques", "cites": "517", "abstract": "The amount of text that is generated every day is increasing dramatically.\nThis tremendous volume of mostly unstructured text cannot be simply processed\nand perceived by computers. Therefore, efficient and effective techniques and\nalgorithms are required to discover useful patterns. Text mining is the task of\nextracting meaningful information from text, which has gained significant\nattentions in recent years. In this paper, we describe several of the most\nfundamental text mining tasks and techniques including text pre-processing,\nclassification and clustering. Additionally, we briefly explain text mining in\nbiomedical and health care domains.", "no": 86}, {"url": "https://arxiv.org/abs/1703.03906", "title": "Massive Exploration of Neural Machine Translation Architectures", "cites": "508", "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past\nfew years with production systems now being deployed to end-users. One major\ndrawback of current architectures is that they are expensive to train,\ntypically requiring days to weeks of GPU time to converge. This makes\nexhaustive hyperparameter search, as is commonly done with other neural network\narchitectures, prohibitively expensive. In this work, we present the first\nlarge-scale analysis of NMT architecture hyperparameters. We report empirical\nresults and variance numbers for several hundred experimental runs,\ncorresponding to over 250,000 GPU hours on the standard WMT English to German\ntranslation task. Our experiments lead to novel insights and practical advice\nfor building and extending NMT architectures. As part of this contribution, we\nrelease an open-source NMT framework that enables researchers to easily\nexperiment with novel techniques and reproduce state of the art results.", "no": 87}, {"url": "https://arxiv.org/abs/1704.08619", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "cites": "508", "abstract": "Automatic affect recognition is a challenging task due to the various\nmodalities emotions can be expressed with. Applications can be found in many\ndomains including multimedia retrieval and human computer interaction. In\nrecent years, deep neural networks have been used with great success in\ndetermining emotional states. Inspired by this success, we propose an emotion\nrecognition system using auditory and visual modalities. To capture the\nemotional content for various styles of speaking, robust features need to be\nextracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to\nextract features from the speech, while for the visual modality a deep residual\nnetwork (ResNet) of 50 layers. In addition to the importance of feature\nextraction, a machine learning algorithm needs also to be insensitive to\noutliers while being able to model the context. To tackle this problem, Long\nShort-Term Memory (LSTM) networks are utilized. The system is then trained in\nan end-to-end fashion where - by also taking advantage of the correlations of\nthe each of the streams - we manage to significantly outperform the traditional\napproaches based on auditory and visual handcrafted features for the prediction\nof spontaneous and natural emotions on the RECOLA database of the AVEC 2016\nresearch challenge on emotion recognition.", "no": 88}, {"url": "https://arxiv.org/abs/1706.04902", "title": "A Survey Of Cross-lingual Word Embedding Models", "cites": "504", "abstract": "Cross-lingual representations of words enable us to reason about word meaning\nin multilingual contexts and are a key facilitator of cross-lingual transfer\nwhen developing natural language processing models for low-resource languages.\nIn this survey, we provide a comprehensive typology of cross-lingual word\nembedding models. We compare their data requirements and objective functions.\nThe recurring theme of the survey is that many of the models presented in the\nliterature optimize for the same objectives, and that seemingly different\nmodels are often equivalent modulo optimization strategies, hyper-parameters,\nand such. We also discuss the different ways cross-lingual word embeddings are\nevaluated, as well as future challenges and research horizons.", "no": 89}, {"url": "https://arxiv.org/abs/1708.03743", "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs", "cites": "496", "abstract": "Past work in relation extraction has focused on binary relations in single\nsentences. Recent NLP inroads in high-value domains have sparked interest in\nthe more general setting of extracting n-ary relations that span multiple\nsentences. In this paper, we explore a general relation extraction framework\nbased on graph long short-term memory networks (graph LSTMs) that can be easily\nextended to cross-sentence n-ary relation extraction. The graph formulation\nprovides a unified way of exploring different LSTM approaches and incorporating\nvarious intra-sentential and inter-sentential dependencies, such as sequential,\nsyntactic, and discourse relations. A robust contextual representation is\nlearned for the entities, which serves as input to the relation classifier.\nThis simplifies handling of relations with arbitrary arity, and enables\nmulti-task learning with related relations. We evaluate this framework in two\nimportant precision medicine settings, demonstrating its effectiveness with\nboth conventional supervised learning and distant supervision. Cross-sentence\nextraction produced larger knowledge bases. and multi-task learning\nsignificantly improved extraction accuracy. A thorough analysis of various LSTM\napproaches yielded useful insight the impact of linguistic analysis on\nextraction accuracy.", "no": 90}, {"url": "https://arxiv.org/abs/1704.01444", "title": "Learning to Generate Reviews and Discovering Sentiment", "cites": "494", "abstract": "We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment.", "no": 91}, {"url": "https://arxiv.org/abs/1711.06861", "title": "Style Transfer in Text: Exploration and Evaluation", "cites": "494", "abstract": "Style transfer is an important problem in natural language processing (NLP).\nHowever, the progress in language style transfer is lagged behind other\ndomains, such as computer vision, mainly because of the lack of parallel data\nand principle evaluation metrics. In this paper, we propose to learn style\ntransfer with non-parallel data. We explore two models to achieve this goal,\nand the key idea behind the proposed models is to learn separate content\nrepresentations and style representations using adversarial networks. We also\npropose novel evaluation metrics which measure two aspects of style transfer:\ntransfer strength and content preservation. We access our models and the\nevaluation metrics on two tasks: paper-news title transfer, and\npositive-negative review transfer. Results show that the proposed content\npreservation metric is highly correlate to human judgments, and the proposed\nmodels are able to generate sentences with higher style transfer strength and\nsimilar content preservation score comparing to auto-encoder.", "no": 92}, {"url": "https://arxiv.org/abs/1707.02268", "title": "Text Summarization Techniques: A Brief Survey", "cites": "492", "abstract": "In recent years, there has been a explosion in the amount of text data from a\nvariety of sources. This volume of text is an invaluable source of information\nand knowledge which needs to be effectively summarized to be useful. In this\nreview, the main approaches to automatic text summarization are described. We\nreview the different processes for summarization and describe the effectiveness\nand shortcomings of the different methods.", "no": 93}, {"url": "https://arxiv.org/abs/1705.02304", "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System", "cites": "487", "abstract": "We present Deep Speaker, a neural speaker embedding system that maps\nutterances to a hypersphere where speaker similarity is measured by cosine\nsimilarity. The embeddings generated by Deep Speaker can be used for many\ntasks, including speaker identification, verification, and clustering. We\nexperiment with ResCNN and GRU architectures to extract the acoustic features,\nthen mean pool to produce utterance-level speaker embeddings, and train using\ntriplet loss based on cosine similarity. Experiments on three distinct datasets\nsuggest that Deep Speaker outperforms a DNN-based i-vector baseline. For\nexample, Deep Speaker reduces the verification equal error rate by 50%\n(relatively) and improves the identification accuracy by 60% (relatively) on a\ntext-independent dataset. We also present results that suggest adapting from a\nmodel trained with Mandarin can improve accuracy for English speaker\nrecognition.", "no": 94}, {"url": "https://arxiv.org/abs/1710.06481", "title": "Constructing Datasets for Multi-hop Reading Comprehension Across\n  Documents", "cites": "487", "abstract": "Most Reading Comprehension methods limit themselves to queries which can be\nanswered using a single sentence, paragraph, or document. Enabling models to\ncombine disjoint pieces of textual evidence would extend the scope of machine\ncomprehension methods, but currently there exist no resources to train and test\nthis capability. We propose a novel task to encourage the development of models\nfor text understanding across multiple documents and to investigate the limits\nof existing methods. In our task, a model learns to seek and combine evidence -\neffectively performing multi-hop (alias multi-step) inference. We devise a\nmethodology to produce datasets for this task, given a collection of\nquery-answer pairs and thematically linked documents. Two datasets from\ndifferent domains are induced, and we identify potential pitfalls and devise\ncircumvention strategies. We evaluate two previously proposed competitive\nmodels and find that one can integrate information across documents. However,\nboth models struggle to select relevant information, as providing documents\nguaranteed to be relevant greatly improves their performance. While the models\noutperform several strong baselines, their best accuracy reaches 42.9% compared\nto human performance at 74.0% - leaving ample room for improvement.", "no": 95}, {"url": "https://arxiv.org/abs/1705.08947", "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech", "cites": "483", "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with\nlowdimensional trainable speaker embeddings to generate different voices from a\nsingle model. As a starting point, we show improvements over the two\nstate-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and\nTacotron. We introduce Deep Voice 2, which is based on a similar pipeline with\nDeep Voice 1, but constructed with higher performance building blocks and\ndemonstrates a significant audio quality improvement over Deep Voice 1. We\nimprove Tacotron by introducing a post-processing neural vocoder, and\ndemonstrate a significant audio quality improvement. We then demonstrate our\ntechnique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron\non two multi-speaker TTS datasets. We show that a single neural TTS system can\nlearn hundreds of unique voices from less than half an hour of data per\nspeaker, while achieving high audio quality synthesis and preserving the\nspeaker identities almost perfectly.", "no": 96}, {"url": "https://arxiv.org/abs/1704.04675", "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "cites": "482", "abstract": "We present a simple and effective approach to incorporating syntactic\nstructure into neural attention-based encoder-decoder models for machine\ntranslation. We rely on graph-convolutional networks (GCNs), a recent class of\nneural networks developed for modeling graph-structured data. Our GCNs use\npredicted syntactic dependency trees of source sentences to produce\nrepresentations of words (i.e. hidden states of the encoder) that are sensitive\nto their syntactic neighborhoods. GCNs take word representations as input and\nproduce word representations as output, so they can easily be incorporated as\nlayers into standard encoders (e.g., on top of bidirectional RNNs or\nconvolutional neural networks). We evaluate their effectiveness with\nEnglish-German and English-Czech translation experiments for different types of\nencoders and observe substantial improvements over their syntax-agnostic\nversions in all the considered setups.", "no": 97}, {"url": "https://arxiv.org/abs/1711.05851", "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in\n  Knowledge Bases using Reinforcement Learning", "cites": "478", "abstract": "Knowledge bases (KB), both automatically and manually constructed, are often\nincomplete --- many valid facts can be inferred from the KB by synthesizing\nexisting information. A popular approach to KB completion is to infer new\nrelations by combinatory reasoning over the information found along other paths\nconnecting a pair of entities. Given the enormous size of KBs and the\nexponential number of paths, previous path-based models have considered only\nthe problem of predicting a missing relation given two entities or evaluating\nthe truth of a proposed triple. Additionally, these methods have traditionally\nused random paths between fixed entity pairs or more recently learned to pick\npaths between them. We propose a new algorithm MINERVA, which addresses the\nmuch more difficult and practical task of answering questions where the\nrelation is known, but only one entity. Since random walks are impractical in a\nsetting with combinatorially many destinations from a start node, we present a\nneural reinforcement learning approach which learns how to navigate the graph\nconditioned on the input query to find predictive paths. Empirically, this\napproach obtains state-of-the-art results on several datasets, significantly\noutperforming prior methods.", "no": 98}, {"url": "https://arxiv.org/abs/1709.08624", "title": "Long Text Generation via Adversarial Training with Leaked Information", "cites": "465", "abstract": "Automatically generating coherent and semantically meaningful text has many\napplications in machine translation, dialogue systems, image captioning, etc.\nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN)\nthat use a discriminative model to guide the training of the generative model\nas a reinforcement learning policy has shown promising results in text\ngeneration. However, the scalar guiding signal is only available after the\nentire text has been generated and lacks intermediate information about text\nstructure during the generative process. As such, it limits its success when\nthe length of the generated text samples is long (more than 20 words). In this\npaper, we propose a new framework, called LeakGAN, to address the problem for\nlong text generation. We allow the discriminative net to leak its own\nhigh-level extracted features to the generative net to further help the\nguidance. The generator incorporates such informative signals into all\ngeneration steps through an additional Manager module, which takes the\nextracted features of current generated words and outputs a latent vector to\nguide the Worker module for next-word generation. Our extensive experiments on\nsynthetic data and various real-world tasks with Turing test demonstrate that\nLeakGAN is highly effective in long text generation and also improves the\nperformance in short text generation scenarios. More importantly, without any\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\nthrough the interaction between Manager and Worker.", "no": 99}, {"url": "https://arxiv.org/abs/1707.00683", "title": "Modulating early visual processing by language", "cites": "464", "abstract": "It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.", "no": 100}]