[{"url": "https://arxiv.org/abs/1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "cites": "85 221", "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).", "no": 1}, {"url": "https://arxiv.org/abs/1802.05365", "title": "Deep contextualized word representations", "cites": "11 180", "abstract": "We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.", "no": 2}, {"url": "https://arxiv.org/abs/1804.07461", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding", "cites": "6 467", "abstract": "For natural language understanding (NLU) technology to be maximally useful,\nboth practically and as a scientific object of study, it must be general: it\nmust be able to process language in a way that is not exclusively tailored to\nany one specific task or dataset. In pursuit of this objective, we introduce\nthe General Language Understanding Evaluation benchmark (GLUE), a tool for\nevaluating and analyzing the performance of models across a diverse range of\nexisting NLU tasks. GLUE is model-agnostic, but it incentivizes sharing\nknowledge across tasks because certain tasks have very limited training data.\nWe further provide a hand-crafted diagnostic test suite that enables detailed\nlinguistic analysis of NLU models. We evaluate baselines based on current\nmethods for multi-task and transfer learning and find that they do not\nimmediately give substantial improvements over the aggregate performance of\ntraining a separate model per task, indicating room for improvement in\ndeveloping general and robust NLU systems.", "no": 3}, {"url": "https://arxiv.org/abs/1803.01271", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling", "cites": "4 185", "abstract": "For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .", "no": 4}, {"url": "https://arxiv.org/abs/1806.09055", "title": "DARTS: Differentiable Architecture Search", "cites": "4 092", "abstract": "This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.", "no": 5}, {"url": "https://arxiv.org/abs/1808.06226", "title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "cites": "3 295", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer\nand detokenizer designed for Neural-based text processing, including Neural\nMachine Translation. It provides open-source C++ and Python implementations for\nsubword units. While existing subword segmentation tools assume that the input\nis pre-tokenized into word sequences, SentencePiece can train subword models\ndirectly from raw sentences, which allows us to make a purely end-to-end and\nlanguage independent system. We perform a validation experiment of NMT on\nEnglish-Japanese machine translation, and find that it is possible to achieve\ncomparable accuracy to direct subword training from raw sentences. We also\ncompare the performance of subword training and segmentation with various\nconfigurations. SentencePiece is available under the Apache 2 license at\nhttps://github.com/google/sentencepiece.", "no": 6}, {"url": "https://arxiv.org/abs/1804.08771", "title": "A Call for Clarity in Reporting BLEU Scores", "cites": "2 716", "abstract": "The field of machine translation faces an under-recognized problem because of\ninconsistency in the reporting of scores from its dominant metric. Although\npeople refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose\nvalues can vary wildly with changes to these parameters. These parameters are\noften not reported or are hard to find, and consequently, BLEU scores between\npapers cannot be directly compared. I quantify this variation, finding\ndifferences as high as 1.8 between commonly used configurations. The main\nculprit is different tokenization and normalization schemes applied to the\nreference. Pointing to the success of the parsing community, I suggest machine\ntranslation researchers settle upon the BLEU scheme used by the annual\nConference on Machine Translation (WMT), which does not allow for user-supplied\nreference processing, and provide a new tool, SacreBLEU, to facilitate this.", "no": 7}, {"url": "https://arxiv.org/abs/1802.03268", "title": "Efficient Neural Architecture Search via Parameter Sharing", "cites": "2 645", "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.", "no": 8}, {"url": "https://arxiv.org/abs/1806.03822", "title": "Know What You Don't Know: Unanswerable Questions for SQuAD", "cites": "2 612", "abstract": "Extractive reading comprehension systems can often locate the correct answer\nto a question in a context document, but they also tend to make unreliable\nguesses on questions for which the correct answer is not stated in the context.\nExisting datasets either focus exclusively on answerable questions, or use\nautomatically generated unanswerable questions that are easy to identify. To\naddress these weaknesses, we present SQuAD 2.0, the latest version of the\nStanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD\ndata with over 50,000 unanswerable questions written adversarially by\ncrowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,\nsystems must not only answer questions when possible, but also determine when\nno answer is supported by the paragraph and abstain from answering. SQuAD 2.0\nis a challenging natural language understanding task for existing models: a\nstrong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on\nSQuAD 2.0.", "no": 9}, {"url": "https://arxiv.org/abs/1803.02155", "title": "Self-Attention with Relative Position Representations", "cites": "2 103", "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by\nVaswani et al. (2017) achieves state-of-the-art results for machine\ntranslation. In contrast to recurrent and convolutional neural networks, it\ndoes not explicitly model relative or absolute position information in its\nstructure. Instead, it requires adding representations of absolute positions to\nits inputs. In this work we present an alternative approach, extending the\nself-attention mechanism to efficiently consider representations of the\nrelative positions, or distances between sequence elements. On the WMT 2014\nEnglish-to-German and English-to-French translation tasks, this approach yields\nimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,\nrespectively. Notably, we observe that combining relative and absolute position\nrepresentations yields no further improvement in translation quality. We\ndescribe an efficient implementation of our method and cast it as an instance\nof relation-aware self-attention mechanisms that can generalize to arbitrary\ngraph-labeled inputs.", "no": 10}, {"url": "https://arxiv.org/abs/1809.09600", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\n  Answering", "cites": "2 063", "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform\ncomplex reasoning and provide explanations for answers. We introduce HotpotQA,\na new dataset with 113k Wikipedia-based question-answer pairs with four key\nfeatures: (1) the questions require finding and reasoning over multiple\nsupporting documents to answer; (2) the questions are diverse and not\nconstrained to any pre-existing knowledge bases or knowledge schemas; (3) we\nprovide sentence-level supporting facts required for reasoning, allowing QA\nsystems to reason with strong supervision and explain the predictions; (4) we\noffer a new type of factoid comparison questions to test QA systems' ability to\nextract relevant facts and perform necessary comparison. We show that HotpotQA\nis challenging for the latest QA systems, and the supporting facts enable\nmodels to improve performance and make explainable predictions.", "no": 11}, {"url": "https://arxiv.org/abs/1803.11175", "title": "Universal Sentence Encoder", "cites": "1 812", "abstract": "We present models for encoding sentences into embedding vectors that\nspecifically target transfer learning to other NLP tasks. The models are\nefficient and result in accurate performance on diverse transfer tasks. Two\nvariants of the encoding models allow for trade-offs between accuracy and\ncompute resources. For both variants, we investigate and report the\nrelationship between model complexity, resource consumption, the availability\nof transfer task training data, and task performance. Comparisons are made with\nbaselines that use word level transfer learning via pretrained word embeddings\nas well as baselines do not use any transfer learning. We find that transfer\nlearning using sentence embeddings tends to outperform word level transfer.\nWith transfer learning via sentence embeddings, we observe surprisingly good\nperformance with minimal amounts of supervised training data for a transfer\ntask. We obtain encouraging results on Word Embedding Association Tests (WEAT)\ntargeted at detecting model bias. Our pre-trained sentence encoding models are\nmade freely available for download and on TF Hub.", "no": 12}, {"url": "https://arxiv.org/abs/1803.05457", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\n  Challenge", "cites": "1 763", "abstract": "We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community.", "no": 13}, {"url": "https://arxiv.org/abs/1809.05679", "title": "Graph Convolutional Networks for Text Classification", "cites": "1 675", "abstract": "Text classification is an important and classical problem in natural language\nprocessing. There have been a number of studies that applied convolutional\nneural networks (convolution on regular grid, e.g., sequence) to\nclassification. However, only a limited number of studies have explored the\nmore flexible graph convolutional neural networks (convolution on non-grid,\ne.g., arbitrary graph) for the task. In this work, we propose to use graph\nconvolutional networks for text classification. We build a single text graph\nfor a corpus based on word co-occurrence and document word relations, then\nlearn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text\nGCN is initialized with one-hot representation for word and document, it then\njointly learns the embeddings for both words and documents, as supervised by\nthe known class labels for documents. Our experimental results on multiple\nbenchmark datasets demonstrate that a vanilla Text GCN without any external\nword embeddings or knowledge outperforms state-of-the-art methods for text\nclassification. On the other hand, Text GCN also learns predictive word and\ndocument embeddings. In addition, experimental results show that the\nimprovement of Text GCN over state-of-the-art comparison methods become more\nprominent as we lower the percentage of training data, suggesting the\nrobustness of Text GCN to less training data in text classification.", "no": 14}, {"url": "https://arxiv.org/abs/1801.07883", "title": "Deep Learning for Sentiment Analysis : A Survey", "cites": "1 509", "abstract": "Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.", "no": 15}, {"url": "https://arxiv.org/abs/1808.08745", "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional\n  Neural Networks for Extreme Summarization", "cites": "1 475", "abstract": "We introduce extreme summarization, a new single-document summarization task\nwhich does not favor extractive strategies and calls for an abstractive\nmodeling approach. The idea is to create a short, one-sentence news summary\nanswering the question \"What is the article about?\". We collect a real-world,\nlarge-scale dataset for this task by harvesting online articles from the\nBritish Broadcasting Corporation (BBC). We propose a novel abstractive model\nwhich is conditioned on the article's topics and based entirely on\nconvolutional neural networks. We demonstrate experimentally that this\narchitecture captures long-range dependencies in a document and recognizes\npertinent content, outperforming an oracle extractive system and\nstate-of-the-art abstractive approaches when evaluated automatically and by\nhumans.", "no": 16}, {"url": "https://arxiv.org/abs/1804.03209", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "cites": "1 465", "abstract": "Describes an audio dataset of spoken words designed to help train and\nevaluate keyword spotting systems. Discusses why this task is an interesting\nchallenge, and why it requires a specialized dataset that is different from\nconventional datasets used for automatic speech recognition of full sentences.\nSuggests a methodology for reproducible and comparable accuracy metrics for\nthis task. Describes how the data was collected and verified, what it contains,\nprevious versions and properties. Concludes by reporting baseline results of\nmodels trained on this dataset.", "no": 17}, {"url": "https://arxiv.org/abs/1805.04833", "title": "Hierarchical Neural Story Generation", "cites": "1 464", "abstract": "We explore story generation: creative systems that can build coherent and\nfluent passages of text about a topic. We collect a large dataset of 300K\nhuman-written stories paired with writing prompts from an online forum. Our\ndataset enables hierarchical story generation, where the model first generates\na premise, and then transforms it into a passage of text. We gain further\nimprovements with a novel form of model fusion that improves the relevance of\nthe story to the prompt, and adding a new gated multi-scale self-attention\nmechanism to model long-range context. Experiments show large improvements over\nstrong baselines on both automated and human evaluations. Human judges prefer\nstories generated by our approach to those from a strong non-hierarchical model\nby a factor of two to one.", "no": 18}, {"url": "https://arxiv.org/abs/1803.05355", "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "cites": "1 429", "abstract": "In this paper we introduce a new publicly available dataset for verification\nagainst textual sources, FEVER: Fact Extraction and VERification. It consists\nof 185,445 claims generated by altering sentences extracted from Wikipedia and\nsubsequently verified without knowledge of the sentence they were derived from.\nThe claims are classified as Supported, Refuted or NotEnoughInfo by annotators\nachieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators\nalso recorded the sentence(s) forming the necessary evidence for their\njudgment. To characterize the challenge of the dataset presented, we develop a\npipeline approach and compare it to suitably designed oracles. The best\naccuracy we achieve on labeling a claim accompanied by the correct evidence is\n31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that\nFEVER is a challenging testbed that will help stimulate progress on claim\nverification against textual sources.", "no": 19}, {"url": "https://arxiv.org/abs/1804.00015", "title": "ESPnet: End-to-End Speech Processing Toolkit", "cites": "1 414", "abstract": "This paper introduces a new open source platform for end-to-end speech\nprocessing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech\nrecognition (ASR), and adopts widely-used dynamic neural network toolkits,\nChainer and PyTorch, as a main deep learning engine. ESPnet also follows the\nKaldi ASR toolkit style for data processing, feature extraction/format, and\nrecipes to provide a complete setup for speech recognition and other speech\nprocessing experiments. This paper explains a major architecture of this\nsoftware platform, several important functionalities, which differentiate\nESPnet from other open source ASR toolkits, and experimental results with major\nASR benchmarks.", "no": 20}, {"url": "https://arxiv.org/abs/1811.03604", "title": "Federated Learning for Mobile Keyboard Prediction", "cites": "1 396", "abstract": "We train a recurrent neural network language model using a distributed,\non-device learning framework called federated learning for the purpose of\nnext-word prediction in a virtual keyboard for smartphones. Server-based\ntraining using stochastic gradient descent is compared with training on client\ndevices using the Federated Averaging algorithm. The federated algorithm, which\nenables training on a higher-quality dataset for this use case, is shown to\nachieve better prediction recall. This work demonstrates the feasibility and\nbenefit of training language models on client devices without exporting\nsensitive user data to servers. The federated learning environment gives users\ngreater control over the use of their data and simplifies the task of\nincorporating privacy by default with distributed training and aggregation\nacross a population of client devices.", "no": 21}, {"url": "https://arxiv.org/abs/1811.00937", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "cites": "1 378", "abstract": "When answering a question, people often draw upon their rich world knowledge\nin addition to the particular context. Recent work has focused primarily on\nanswering questions given some relevant document or context, and required very\nlittle general background. To investigate question answering with prior\nknowledge, we present CommonsenseQA: a challenging new dataset for commonsense\nquestion answering. To capture common sense beyond associations, we extract\nfrom ConceptNet (Speer et al., 2017) multiple target concepts that have the\nsame semantic relation to a single source concept. Crowd-workers are asked to\nauthor multiple-choice questions that mention the source concept and\ndiscriminate in turn between each of the target concepts. This encourages\nworkers to create questions with complex semantics that often require prior\nknowledge. We create 12,247 questions through this procedure and demonstrate\nthe difficulty of our task with a large number of strong baselines. Our best\nbaseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,\nwell below human performance, which is 89%.", "no": 22}, {"url": "https://arxiv.org/abs/1801.07243", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "cites": "1 359", "abstract": "Chit-chat models are known to have several problems: they lack specificity,\ndo not display a consistent personality and are often not very captivating. In\nthis work we present the task of making chit-chat more engaging by conditioning\non profile information. We collect data and train models to (i) condition on\ntheir given profile information; and (ii) information about the person they are\ntalking to, resulting in improved dialogues, as measured by next utterance\nprediction. Since (ii) is initially unknown our model is trained to engage its\npartner with personal topics, and we show the resulting dialogue can be used to\npredict profile information about the interlocutors.", "no": 23}, {"url": "https://arxiv.org/abs/1802.06893", "title": "Learning Word Vectors for 157 Languages", "cites": "1 349", "abstract": "Distributed word representations, or word vectors, have recently been applied\nto many tasks in natural language processing, leading to state-of-the-art\nperformance. A key ingredient to the successful application of these\nrepresentations is to train them on very large corpora, and use these\npre-trained models in downstream tasks. In this paper, we describe how we\ntrained such high quality word representations for 157 languages. We used two\nsources of data to train these models: the free online encyclopedia Wikipedia\nand data from the common crawl project. We also introduce three new word\nanalogy datasets to evaluate these word vectors, for French, Hindi and Polish.\nFinally, we evaluate our pre-trained word vectors on 10 languages for which\nevaluation datasets exists, showing very strong performance compared to\nprevious models.", "no": 24}, {"url": "https://arxiv.org/abs/1812.04606", "title": "Deep Anomaly Detection with Outlier Exposure", "cites": "1 338", "abstract": "It is important to detect anomalous inputs when deploying machine learning\nsystems. The use of larger and more complex inputs in deep learning magnifies\nthe difficulty of distinguishing between anomalous and in-distribution\nexamples. At the same time, diverse image and text data are available in\nenormous quantities. We propose leveraging these data to improve deep anomaly\ndetection by training anomaly detectors against an auxiliary dataset of\noutliers, an approach we call Outlier Exposure (OE). This enables anomaly\ndetectors to generalize and detect unseen anomalies. In extensive experiments\non natural language processing and small- and large-scale vision tasks, we find\nthat Outlier Exposure significantly improves detection performance. We also\nobserve that cutting-edge generative models trained on CIFAR-10 may assign\nhigher likelihoods to SVHN images than to CIFAR-10 images; we use OE to\nmitigate this issue. We also analyze the flexibility and robustness of Outlier\nExposure, and identify characteristics of the auxiliary dataset that improve\nperformance.", "no": 25}, {"url": "https://arxiv.org/abs/1805.12471", "title": "Neural Network Acceptability Judgments", "cites": "1 285", "abstract": "This paper investigates the ability of artificial neural networks to judge\nthe grammatical acceptability of a sentence, with the goal of testing their\nlinguistic competence. We introduce the Corpus of Linguistic Acceptability\n(CoLA), a set of 10,657 English sentences labeled as grammatical or\nungrammatical from published linguistics literature. As baselines, we train\nseveral recurrent neural network models on acceptability classification, and\nfind that our models outperform unsupervised models by Lau et al (2016) on\nCoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et\nal.'s models and ours learn systematic generalizations like subject-verb-object\norder. However, all models we test perform far below human level on a wide\nrange of grammatical constructions.", "no": 26}, {"url": "https://arxiv.org/abs/1809.05053", "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "cites": "1 263", "abstract": "State-of-the-art natural language processing systems rely on supervision in\nthe form of annotated data to learn competent models. These models are\ngenerally trained on data in a single language (usually English), and cannot be\ndirectly used beyond that language. Since collecting data in every language is\nnot realistic, there has been a growing interest in cross-lingual language\nunderstanding (XLU) and low-resource cross-language transfer. In this work, we\nconstruct an evaluation set for XLU by extending the development and test sets\nof the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15\nlanguages, including low-resource languages such as Swahili and Urdu. We hope\nthat our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence\nunderstanding by providing an informative standard evaluation task. In\naddition, we provide several baselines for multilingual sentence understanding,\nincluding two based on machine translation systems, and two that use parallel\ndata to train aligned multilingual bag-of-words and LSTM encoders. We find that\nXNLI represents a practical and challenging evaluation suite, and that directly\ntranslating the test data yields the best performance among available\nbaselines.", "no": 27}, {"url": "https://arxiv.org/abs/1803.07640", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "cites": "1 261", "abstract": "This paper describes AllenNLP, a platform for research on deep learning\nmethods in natural language understanding. AllenNLP is designed to support\nresearchers who want to build novel language understanding models quickly and\neasily. It is built on top of PyTorch, allowing for dynamic computation graphs,\nand provides (1) a flexible data API that handles intelligent batching and\npadding, (2) high-level abstractions for common operations in working with\ntext, and (3) a modular and extensible experiment framework that makes doing\ngood science easy. It also includes reference implementations of high quality\napproaches for both core semantic problems (e.g. semantic role labeling (Palmer\net al., 2005)) and language understanding applications (e.g. machine\ncomprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source\neffort maintained by engineers and researchers at the Allen Institute for\nArtificial Intelligence.", "no": 28}, {"url": "https://arxiv.org/abs/1810.00278", "title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\n  Task-Oriented Dialogue Modelling", "cites": "1 235", "abstract": "Even though machine learning has become the major scene in dialogue research\ncommunity, the real breakthrough has been blocked by the scale of data\navailable. To address this fundamental obstacle, we introduce the Multi-Domain\nWizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human\nwritten conversations spanning over multiple domains and topics. At a size of\n$10$k dialogues, it is at least one order of magnitude larger than all previous\nannotated task-oriented corpora. The contribution of this work apart from the\nopen-sourced dataset labelled with dialogue belief states and dialogue actions\nis two-fold: firstly, a detailed description of the data collection procedure\nalong with a summary of data structure and analysis is provided. The proposed\ndata-collection pipeline is entirely based on crowd-sourcing without the need\nof hiring professional annotators; secondly, a set of benchmark results of\nbelief tracking, dialogue act and response generation is reported, which shows\nthe usability of the data and sets a baseline for future studies.", "no": 29}, {"url": "https://arxiv.org/abs/1809.02789", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book\n  Question Answering", "cites": "1 135", "abstract": "We present a new kind of question answering dataset, OpenBookQA, modeled\nafter open book exams for assessing human understanding of a subject. The open\nbook that comes with our questions is a set of 1329 elementary level science\nfacts. Roughly 6000 questions probe an understanding of these facts and their\napplication to novel situations. This requires combining an open book fact\n(e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of\narmor is made of metal) obtained from other sources. While existing QA datasets\nover documents or knowledge bases, being generally self-contained, focus on\nlinguistic understanding, OpenBookQA probes a deeper understanding of both the\ntopic---in the context of common knowledge---and the language it is expressed\nin. Human performance on OpenBookQA is close to 92%, but many state-of-the-art\npre-trained QA methods perform surprisingly poorly, worse than several simple\nneural baselines we develop. Our oracle experiments designed to circumvent the\nknowledge retrieval bottleneck demonstrate the value of both the open book and\nadditional facts. We leave it as a challenge to solve the retrieval problem in\nthis multi-hop setting and to close the large gap to human performance.", "no": 30}, {"url": "https://arxiv.org/abs/1803.02324", "title": "Annotation Artifacts in Natural Language Inference Data", "cites": "1 122", "abstract": "Large-scale datasets for natural language inference are created by presenting\ncrowd workers with a sentence (premise), and asking them to generate three new\nsentences (hypotheses) that it entails, contradicts, or is logically neutral\nwith respect to. We show that, in a significant portion of such data, this\nprotocol leaves clues that make it possible to identify the label by looking\nonly at the hypothesis, without observing the premise. Specifically, we show\nthat a simple text categorization model can correctly classify the hypothesis\nalone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams\net. al, 2017). Our analysis reveals that specific linguistic phenomena such as\nnegation and vagueness are highly correlated with certain inference classes.\nOur findings suggest that the success of natural language inference models to\ndate has been overestimated, and that the task remains a hard open problem.", "no": 31}, {"url": "https://arxiv.org/abs/1804.10959", "title": "Subword Regularization: Improving Neural Network Translation Models with\n  Multiple Subword Candidates", "cites": "1 092", "abstract": "Subword units are an effective way to alleviate the open vocabulary problems\nin neural machine translation (NMT). While sentences are usually converted into\nunique subword sequences, subword segmentation is potentially ambiguous and\nmultiple segmentations are possible even with the same vocabulary. The question\naddressed in this paper is whether it is possible to harness the segmentation\nambiguity as a noise to improve the robustness of NMT. We present a simple\nregularization method, subword regularization, which trains the model with\nmultiple subword segmentations probabilistically sampled during training. In\naddition, for better subword sampling, we propose a new subword segmentation\nalgorithm based on a unigram language model. We experiment with multiple\ncorpora and report consistent improvements especially on low resource and\nout-of-domain settings.", "no": 32}, {"url": "https://arxiv.org/abs/1808.07042", "title": "CoQA: A Conversational Question Answering Challenge", "cites": "1 089", "abstract": "Humans gather information by engaging in conversations involving a series of\ninterconnected questions and answers. For machines to assist in information\ngathering, it is therefore essential to enable them to answer conversational\nquestions. We introduce CoQA, a novel dataset for building Conversational\nQuestion Answering systems. Our dataset contains 127k questions with answers,\nobtained from 8k conversations about text passages from seven diverse domains.\nThe questions are conversational, and the answers are free-form text with their\ncorresponding evidence highlighted in the passage. We analyze CoQA in depth and\nshow that conversational questions have challenging phenomena not present in\nexisting reading comprehension datasets, e.g., coreference and pragmatic\nreasoning. We evaluate strong conversational and reading comprehension models\non CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points\nbehind human performance (88.8%), indicating there is ample room for\nimprovement. We launch CoQA as a challenge to the community at\nhttp://stanfordnlp.github.io/coqa/", "no": 33}, {"url": "https://arxiv.org/abs/1804.09541", "title": "QANet: Combining Local Convolution with Global Self-Attention for\n  Reading Comprehension", "cites": "1 066", "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are\nprimarily based on recurrent neural networks (RNNs) with attention. Despite\ntheir success, these models are often slow for both training and inference due\nto the sequential nature of RNNs. We propose a new Q\\&A architecture called\nQANet, which does not require recurrent networks: Its encoder consists\nexclusively of convolution and self-attention, where convolution models local\ninteractions and self-attention models global interactions. On the SQuAD\ndataset, our model is 3x to 13x faster in training and 4x to 9x faster in\ninference, while achieving equivalent accuracy to recurrent models. The\nspeed-up gain allows us to train the model with much more data. We hence\ncombine our model with data generated by backtranslation from a neural machine\ntranslation model. On the SQuAD dataset, our single model, trained with\naugmented data, achieves 84.6 F1 score on the test set, which is significantly\nbetter than the best published F1 score of 81.8.", "no": 34}, {"url": "https://arxiv.org/abs/1812.09449", "title": "A Survey on Deep Learning for Named Entity Recognition", "cites": "1 031", "abstract": "Named entity recognition (NER) is the task to identify mentions of rigid\ndesignators from text belonging to predefined semantic types such as person,\nlocation, organization etc. NER always serves as the foundation for many\nnatural language applications such as question answering, text summarization,\nand machine translation. Early NER systems got a huge success in achieving good\nperformance with the cost of human engineering in designing domain-specific\nfeatures and rules. In recent years, deep learning, empowered by continuous\nreal-valued vector representations and semantic composition through nonlinear\nprocessing, has been employed in NER systems, yielding stat-of-the-art\nperformance. In this paper, we provide a comprehensive review on existing deep\nlearning techniques for NER. We first introduce NER resources, including tagged\nNER corpora and off-the-shelf NER tools. Then, we systematically categorize\nexisting works based on a taxonomy along three axes: distributed\nrepresentations for input, context encoder, and tag decoder. Next, we survey\nthe most representative methods for recent applied techniques of deep learning\nin new NER problem settings and applications. Finally, we present readers with\nthe challenges faced by NER systems and outline future directions in this area.", "no": 35}, {"url": "https://arxiv.org/abs/1809.08887", "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain\n  Semantic Parsing and Text-to-SQL Task", "cites": "1 001", "abstract": "We present Spider, a large-scale, complex and cross-domain semantic parsing\nand text-to-SQL dataset annotated by 11 college students. It consists of 10,181\nquestions and 5,693 unique complex SQL queries on 200 databases with multiple\ntables, covering 138 different domains. We define a new complex and\ncross-domain semantic parsing and text-to-SQL task where different complex SQL\nqueries and databases appear in train and test sets. In this way, the task\nrequires the model to generalize well to both new SQL queries and new database\nschemas. Spider is distinct from most of the previous semantic parsing tasks\nbecause they all use a single database and the exact same programs in the train\nset and the test set. We experiment with various state-of-the-art models and\nthe best model achieves only 12.4% exact matching accuracy on a database split\nsetting. This shows that Spider presents a strong challenge for future\nresearch. Our dataset and task are publicly available at\nhttps://yale-lily.github.io/spider", "no": 36}, {"url": "https://arxiv.org/abs/1812.10464", "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual\n  Transfer and Beyond", "cites": "938", "abstract": "We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER", "no": 37}, {"url": "https://arxiv.org/abs/1810.02508", "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in\n  Conversations", "cites": "901", "abstract": "Emotion recognition in conversations is a challenging task that has recently\ngained popularity due to its potential applications. Until now, however, a\nlarge-scale multimodal multi-party emotional conversational database containing\nmore than two speakers per dialogue was missing. Thus, we propose the\nMultimodal EmotionLines Dataset (MELD), an extension and enhancement of\nEmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from\nthe TV-series Friends. Each utterance is annotated with emotion and sentiment\nlabels, and encompasses audio, visual and textual modalities. We propose\nseveral strong multimodal baselines and show the importance of contextual and\nmultimodal information for emotion recognition in conversations. The full\ndataset is available for use at http:// affective-meld.github.io.", "no": 38}, {"url": "https://arxiv.org/abs/1811.01241", "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents", "cites": "892", "abstract": "In open-domain dialogue intelligent agents should exhibit the use of\nknowledge, however there are few convincing demonstrations of this to date. The\nmost popular sequence to sequence models typically \"generate and hope\" generic\nutterances that can be memorized in the weights of the model when mapping from\ninput utterance(s) to output, rather than employing recalled knowledge as\ncontext. Use of knowledge has so far proved difficult, in part because of the\nlack of a supervised learning benchmark task which exhibits knowledgeable open\ndialogue with clear grounding. To that end we collect and release a large\ndataset with conversations directly grounded with knowledge retrieved from\nWikipedia. We then design architectures capable of retrieving knowledge,\nreading and conditioning on it, and finally generating natural responses. Our\nbest performing dialogue models are able to conduct knowledgeable discussions\non open-domain topics as evaluated by automatic metrics and human evaluations,\nwhile our new benchmark allows for measuring further improvements in this\nimportant research direction.", "no": 39}, {"url": "https://arxiv.org/abs/1804.07998", "title": "Generating Natural Language Adversarial Examples", "cites": "889", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples,\nperturbations to correctly classified examples which can cause the model to\nmisclassify. In the image domain, these perturbations are often virtually\nindistinguishable to human perception, causing humans and state-of-the-art\nmodels to disagree. However, in the natural language domain, small\nperturbations are clearly perceptible, and the replacement of a single word can\ndrastically alter the semantics of the document. Given these challenges, we use\na black-box population-based optimization algorithm to generate semantically\nand syntactically similar adversarial examples that fool well-trained sentiment\nanalysis and textual entailment models with success rates of 97% and 70%,\nrespectively. We additionally demonstrate that 92.3% of the successful\nsentiment analysis adversarial examples are classified to their original label\nby 20 human annotators, and that the examples are perceptibly quite similar.\nFinally, we discuss an attempt to use adversarial training as a defense, but\nfail to yield improvement, demonstrating the strength and diversity of our\nadversarial examples. We hope our findings encourage researchers to pursue\nimproving the robustness of DNNs in the natural language domain.", "no": 40}, {"url": "https://arxiv.org/abs/1805.01070", "title": "What you can cram into a single vector: Probing sentence embeddings for\n  linguistic properties", "cites": "849", "abstract": "Although much effort has recently been devoted to training high-quality\nsentence embeddings, we still have a poor understanding of what they are\ncapturing. \"Downstream\" tasks, often based on sentence classification, are\ncommonly used to evaluate the quality of sentence representations. The\ncomplexity of the tasks makes it however difficult to infer what kind of\ninformation is present in the representations. We introduce here 10 probing\ntasks designed to capture simple linguistic features of sentences, and we use\nthem to study embeddings generated by three different encoders trained in eight\ndistinct ways, uncovering intriguing properties of both encoders and training\nmethods.", "no": 41}, {"url": "https://arxiv.org/abs/1804.06876", "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods", "cites": "840", "abstract": "We introduce a new benchmark, WinoBias, for coreference resolution focused on\ngender bias. Our corpus contains Winograd-schema style sentences with entities\ncorresponding to people referred by their occupation (e.g. the nurse, the\ndoctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a\nneural coreference system all link gendered pronouns to pro-stereotypical\nentities with higher accuracy than anti-stereotypical entities, by an average\ndifference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation\napproach that, in combination with existing word-embedding debiasing\ntechniques, removes the bias demonstrated by these systems in WinoBias without\nsignificantly affecting their performance on existing coreference benchmark\ndatasets. Our dataset and code are available at http://winobias.org.", "no": 42}, {"url": "https://arxiv.org/abs/1805.07932", "title": "Bilinear Attention Networks", "cites": "814", "abstract": "Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets.", "no": 43}, {"url": "https://arxiv.org/abs/1811.10830", "title": "From Recognition to Cognition: Visual Commonsense Reasoning", "cites": "814", "abstract": "Visual understanding goes well beyond object recognition. With one glance at\nan image, we can effortlessly imagine the world beyond the pixels: for\ninstance, we can infer people's actions, goals, and mental states. While this\ntask is easy for humans, it is tremendously difficult for today's vision\nsystems, requiring higher-order cognition and commonsense reasoning about the\nworld. We formalize this task as Visual Commonsense Reasoning. Given a\nchallenging question about an image, a machine must answer correctly and then\nprovide a rationale justifying its answer.\n  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA\nproblems derived from 110k movie scenes. The key recipe for generating\nnon-trivial and high-quality problems at scale is Adversarial Matching, a new\napproach to transform rich annotations into multiple choice questions with\nminimal bias. Experimental results show that while humans find VCR easy (over\n90% accuracy), state-of-the-art vision models struggle (~45%).\n  To move towards cognition-level understanding, we present a new reasoning\nengine, Recognition to Cognition Networks (R2C), that models the necessary\nlayered inferences for grounding, contextualization, and reasoning. R2C helps\nnarrow the gap between humans and machines (~65%); still, the challenge is far\nfrom solved, and we provide analysis that suggests avenues for future work.", "no": 44}, {"url": "https://arxiv.org/abs/1805.10190", "title": "Snips Voice Platform: an embedded Spoken Language Understanding system\n  for private-by-design voice interfaces", "cites": "792", "abstract": "This paper presents the machine learning architecture of the Snips Voice\nPlatform, a software solution to perform Spoken Language Understanding on\nmicroprocessors typical of IoT devices. The embedded inference is fast and\naccurate while enforcing privacy by design, as no personal user data is ever\ncollected. Focusing on Automatic Speech Recognition and Natural Language\nUnderstanding, we detail our approach to training high-performance Machine\nLearning models that are small enough to run in real-time on small devices.\nAdditionally, we describe a data generation procedure that provides sufficient,\nhigh-quality training data without compromising user privacy.", "no": 45}, {"url": "https://arxiv.org/abs/1803.09017", "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in\n  End-to-End Speech Synthesis", "cites": "779", "abstract": "In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings\nthat are jointly trained within Tacotron, a state-of-the-art end-to-end speech\nsynthesis system. The embeddings are trained with no explicit labels, yet learn\nto model a large range of acoustic expressiveness. GSTs lead to a rich set of\nsignificant results. The soft interpretable \"labels\" they generate can be used\nto control synthesis in novel ways, such as varying speed and speaking style -\nindependently of the text content. They can also be used for style transfer,\nreplicating the speaking style of a single audio clip across an entire\nlong-form text corpus. When trained on noisy, unlabeled found data, GSTs learn\nto factorize noise and speaker identity, providing a path towards highly\nscalable but robust speech synthesis.", "no": 46}, {"url": "https://arxiv.org/abs/1808.07036", "title": "QuAC : Question Answering in Context", "cites": "779", "abstract": "We present QuAC, a dataset for Question Answering in Context that contains\n14K information-seeking QA dialogs (100K questions in total). The dialogs\ninvolve two crowd workers: (1) a student who poses a sequence of freeform\nquestions to learn as much as possible about a hidden Wikipedia text, and (2) a\nteacher who answers the questions by providing short excerpts from the text.\nQuAC introduces challenges not found in existing machine comprehension\ndatasets: its questions are often more open-ended, unanswerable, or only\nmeaningful within the dialog context, as we show in a detailed qualitative\nevaluation. We also report results for a number of reference models, including\na recently state-of-the-art reading comprehension architecture extended to\nmodel dialog context. Our best model underperforms humans by 20 F1, suggesting\nthat there is significant room for future work on this data. Dataset, baseline,\nand leaderboard available at http://quac.ai.", "no": 47}, {"url": "https://arxiv.org/abs/1806.04558", "title": "Transfer Learning from Speaker Verification to Multispeaker\n  Text-To-Speech Synthesis", "cites": "776", "abstract": "We describe a neural network-based system for text-to-speech (TTS) synthesis\nthat is able to generate speech audio in the voice of many different speakers,\nincluding those unseen during training. Our system consists of three\nindependently trained components: (1) a speaker encoder network, trained on a\nspeaker verification task using an independent dataset of noisy speech from\nthousands of speakers without transcripts, to generate a fixed-dimensional\nembedding vector from seconds of reference speech from a target speaker; (2) a\nsequence-to-sequence synthesis network based on Tacotron 2, which generates a\nmel spectrogram from text, conditioned on the speaker embedding; (3) an\nauto-regressive WaveNet-based vocoder that converts the mel spectrogram into a\nsequence of time domain waveform samples. We demonstrate that the proposed\nmodel is able to transfer the knowledge of speaker variability learned by the\ndiscriminatively-trained speaker encoder to the new task, and is able to\nsynthesize natural speech from speakers that were not seen during training. We\nquantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we\nshow that randomly sampled speaker embeddings can be used to synthesize speech\nin the voice of novel speakers dissimilar from those used in training,\nindicating that the model has learned a high quality speaker representation.", "no": 48}, {"url": "https://arxiv.org/abs/1801.08186", "title": "MAttNet: Modular Attention Network for Referring Expression\n  Comprehension", "cites": "774", "abstract": "In this paper, we address referring expression comprehension: localizing an\nimage region described by a natural language expression. While most recent work\ntreats expressions as a single unit, we propose to decompose them into three\nmodular components related to subject appearance, location, and relationship to\nother objects. This allows us to flexibly adapt to expressions containing\ndifferent types of information in an end-to-end framework. In our model, which\nwe call the Modular Attention Network (MAttNet), two types of attention are\nutilized: language-based attention that learns the module weights as well as\nthe word/phrase attention that each module should focus on; and visual\nattention that allows the subject and relationship modules to focus on relevant\nimage components. Module weights combine scores from all three modules\ndynamically to output an overall score. Experiments show that MAttNet\noutperforms previous state-of-art methods by a large margin on both\nbounding-box-level and pixel-level comprehension tasks. Demo and code are\nprovided.", "no": 49}, {"url": "https://arxiv.org/abs/1801.10198", "title": "Generating Wikipedia by Summarizing Long Sequences", "cites": "762", "abstract": "We show that generating English Wikipedia articles can be approached as a\nmulti- document summarization of source documents. We use extractive\nsummarization to coarsely identify salient information and a neural abstractive\nmodel to generate the article. For the abstractive model, we introduce a\ndecoder-only architecture that can scalably attend to very long sequences, much\nlonger than typical encoder- decoder architectures used in sequence\ntransduction. We show that this model can generate fluent, coherent\nmulti-sentence paragraphs and even whole Wikipedia articles. When given\nreference documents, we show it can extract relevant factual information as\nreflected in perplexity, ROUGE scores and human evaluations.", "no": 50}, {"url": "https://arxiv.org/abs/1807.03819", "title": "Universal Transformers", "cites": "707", "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for\nsequence modeling tasks. However, their inherently sequential computation makes\nthem slow to train. Feed-forward and convolutional architectures have recently\nbeen shown to achieve superior results on some sequence modeling tasks such as\nmachine translation, with the added advantage that they concurrently process\nall inputs in the sequence, leading to easy parallelization and faster training\ntimes. Despite these successes, however, popular feed-forward sequence models\nlike the Transformer fail to generalize in many simple tasks that recurrent\nmodels handle with ease, e.g. copying strings or even simple logical inference\nwhen the string or formula lengths exceed those observed at training time. We\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\nrecurrent sequence model which can be cast as a generalization of the\nTransformer model and which addresses these issues. UTs combine the\nparallelizability and global receptive field of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\ndynamic per-position halting mechanism and find that it improves accuracy on\nseveral tasks. In contrast to the standard Transformer, under certain\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and\nlanguage understanding tasks, including the challenging LAMBADA language\nmodeling task where UTs achieve a new state of the art, and machine translation\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\ndataset.", "no": 51}, {"url": "https://arxiv.org/abs/1809.10185", "title": "Graph Convolution over Pruned Dependency Trees Improves Relation\n  Extraction", "cites": "699", "abstract": "Dependency trees help relation extraction models capture long-range relations\nbetween words. However, existing dependency-based models either neglect crucial\ninformation (e.g., negation) by pruning the dependency trees too aggressively,\nor are computationally inefficient because it is difficult to parallelize over\ndifferent tree structures. We propose an extension of graph convolutional\nnetworks that is tailored for relation extraction, which pools information over\narbitrary dependency structures efficiently in parallel. To incorporate\nrelevant information while maximally removing irrelevant content, we further\napply a novel pruning strategy to the input trees by keeping words immediately\naround the shortest path between the two entities among which a relation might\nhold. The resulting model achieves state-of-the-art performance on the\nlarge-scale TACRED dataset, outperforming existing sequence and\ndependency-based neural models. We also show through detailed analysis that\nthis model has complementary strengths to sequence models, and combining them\nfurther improves the state of the art.", "no": 52}, {"url": "https://arxiv.org/abs/1804.06059", "title": "Adversarial Example Generation with Syntactically Controlled Paraphrase\n  Networks", "cites": "684", "abstract": "We propose syntactically controlled paraphrase networks (SCPNs) and use them\nto generate adversarial examples. Given a sentence and a target syntactic form\n(e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the\nsentence with the desired syntax. We show it is possible to create training\ndata for this task by first doing backtranslation at a very large scale, and\nthen using a parser to label the syntactic transformations that naturally occur\nduring this process. Such data allows us to train a neural encoder-decoder\nmodel with extra inputs to specify the target syntax. A combination of\nautomated and human evaluations show that SCPNs generate paraphrases that\nfollow their target specifications without decreasing paraphrase quality when\ncompared to baseline (uncontrolled) paraphrase systems. Furthermore, they are\nmore capable of generating syntactically adversarial examples that both (1)\n\"fool\" pretrained models and (2) improve the robustness of these models to\nsyntactic variation when used to augment their training data.", "no": 53}, {"url": "https://arxiv.org/abs/1808.05326", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense\n  Inference", "cites": "682", "abstract": "Given a partial description like \"she opened the hood of the car,\" humans can\nreason about the situation and anticipate what might come next (\"then, she\nexamined the engine\"). In this paper, we introduce the task of grounded\ncommonsense inference, unifying natural language inference and commonsense\nreasoning.\n  We present SWAG, a new dataset with 113k multiple choice questions about a\nrich spectrum of grounded situations. To address the recurring challenges of\nthe annotation artifacts and human biases found in many existing datasets, we\npropose Adversarial Filtering (AF), a novel procedure that constructs a\nde-biased dataset by iteratively training an ensemble of stylistic classifiers,\nand using them to filter the data. To account for the aggressive adversarial\nfiltering, we use state-of-the-art language models to massively oversample a\ndiverse set of potential counterfactuals. Empirical results demonstrate that\nwhile humans can solve the resulting inference problems with high accuracy\n(88%), various competitive models struggle on our task. We provide\ncomprehensive analysis that indicates significant opportunities for future\nresearch.", "no": 54}, {"url": "https://arxiv.org/abs/1804.00344", "title": "Marian: Fast Neural Machine Translation in C++", "cites": "681", "abstract": "We present Marian, an efficient and self-contained Neural Machine Translation\nframework with an integrated automatic differentiation engine based on dynamic\ncomputation graphs. Marian is written entirely in C++. We describe the design\nof the encoder-decoder framework and demonstrate that a research-friendly\ntoolkit can achieve high training and translation speed.", "no": 55}, {"url": "https://arxiv.org/abs/1812.05271", "title": "TextBugger: Generating Adversarial Text Against Real-world Applications", "cites": "680", "abstract": "Deep Learning-based Text Understanding (DLTU) is the backbone technique\nbehind various applications, including question answering, machine translation,\nand text classification. Despite its tremendous popularity, the security\nvulnerabilities of DLTU are still largely unknown, which is highly concerning\ngiven its increasing use in security-sensitive applications such as sentiment\nanalysis and toxic content detection. In this paper, we show that DLTU is\ninherently vulnerable to adversarial text attacks, in which maliciously crafted\ntexts trigger target DLTU systems and services to misbehave. Specifically, we\npresent TextBugger, a general attack framework for generating adversarial\ntexts. In contrast to prior works, TextBugger differs in significant ways: (i)\neffective -- it outperforms state-of-the-art attacks in terms of attack success\nrate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of\nthe adversarial text correctly recognized by human readers; and (iii) efficient\n-- it generates adversarial text with computational complexity sub-linear to\nthe text length. We empirically evaluate TextBugger on a set of real-world DLTU\nsystems and services used for sentiment analysis and toxic content detection,\ndemonstrating its effectiveness, evasiveness, and efficiency. For instance,\nTextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS\nComprehend within 4.61 seconds and preserves 97\\% semantic similarity. We\nfurther discuss possible defense mechanisms to mitigate such attack and the\nadversary's potential countermeasures, which leads to promising directions for\nfurther research.", "no": 56}, {"url": "https://arxiv.org/abs/1804.05685", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long\n  Documents", "cites": "677", "abstract": "Neural abstractive summarization models have led to promising results in\nsummarizing relatively short documents. We propose the first model for\nabstractive summarization of single, longer-form documents (e.g., research\npapers). Our approach consists of a new hierarchical encoder that models the\ndiscourse structure of a document, and an attentive discourse-aware decoder to\ngenerate the summary. Empirical results on two large-scale datasets of\nscientific papers show that our model significantly outperforms\nstate-of-the-art models.", "no": 57}, {"url": "https://arxiv.org/abs/1804.07755", "title": "Phrase-Based & Neural Unsupervised Machine Translation", "cites": "673", "abstract": "Machine translation systems achieve near human-level performance on some\nlanguages, yet their effectiveness strongly relies on the availability of large\namounts of parallel sentences, which hinders their applicability to the\nmajority of language pairs. This work investigates how to learn to translate\nwhen having access to only large monolingual corpora in each language. We\npropose two model variants, a neural and a phrase-based model. Both versions\nleverage a careful initialization of the parameters, the denoising effect of\nlanguage models and automatic generation of parallel data by iterative\nback-translation. These models are significantly better than methods from the\nliterature, while being simpler and having fewer hyper-parameters. On the\nwidely used WMT'14 English-French and WMT'16 German-English benchmarks, our\nmodels respectively obtain 28.1 and 25.2 BLEU points without using a single\nparallel sentence, outperforming the state of the art by more than 11 BLEU\npoints. On low-resource languages like English-Urdu and English-Romanian, our\nmethods achieve even better results than semi-supervised and supervised\napproaches leveraging the paucity of available bitexts. Our code for NMT and\nPBSMT is publicly available.", "no": 58}, {"url": "https://arxiv.org/abs/1808.10792", "title": "Bottom-Up Abstractive Summarization", "cites": "673", "abstract": "Neural network-based methods for abstractive summarization produce outputs\nthat are more fluent than other techniques, but which can be poor at content\nselection. This work proposes a simple technique for addressing this issue: use\na data-efficient content selector to over-determine phrases in a source\ndocument that should be part of the summary. We use this selector as a\nbottom-up attention step to constrain the model to likely phrases. We show that\nthis approach improves the ability to compress text, while still generating\nfluent summaries. This two-step process is both simpler and higher performing\nthan other end-to-end content selection models, leading to significant\nimprovements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the\ncontent selector can be trained with as little as 1,000 sentences, making it\neasy to transfer a trained summarizer to a new domain.", "no": 59}, {"url": "https://arxiv.org/abs/1802.08218", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People", "cites": "663", "abstract": "The study of algorithms to automatically answer visual questions currently is\nmotivated by visual question answering (VQA) datasets constructed in artificial\nVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising\nfrom a natural VQA setting. VizWiz consists of over 31,000 visual questions\noriginating from blind people who each took a picture using a mobile phone and\nrecorded a spoken question about it, together with 10 crowdsourced answers per\nvisual question. VizWiz differs from the many existing VQA datasets because (1)\nimages are captured by blind photographers and so are often poor quality, (2)\nquestions are spoken and so are more conversational, and (3) often visual\nquestions cannot be answered. Evaluation of modern algorithms for answering\nvisual questions and deciding if a visual question is answerable reveals that\nVizWiz is a challenging dataset. We introduce this dataset to encourage a\nlarger community to develop more generalized algorithms that can assist blind\npeople.", "no": 60}, {"url": "https://arxiv.org/abs/1801.04354", "title": "Black-box Generation of Adversarial Text Sequences to Evade Deep\n  Learning Classifiers", "cites": "656", "abstract": "Although various techniques have been proposed to generate adversarial\nsamples for white-box attacks on text, little attention has been paid to\nblack-box attacks, which are more realistic scenarios. In this paper, we\npresent a novel algorithm, DeepWordBug, to effectively generate small text\nperturbations in a black-box setting that forces a deep-learning classifier to\nmisclassify a text input. We employ novel scoring strategies to identify the\ncritical tokens that, if modified, cause the classifier to make an incorrect\nprediction. Simple character-level transformations are applied to the\nhighest-ranked tokens in order to minimize the edit distance of the\nperturbation, yet change the original classification. We evaluated DeepWordBug\non eight real-world text datasets, including text classification, sentiment\nanalysis, and spam detection. We compare the result of DeepWordBug with two\nbaselines: Random (Black-box) and Gradient (White-box). Our experimental\nresults indicate that DeepWordBug reduces the prediction accuracy of current\nstate-of-the-art deep-learning models, including a decrease of 68\\% on average\nfor a Word-LSTM model and 48\\% on average for a Char-CNN model.", "no": 61}, {"url": "https://arxiv.org/abs/1809.08267", "title": "Neural Approaches to Conversational AI", "cites": "648", "abstract": "The present paper surveys neural approaches to conversational AI that have\nbeen developed in the last few years. We group conversational systems into\nthree categories: (1) question answering agents, (2) task-oriented dialogue\nagents, and (3) chatbots. For each category, we present a review of\nstate-of-the-art neural approaches, draw the connection between them and\ntraditional approaches, and discuss the progress that has been made and\nchallenges still being faced, using specific systems and models as case\nstudies.", "no": 62}, {"url": "https://arxiv.org/abs/1811.00405", "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations", "cites": "644", "abstract": "Emotion detection in conversations is a necessary step for a number of\napplications, including opinion mining over chat history, social media threads,\ndebates, argumentation mining, understanding consumer feedback in live\nconversations, etc. Currently, systems do not treat the parties in the\nconversation individually by adapting to the speaker of each utterance. In this\npaper, we describe a new method based on recurrent neural networks that keeps\ntrack of the individual party states throughout the conversation and uses this\ninformation for emotion classification. Our model outperforms the state of the\nart by a significant margin on two different datasets.", "no": 63}, {"url": "https://arxiv.org/abs/1808.09602", "title": "Multi-Task Identification of Entities, Relations, and Coreference for\n  Scientific Knowledge Graph Construction", "cites": "631", "abstract": "We introduce a multi-task setup of identifying and classifying entities,\nrelations, and coreference clusters in scientific articles. We create SciERC, a\ndataset that includes annotations for all three tasks and develop a unified\nframework called Scientific Information Extractor (SciIE) for with shared span\nrepresentations. The multi-task setup reduces cascading errors between tasks\nand leverages cross-sentence relations through coreference links. Experiments\nshow that our multi-task model outperforms previous models in scientific\ninformation extraction without using any domain-specific features. We further\nshow that the framework supports construction of a scientific knowledge graph,\nwhich we use to analyze information in scientific literature.", "no": 64}, {"url": "https://arxiv.org/abs/1805.02023", "title": "Chinese NER Using Lattice LSTM", "cites": "627", "abstract": "We investigate a lattice-structured LSTM model for Chinese NER, which encodes\na sequence of input characters as well as all potential words that match a\nlexicon. Compared with character-based methods, our model explicitly leverages\nword and word sequence information. Compared with word-based methods, lattice\nLSTM does not suffer from segmentation errors. Gated recurrent cells allow our\nmodel to choose the most relevant characters and words from a sentence for\nbetter NER results. Experiments on various datasets show that lattice LSTM\noutperforms both word-based and character-based LSTM baselines, achieving the\nbest results.", "no": 65}, {"url": "https://arxiv.org/abs/1809.04206", "title": "Temporal Pattern Attention for Multivariate Time Series Forecasting", "cites": "626", "abstract": "Forecasting multivariate time series data, such as prediction of electricity\nconsumption, solar power production, and polyphonic piano pieces, has numerous\nvaluable applications. However, complex and non-linear interdependencies\nbetween time steps and series complicate the task. To obtain accurate\nprediction, it is crucial to model long-term dependency in time series data,\nwhich can be achieved to some good extent by recurrent neural network (RNN)\nwith attention mechanism. Typical attention mechanism reviews the information\nat each previous time step and selects the relevant information to help\ngenerate the outputs, but it fails to capture the temporal patterns across\nmultiple time steps. In this paper, we propose to use a set of filters to\nextract time-invariant temporal patterns, which is similar to transforming time\nseries data into its \"frequency domain\". Then we proposed a novel attention\nmechanism to select relevant time series, and use its \"frequency domain\"\ninformation for forecasting. We applied the proposed model on several\nreal-world tasks and achieved state-of-the-art performance in all of them with\nonly one exception.", "no": 66}, {"url": "https://arxiv.org/abs/1806.08730", "title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "cites": "614", "abstract": "Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN's\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP.", "no": 67}, {"url": "https://arxiv.org/abs/1802.01886", "title": "Texygen: A Benchmarking Platform for Text Generation Models", "cites": "613", "abstract": "We introduce Texygen, a benchmarking platform to support research on\nopen-domain text generation models. Texygen has not only implemented a majority\nof text generation models, but also covered a set of metrics that evaluate the\ndiversity, the quality and the consistency of the generated texts. The Texygen\nplatform could help standardize the research on text generation and facilitate\nthe sharing of fine-tuned open-source implementations among researchers for\ntheir work. As a consequence, this would help in improving the reproductivity\nand reliability of future research work in text generation.", "no": 68}, {"url": "https://arxiv.org/abs/1803.05449", "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations", "cites": "612", "abstract": "We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.", "no": 69}, {"url": "https://arxiv.org/abs/1811.06621", "title": "Streaming End-to-end Speech Recognition For Mobile Devices", "cites": "611", "abstract": "End-to-end (E2E) models, which directly predict output character sequences\ngiven input speech, are good candidates for on-device speech recognition. E2E\nmodels, however, present numerous challenges: In order to be truly useful, such\nmodels must decode speech utterances in a streaming fashion, in real time; they\nmust be robust to the long tail of use cases; they must be able to leverage\nuser-specific context (e.g., contact lists); and above all, they must be\nextremely accurate. In this work, we describe our efforts at building an E2E\nspeech recognizer using a recurrent neural network transducer. In experimental\nevaluations, we find that the proposed approach can outperform a conventional\nCTC-based model in terms of both latency and accuracy in a number of evaluation\ncategories.", "no": 70}, {"url": "https://arxiv.org/abs/1803.05567", "title": "Achieving Human Parity on Automatic Chinese to English News Translation", "cites": "595", "abstract": "Machine translation has made rapid advances in recent years. Millions of\npeople are using it today in online translation systems and mobile applications\nin order to communicate across language barriers. The question naturally arises\nwhether such systems can approach or achieve parity with human translations. In\nthis paper, we first address the problem of how to define and accurately\nmeasure human parity in translation. We then describe Microsoft's machine\ntranslation system and measure the quality of its translations on the widely\nused WMT 2017 news translation task from Chinese to English. We find that our\nlatest neural machine translation system has reached a new state-of-the-art,\nand that the translation quality is at human parity when compared to\nprofessional human translations. We also find that it significantly exceeds the\nquality of crowd-sourced non-professional translations.", "no": 71}, {"url": "https://arxiv.org/abs/1806.00187", "title": "Scaling Neural Machine Translation", "cites": "593", "abstract": "Sequence to sequence learning models still require several days to reach\nstate of the art performance on large benchmark datasets using a single\nmachine. This paper shows that reduced precision and large batch training can\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\nimplementation. On WMT'14 English-German translation, we match the accuracy of\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\nfurther improve these results to 29.8 BLEU by training on the much larger\nParacrawl dataset. On the WMT'14 English-French task, we obtain a\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.", "no": 72}, {"url": "https://arxiv.org/abs/1804.09301", "title": "Gender Bias in Coreference Resolution", "cites": "589", "abstract": "We present an empirical study of gender bias in coreference resolution\nsystems. We first introduce a novel, Winograd schema-style set of minimal pair\nsentences that differ only by pronoun gender. With these \"Winogender schemas,\"\nwe evaluate and confirm systematic gender bias in three publicly-available\ncoreference resolution systems, and correlate this bias with real-world and\ntextual gender statistics.", "no": 73}, {"url": "https://arxiv.org/abs/1805.06201", "title": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic\n  Relations", "cites": "589", "abstract": "We propose a novel data augmentation for labeled sentences called contextual\naugmentation. We assume an invariance that sentences are natural even if the\nwords in the sentences are replaced with other words with paradigmatic\nrelations. We stochastically replace words with other words that are predicted\nby a bi-directional language model at the word positions. Words predicted\naccording to a context are numerous but appropriate for the augmentation of the\noriginal words. Furthermore, we retrofit a language model with a\nlabel-conditional architecture, which allows the model to augment sentences\nwithout breaking the label-compatibility. Through the experiments for six\nvarious different text classification tasks, we demonstrate that the proposed\nmethod improves classifiers based on the convolutional or recurrent neural\nnetworks.", "no": 74}, {"url": "https://arxiv.org/abs/1812.01193", "title": "e-SNLI: Natural Language Inference with Natural Language Explanations", "cites": "576", "abstract": "In order for machine learning to garner widespread public adoption, models\nmust be able to provide interpretable and robust explanations for their\ndecisions, as well as learn from human-provided explanations at train time. In\nthis work, we extend the Stanford Natural Language Inference dataset with an\nadditional layer of human-annotated natural language explanations of the\nentailment relations. We further implement models that incorporate these\nexplanations into their training process and output them at test time. We show\nhow our corpus of explanations, which we call e-SNLI, can be used for various\ngoals, such as obtaining full sentence justifications of a model's decisions,\nimproving universal sentence representations and transferring to out-of-domain\nNLI datasets. Our dataset thus opens up a range of research directions for\nusing natural language explanations, both for improving models and for\nasserting their trust.", "no": 75}, {"url": "https://arxiv.org/abs/1805.11080", "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence\n  Rewriting", "cites": "574", "abstract": "Inspired by how humans summarize long documents, we propose an accurate and\nfast summarization model that first selects salient sentences and then rewrites\nthem abstractively (i.e., compresses and paraphrases) to generate a concise\noverall summary. We use a novel sentence-level policy gradient method to bridge\nthe non-differentiable computation between these two neural networks in a\nhierarchical way, while maintaining language fluency. Empirically, we achieve\nthe new state-of-the-art on all metrics (including human evaluation) on the\nCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.\nMoreover, by first operating at the sentence-level and then the word-level, we\nenable parallel decoding of our neural generative model that results in\nsubstantially faster (10-20x) inference speed as well as 4x faster training\nconvergence than previous long-paragraph encoder-decoder models. We also\ndemonstrate the generalization of our model on the test-only DUC-2002 dataset,\nwhere we achieve higher scores than a state-of-the-art model.", "no": 76}, {"url": "https://arxiv.org/abs/1809.01696", "title": "TVQA: Localized, Compositional Video Question Answering", "cites": "574", "abstract": "Recent years have witnessed an increasing interest in image-based\nquestion-answering (QA) tasks. However, due to data limitations, there has been\nmuch less work on video-based QA. In this paper, we present TVQA, a large-scale\nvideo QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs\nfrom 21,793 clips, spanning over 460 hours of video. Questions are designed to\nbe compositional in nature, requiring systems to jointly localize relevant\nmoments within a clip, comprehend subtitle-based dialogue, and recognize\nrelevant visual concepts. We provide analyses of this new dataset as well as\nseveral baselines and a multi-stream end-to-end trainable neural network\nframework for the TVQA task. The dataset is publicly available at\nhttp://tvqa.cs.unc.edu.", "no": 77}, {"url": "https://arxiv.org/abs/1805.06297", "title": "A robust self-learning method for fully unsupervised cross-lingual\n  mappings of word embeddings", "cites": "573", "abstract": "Recent work has managed to learn cross-lingual word embeddings without\nparallel data by mapping monolingual embeddings to a shared space through\nadversarial training. However, their evaluation has focused on favorable\nconditions, using comparable corpora or closely-related languages, and we show\nthat they often fail in more realistic scenarios. This work proposes an\nalternative approach based on a fully unsupervised initialization that\nexplicitly exploits the structural similarity of the embeddings, and a robust\nself-learning algorithm that iteratively improves this solution. Our method\nsucceeds in all tested scenarios and obtains the best published results in\nstandard datasets, even surpassing previous supervised systems. Our\nimplementation is released as an open source project at\nhttps://github.com/artetxem/vecmap", "no": 78}, {"url": "https://arxiv.org/abs/1810.10147", "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification\n  Dataset with State-of-the-Art Evaluation", "cites": "570", "abstract": "We present a Few-Shot Relation Classification Dataset (FewRel), consisting of\n70, 000 sentences on 100 relations derived from Wikipedia and annotated by\ncrowdworkers. The relation of each sentence is first recognized by distant\nsupervision methods, and then filtered by crowdworkers. We adapt the most\nrecent state-of-the-art few-shot learning methods for relation classification\nand conduct a thorough evaluation of these methods. Empirical results show that\neven the most competitive few-shot learning models struggle on this task,\nespecially as compared with humans. We also show that a range of different\nreasoning skills are needed to solve our task. These results indicate that\nfew-shot relation classification remains an open problem and still requires\nfurther research. Our detailed analysis points multiple directions for future\nresearch. All details and resources about the dataset and baselines are\nreleased on http://zhuhao.me/fewrel.", "no": 79}, {"url": "https://arxiv.org/abs/1803.08035", "title": "Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs", "cites": "569", "abstract": "We consider the problem of zero-shot recognition: learning a visual\nclassifier for a category with zero training examples, just using the word\nembedding of the category and its relationship to other categories, which\nvisual data are provided. The key to dealing with the unfamiliar or novel\ncategory is to transfer knowledge obtained from familiar classes to describe\nthe unfamiliar class. In this paper, we build upon the recently introduced\nGraph Convolutional Network (GCN) and propose an approach that uses both\nsemantic embeddings and the categorical relationships to predict the\nclassifiers. Given a learned knowledge graph (KG), our approach takes as input\nsemantic embeddings for each node (representing visual category). After a\nseries of graph convolutions, we predict the visual classifier for each\ncategory. During training, the visual classifiers for a few categories are\ngiven to learn the GCN parameters. At test time, these filters are used to\npredict the visual classifiers of unseen categories. We show that our approach\nis robust to noise in the KG. More importantly, our approach provides\nsignificant improvement in performance compared to the current state-of-the-art\nresults (from 2 ~ 3% on some metrics to whopping 20% on a few).", "no": 80}, {"url": "https://arxiv.org/abs/1810.02338", "title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language\n  Understanding", "cites": "562", "abstract": "We marry two powerful ideas: deep representation learning for visual\nrecognition and language understanding, and symbolic program execution for\nreasoning. Our neural-symbolic visual question answering (NS-VQA) system first\nrecovers a structural scene representation from the image and a program trace\nfrom the question. It then executes the program on the scene representation to\nobtain an answer. Incorporating symbolic structure as prior knowledge offers\nthree unique advantages. First, executing programs on a symbolic space is more\nrobust to long program traces; our model can solve complex reasoning tasks\nbetter, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model\nis more data- and memory-efficient: it performs well after learning on a small\nnumber of training data; it can also encode an image into a compact\nrepresentation, requiring less storage than existing methods for offline\nquestion answering. Third, symbolic program execution offers full transparency\nto the reasoning process; we are thus able to interpret and diagnose each\nexecution step.", "no": 81}, {"url": "https://arxiv.org/abs/1805.01042", "title": "Hypothesis Only Baselines in Natural Language Inference", "cites": "554", "abstract": "We propose a hypothesis only baseline for diagnosing Natural Language\nInference (NLI). Especially when an NLI dataset assumes inference is occurring\nbased purely on the relationship between a context and a hypothesis, it follows\nthat assessing entailment relations while ignoring the provided context is a\ndegenerate solution. Yet, through experiments on ten distinct NLI datasets, we\nfind that this approach, which we refer to as a hypothesis-only model, is able\nto significantly outperform a majority class baseline across a number of NLI\ndatasets. Our analysis suggests that statistical irregularities may allow a\nmodel to perform NLI in some datasets beyond what should be achievable without\naccess to the context.", "no": 82}, {"url": "https://arxiv.org/abs/1812.08989", "title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot", "cites": "548", "abstract": "This paper describes the development of Microsoft XiaoIce, the most popular\nsocial chatbot in the world. XiaoIce is uniquely designed as an AI companion\nwith an emotional connection to satisfy the human need for communication,\naffection, and social belonging. We take into account both intelligent quotient\n(IQ) and emotional quotient (EQ) in system design, cast human-machine social\nchat as decision-making over Markov Decision Processes (MDPs), and optimize\nXiaoIce for long-term user engagement, measured in expected Conversation-turns\nPer Session (CPS). We detail the system architecture and key components\nincluding dialogue manager, core chat, skills, and an empathetic computing\nmodule. We show how XiaoIce dynamically recognizes human feelings and states,\nunderstands user intent, and responds to user needs throughout long\nconversations. Since her launch in 2014, XiaoIce has communicated with over 660\nmillion active users and succeeded in establishing long-term relationships with\nmany of them. Analysis of large scale online logs shows that XiaoIce has\nachieved an average CPS of 23, which is significantly higher than that of other\nchatbots and even human conversations.", "no": 83}, {"url": "https://arxiv.org/abs/1811.00491", "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs", "cites": "547", "abstract": "We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.", "no": 84}, {"url": "https://arxiv.org/abs/1802.05695", "title": "Explainable Prediction of Medical Codes from Clinical Text", "cites": "540", "abstract": "Clinical notes are text documents that are created by clinicians for each\npatient encounter. They are typically accompanied by medical codes, which\ndescribe the diagnosis and treatment. Annotating these codes is labor intensive\nand error prone; furthermore, the connection between the codes and the text is\nnot annotated, obscuring the reasons and details behind specific diagnoses and\ntreatments. We present an attentional convolutional network that predicts\nmedical codes from clinical text. Our method aggregates information across the\ndocument using a convolutional neural network, and uses an attention mechanism\nto select the most relevant segments for each of the thousands of possible\ncodes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of\n0.54, which are both better than the prior state of the art. Furthermore,\nthrough an interpretability evaluation by a physician, we show that the\nattention mechanism identifies meaningful explanations for each code assignment", "no": 85}, {"url": "https://arxiv.org/abs/1804.06437", "title": "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style\n  Transfer", "cites": "534", "abstract": "We consider the task of text attribute transfer: transforming a sentence to\nalter a specific attribute (e.g., sentiment) while preserving its\nattribute-independent content (e.g., changing \"screen is just the right size\"\nto \"screen is too small\"). Our training data includes only sentences labeled\nwith their attribute (e.g., positive or negative), but not pairs of sentences\nthat differ only in their attributes, so we must learn to disentangle\nattributes from attribute-independent content in an unsupervised way. Previous\nwork using adversarial methods has struggled to produce high-quality outputs.\nIn this paper, we propose simpler methods motivated by the observation that\ntext attributes are often marked by distinctive phrases (e.g., \"too small\").\nOur strongest method extracts content words by deleting phrases associated with\nthe sentence's original attribute value, retrieves new phrases associated with\nthe target attribute, and uses a neural model to fluently combine these into a\nfinal output. On human evaluation, our best method generates grammatical and\nappropriate responses on 22% more inputs than the best previous system,\naveraged over three attribute transfer datasets: altering sentiment of reviews\non Yelp, altering sentiment of reviews on Amazon, and altering image captions\nto be more romantic or humorous.", "no": 86}, {"url": "https://arxiv.org/abs/1802.08636", "title": "Ranking Sentences for Extractive Summarization with Reinforcement\n  Learning", "cites": "533", "abstract": "Single document summarization is the task of producing a shorter version of a\ndocument while preserving its principal information content. In this paper we\nconceptualize extractive summarization as a sentence ranking task and propose a\nnovel training algorithm which globally optimizes the ROUGE evaluation metric\nthrough a reinforcement learning objective. We use our algorithm to train a\nneural summarization model on the CNN and DailyMail datasets and demonstrate\nexperimentally that it outperforms state-of-the-art extractive and abstractive\nsystems when evaluated automatically and by humans.", "no": 87}, {"url": "https://arxiv.org/abs/1803.09047", "title": "Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with\n  Tacotron", "cites": "530", "abstract": "We present an extension to the Tacotron speech synthesis architecture that\nlearns a latent embedding space of prosody, derived from a reference acoustic\nrepresentation containing the desired prosody. We show that conditioning\nTacotron on this learned embedding space results in synthesized audio that\nmatches the prosody of the reference signal with fine time detail even when the\nreference and synthesis speakers are different. Additionally, we show that a\nreference prosody embedding can be used to synthesize text that is different\nfrom that of the reference utterance. We define several quantitative and\nsubjective metrics for evaluating prosody transfer, and report results with\naccompanying audio samples from single-speaker and 44-speaker Tacotron models\non a prosody transfer task.", "no": 88}, {"url": "https://arxiv.org/abs/1803.02893", "title": "An efficient framework for learning sentence representations", "cites": "526", "abstract": "In this work we propose a simple and efficient framework for learning\nsentence representations from unlabelled data. Drawing inspiration from the\ndistributional hypothesis and recent work on learning sentence representations,\nwe reformulate the problem of predicting the context in which a sentence\nappears as a classification problem. Given a sentence and its context, a\nclassifier distinguishes context sentences from other contrastive sentences\nbased on their vector representations. This allows us to efficiently learn\ndifferent types of encoding functions, and we show that the model learns\nhigh-quality sentence representations. We demonstrate that our sentence\nrepresentations outperform state-of-the-art unsupervised and supervised\nrepresentation learning methods on several downstream NLP tasks that involve\nunderstanding sentence semantics while achieving an order of magnitude speedup\nin training time.", "no": 89}, {"url": "https://arxiv.org/abs/1804.11283", "title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive\n  Strategies", "cites": "526", "abstract": "We present NEWSROOM, a summarization dataset of 1.3 million articles and\nsummaries written by authors and editors in newsrooms of 38 major news\npublications. Extracted from search and social media metadata between 1998 and\n2017, these high-quality summaries demonstrate high diversity of summarization\nstyles. In particular, the summaries combine abstractive and extractive\nstrategies, borrowing words and phrases from articles at varying rates. We\nanalyze the extraction strategies used in NEWSROOM summaries against other\ndatasets to quantify the diversity and difficulty of our new data, and train\nexisting methods on the data to evaluate its utility and challenges.", "no": 90}, {"url": "https://arxiv.org/abs/1805.01052", "title": "Constituency Parsing with a Self-Attentive Encoder", "cites": "519", "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive\narchitecture can lead to improvements to a state-of-the-art discriminative\nconstituency parser. The use of attention makes explicit the manner in which\ninformation is propagated between different locations in the sentence, which we\nuse to both analyze our model and propose potential improvements. For example,\nwe find that separating positional and content information in the encoder can\nlead to improved parsing accuracy. Additionally, we evaluate different\napproaches for lexical representation. Our parser achieves new state-of-the-art\nresults for single models trained on the Penn Treebank: 93.55 F1 without the\nuse of any external data, and 95.13 F1 when using pre-trained word\nrepresentations. Our parser also outperforms the previous best-published\naccuracy figures on 8 of the 9 languages in the SPMRL dataset.", "no": 91}, {"url": "https://arxiv.org/abs/1803.07416", "title": "Tensor2Tensor for Neural Machine Translation", "cites": "518", "abstract": "Tensor2Tensor is a library for deep learning models that is well-suited for\nneural machine translation and includes the reference implementation of the\nstate-of-the-art Transformer model.", "no": 92}, {"url": "https://arxiv.org/abs/1802.05300", "title": "Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\n  Noise", "cites": "517", "abstract": "The growing importance of massive datasets used for deep learning makes\nrobustness to label noise a critical property for classifiers to have. Sources\nof label noise include automatic labeling, non-expert labeling, and label\ncorruption by data poisoning adversaries. Numerous previous works assume that\nno source of labels can be trusted. We relax this assumption and assume that a\nsmall subset of the training data is trusted. This enables substantial label\ncorruption robustness performance gains. In addition, particularly severe label\nnoise can be combated by using a set of trusted data with clean labels. We\nutilize trusted data by proposing a loss correction technique that utilizes\ntrusted examples in a data-efficient manner to mitigate the effects of label\nnoise on deep neural network classifiers. Across vision and natural language\nprocessing tasks, we experiment with various label noises at several strengths,\nand show that our method significantly outperforms existing methods.", "no": 93}, {"url": "https://arxiv.org/abs/1805.07043", "title": "Aspect Based Sentiment Analysis with Gated Convolutional Networks", "cites": "515", "abstract": "Aspect based sentiment analysis (ABSA) can provide more detailed information\nthan general sentiment analysis, because it aims to predict the sentiment\npolarities of the given aspects or entities in text. We summarize previous\napproaches into two subtasks: aspect-category sentiment analysis (ACSA) and\naspect-term sentiment analysis (ATSA). Most previous approaches employ long\nshort-term memory and attention mechanisms to predict the sentiment polarity of\nthe concerned targets, which are often complicated and need more training time.\nWe propose a model based on convolutional neural networks and gating\nmechanisms, which is more accurate and efficient. First, the novel Gated\nTanh-ReLU Units can selectively output the sentiment features according to the\ngiven aspect or entity. The architecture is much simpler than attention layer\nused in the existing models. Second, the computations of our model could be\neasily parallelized during training, because convolutional layers do not have\ntime dependency as in LSTM layers, and gating units also work independently.\nThe experiments on SemEval datasets demonstrate the efficiency and\neffectiveness of our models.", "no": 94}, {"url": "https://arxiv.org/abs/1812.08951", "title": "Analysis Methods in Neural Language Processing: A Survey", "cites": "515", "abstract": "The field of natural language processing has seen impressive progress in\nrecent years, with neural network models replacing many of the traditional\nsystems. A plethora of new models have been proposed, many of which are thought\nto be opaque compared to their feature-rich counterparts. This has led\nresearchers to analyze, interpret, and evaluate neural networks in novel and\nmore fine-grained ways. In this survey paper, we review analysis methods in\nneural language processing, categorize them according to prominent research\ntrends, highlight existing limitations, and point to potential directions for\nfuture work.", "no": 95}, {"url": "https://arxiv.org/abs/1803.06643", "title": "The Web as a Knowledge-base for Answering Complex Questions", "cites": "500", "abstract": "Answering complex questions is a time-consuming activity for humans that\nrequires reasoning and integration of information. Recent work on reading\ncomprehension made headway in answering simple questions, but tackling complex\nquestions is still an ongoing research challenge. Conversely, semantic parsers\nhave been successful at handling compositionality, but only when the\ninformation resides in a target knowledge-base. In this paper, we present a\nnovel framework for answering broad and complex questions, assuming answering\nsimple questions is possible using a search engine and a reading comprehension\nmodel. We propose to decompose complex questions into a sequence of simple\nquestions, and compute the final answer from the sequence of answers. To\nillustrate the viability of our approach, we create a new dataset of complex\nquestions, ComplexWebQuestions, and present a model that decomposes questions\nand interacts with the web to compute an answer. We empirically demonstrate\nthat question decomposition improves performance from 20.8 precision@1 to 27.5\nprecision@1 on this new dataset.", "no": 96}, {"url": "https://arxiv.org/abs/1811.10092", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning\n  for Vision-Language Navigation", "cites": "491", "abstract": "Vision-language navigation (VLN) is the task of navigating an embodied agent\nto carry out natural language instructions inside real 3D environments. In this\npaper, we study how to address three critical challenges for this task: the\ncross-modal grounding, the ill-posed feedback, and the generalization problems.\nFirst, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that\nenforces cross-modal grounding both locally and globally via reinforcement\nlearning (RL). Particularly, a matching critic is used to provide an intrinsic\nreward to encourage global matching between instructions and trajectories, and\na reasoning navigator is employed to perform cross-modal grounding in the local\nvisual scene. Evaluation on a VLN benchmark dataset shows that our RCM model\nsignificantly outperforms previous methods by 10% on SPL and achieves the new\nstate-of-the-art performance. To improve the generalizability of the learned\npolicy, we further introduce a Self-Supervised Imitation Learning (SIL) method\nto explore unseen environments by imitating its own past, good decisions. We\ndemonstrate that SIL can approximate a better and more efficient policy, which\ntremendously minimizes the success rate performance gap between seen and unseen\nenvironments (from 30.7% to 11.7%).", "no": 97}, {"url": "https://arxiv.org/abs/1803.11138", "title": "Colorless green recurrent networks dream hierarchically", "cites": "484", "abstract": "Recurrent neural networks (RNNs) have achieved impressive results in a\nvariety of linguistic processing tasks, suggesting that they can induce\nnon-trivial properties of language. We investigate here to what extent RNNs\nlearn to track abstract hierarchical syntactic structure. We test whether RNNs\ntrained with a generic language modeling objective in four languages (Italian,\nEnglish, Hebrew, Russian) can predict long-distance number agreement in various\nconstructions. We include in our evaluation nonsensical sentences where RNNs\ncannot rely on semantic or lexical cues (\"The colorless green ideas I ate with\nthe chair sleep furiously\"), and, for Italian, we compare model performance to\nhuman intuitions. Our language-model-trained RNNs make reliable predictions\nabout long-distance agreement, and do not lag much behind human performance. We\nthus bring support to the hypothesis that RNNs are not just shallow-pattern\nextractors, but they also acquire deeper grammatical competence.", "no": 98}, {"url": "https://arxiv.org/abs/1806.02724", "title": "Speaker-Follower Models for Vision-and-Language Navigation", "cites": "464", "abstract": "Navigation guided by natural language instructions presents a challenging\nreasoning problem for instruction followers. Natural language instructions\ntypically identify only a few high-level decisions and landmarks rather than\ncomplete low-level motor behaviors; much of the missing information must be\ninferred based on perceptual context. In machine learning settings, this is\ndoubly challenging: it is difficult to collect enough annotated data to enable\nlearning of this reasoning process from scratch, and also difficult to\nimplement the reasoning process using generic sequence models. Here we describe\nan approach to vision-and-language navigation that addresses both these issues\nwith an embedded speaker model. We use this speaker model to (1) synthesize new\ninstructions for data augmentation and to (2) implement pragmatic reasoning,\nwhich evaluates how well candidate action sequences explain an instruction.\nBoth steps are supported by a panoramic action space that reflects the\ngranularity of human-generated instructions. Experiments show that all three\ncomponents of this approach---speaker-driven data augmentation, pragmatic\nreasoning and panoramic action space---dramatically improve the performance of\na baseline instruction follower, more than doubling the success rate over the\nbest existing approach on a standard benchmark.", "no": 99}, {"url": "https://arxiv.org/abs/1804.05392", "title": "Higher-order Coreference Resolution with Coarse-to-fine Inference", "cites": "463", "abstract": "We introduce a fully differentiable approximation to higher-order inference\nfor coreference resolution. Our approach uses the antecedent distribution from\na span-ranking architecture as an attention mechanism to iteratively refine\nspan representations. This enables the model to softly consider multiple hops\nin the predicted clusters. To alleviate the computational cost of this\niterative process, we introduce a coarse-to-fine approach that incorporates a\nless accurate but more efficient bilinear factor, enabling more aggressive\npruning without hurting accuracy. Compared to the existing state-of-the-art\nspan-ranking approach, our model significantly improves accuracy on the English\nOntoNotes benchmark, while being far more computationally efficient.", "no": 100}]