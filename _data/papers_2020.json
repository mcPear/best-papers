[{"url": "https://arxiv.org/abs/2005.14165", "title": "Language Models are Few-Shot Learners", "cites": 15614}, {"url": "https://arxiv.org/abs/2006.11477", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 2655}, {"url": "https://arxiv.org/abs/2004.05150", "title": "Longformer: The Long-Document Transformer", "cites": 2037}, {"url": "https://arxiv.org/abs/2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 1706}, {"url": "https://arxiv.org/abs/2004.10964", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 1491}, {"url": "https://arxiv.org/abs/2001.04451", "title": "Reformer: The Efficient Transformer", "cites": 1474}, {"url": "https://arxiv.org/abs/2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 1276}, {"url": "https://arxiv.org/abs/2006.03654", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 1204}, {"url": "https://arxiv.org/abs/2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 1194}, {"url": "https://arxiv.org/abs/2007.14062", "title": "Big Bird: Transformers for Longer Sequences", "cites": 1190}, {"url": "https://arxiv.org/abs/2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": 1169}, {"url": "https://arxiv.org/abs/2001.08210", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 1070}, {"url": "https://arxiv.org/abs/2003.07082", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 1059}, {"url": "https://arxiv.org/abs/2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": 1037}, {"url": "https://arxiv.org/abs/2002.00388", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": 1005}, {"url": "https://arxiv.org/abs/2002.08909", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": 990}, {"url": "https://arxiv.org/abs/2002.12327", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 976}, {"url": "https://arxiv.org/abs/2001.07676", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": 926}, {"url": "https://arxiv.org/abs/2003.08271", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": 926}, {"url": "https://arxiv.org/abs/2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": 897}, {"url": "https://arxiv.org/abs/2009.14794", "title": "Rethinking Attention with Performers", "cites": 888}, {"url": "https://arxiv.org/abs/2007.15779", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": 794}, {"url": "https://arxiv.org/abs/2006.04558", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": 783}, {"url": "https://arxiv.org/abs/2004.04696", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": 758}, {"url": "https://arxiv.org/abs/2012.07805", "title": "Extracting Training Data from Large Language Models", "cites": 747}, {"url": "https://arxiv.org/abs/2005.04118", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": 744}, {"url": "https://arxiv.org/abs/2001.09977", "title": "Towards a Human-like Open-Domain Chatbot", "cites": 708}, {"url": "https://arxiv.org/abs/2004.13637", "title": "Recipes for building an open-domain chatbot", "cites": 706}, {"url": "https://arxiv.org/abs/2101.00027", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "cites": 705}, {"url": "https://arxiv.org/abs/2009.06732", "title": "Efficient Transformers: A Survey", "cites": 700}, {"url": "https://arxiv.org/abs/2005.14050", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "cites": 697}, {"url": "https://arxiv.org/abs/2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "cites": 694}, {"url": "https://arxiv.org/abs/2003.11080", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization", "cites": 692}, {"url": "https://arxiv.org/abs/2007.00808", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "cites": 680}, {"url": "https://arxiv.org/abs/2004.10706", "title": "CORD-19: The COVID-19 Open Research Dataset", "cites": 666}, {"url": "https://arxiv.org/abs/2009.09761", "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis", "cites": 632}, {"url": "https://arxiv.org/abs/2009.07118", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "cites": 618}, {"url": "https://arxiv.org/abs/2002.10957", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "cites": 579}, {"url": "https://arxiv.org/abs/2005.00661", "title": "On Faithfulness and Factuality in Abstractive Summarization", "cites": 566}, {"url": "https://arxiv.org/abs/2002.08910", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "cites": 541}, {"url": "https://arxiv.org/abs/2005.10200", "title": "BERTweet: A pre-trained language model for English Tweets", "cites": 539}, {"url": "https://arxiv.org/abs/2009.01325", "title": "Learning to summarize from human feedback", "cites": 536}, {"url": "https://arxiv.org/abs/2003.00104", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding", "cites": 533}, {"url": "https://arxiv.org/abs/2004.02984", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "cites": 532}, {"url": "https://arxiv.org/abs/2004.09813", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "cites": 526}, {"url": "https://arxiv.org/abs/2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain\n  Question Answering", "cites": 523}, {"url": "https://arxiv.org/abs/2009.08366", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "cites": 510}, {"url": "https://arxiv.org/abs/2002.04745", "title": "On Layer Normalization in the Transformer Architecture", "cites": 502}, {"url": "https://arxiv.org/abs/2007.06225", "title": "ProtTrans: Towards Cracking the Language of Life's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "cites": 499}, {"url": "https://arxiv.org/abs/2005.00700", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "cites": 494}, {"url": "https://arxiv.org/abs/2006.16668", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "cites": 489}, {"url": "https://arxiv.org/abs/2009.11462", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\n  Models", "cites": 473}, {"url": "https://arxiv.org/abs/2010.11125", "title": "Beyond English-Centric Multilingual Machine Translation", "cites": 457}, {"url": "https://arxiv.org/abs/2010.01057", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "cites": 448}, {"url": "https://arxiv.org/abs/2004.09297", "title": "MPNet: Masked and Permuted Pre-training for Language Understanding", "cites": 443}, {"url": "https://arxiv.org/abs/2004.09456", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "cites": 441}, {"url": "https://arxiv.org/abs/2007.01852", "title": "Language-agnostic BERT Sentence Embedding", "cites": 435}, {"url": "https://arxiv.org/abs/2004.09095", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "cites": 433}, {"url": "https://arxiv.org/abs/2009.09025", "title": "COMET: A Neural Framework for MT Evaluation", "cites": 432}, {"url": "https://arxiv.org/abs/2004.03705", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "cites": 407}, {"url": "https://arxiv.org/abs/2009.03300", "title": "Measuring Massive Multitask Language Understanding", "cites": 407}, {"url": "https://arxiv.org/abs/2006.13979", "title": "Unsupervised Cross-lingual Representation Learning for Speech\n  Recognition", "cites": 405}, {"url": "https://arxiv.org/abs/2004.13922", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing", "cites": 403}, {"url": "https://arxiv.org/abs/2004.09984", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "cites": 390}, {"url": "https://arxiv.org/abs/2002.06305", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "cites": 385}, {"url": "https://arxiv.org/abs/2005.00928", "title": "Quantifying Attention Flow in Transformers", "cites": 383}, {"url": "https://arxiv.org/abs/2010.06467", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "cites": 378}, {"url": "https://arxiv.org/abs/2006.07235", "title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "cites": 377}, {"url": "https://arxiv.org/abs/2011.04006", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "cites": 370}, {"url": "https://arxiv.org/abs/2005.00796", "title": "A Simple Language Model for Task-Oriented Dialogue", "cites": 369}, {"url": "https://arxiv.org/abs/2005.00247", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "cites": 368}, {"url": "https://arxiv.org/abs/2010.08191", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "cites": 365}, {"url": "https://arxiv.org/abs/2005.00052", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "cites": 362}, {"url": "https://arxiv.org/abs/2010.12421", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "cites": 362}, {"url": "https://arxiv.org/abs/2004.02349", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "cites": 361}, {"url": "https://arxiv.org/abs/2007.07779", "title": "AdapterHub: A Framework for Adapting Transformers", "cites": 360}, {"url": "https://arxiv.org/abs/2002.01808", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "cites": 356}, {"url": "https://arxiv.org/abs/2004.10643", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "cites": 356}, {"url": "https://arxiv.org/abs/2003.05002", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in\n  Typologically Diverse Languages", "cites": 347}, {"url": "https://arxiv.org/abs/2004.03685", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "cites": 344}, {"url": "https://arxiv.org/abs/2007.12626", "title": "SummEval: Re-evaluating Summarization Evaluation", "cites": 342}, {"url": "https://arxiv.org/abs/2010.00747", "title": "Contrastive Learning of Medical Visual Representations from Paired\n  Images and Text", "cites": 341}, {"url": "https://arxiv.org/abs/2002.02562", "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "cites": 340}, {"url": "https://arxiv.org/abs/2010.02559", "title": "LEGAL-BERT: The Muppets straight out of Law School", "cites": 337}, {"url": "https://arxiv.org/abs/2005.08314", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "cites": 334}, {"url": "https://arxiv.org/abs/2004.01970", "title": "BAE: BERT-based Adversarial Examples for Text Classification", "cites": 330}, {"url": "https://arxiv.org/abs/2011.05864", "title": "On the Sentence Embeddings from Pre-trained Language Models", "cites": 330}, {"url": "https://arxiv.org/abs/2005.00200", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "cites": 327}, {"url": "https://arxiv.org/abs/2006.06195", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "cites": 327}, {"url": "https://arxiv.org/abs/2006.05987", "title": "Revisiting Few-sample BERT Fine-tuning", "cites": 326}, {"url": "https://arxiv.org/abs/2006.03659", "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual\n  Representations", "cites": 324}, {"url": "https://arxiv.org/abs/2001.04063", "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training", "cites": 323}, {"url": "https://arxiv.org/abs/2005.00547", "title": "GoEmotions: A Dataset of Fine-Grained Emotions", "cites": 320}, {"url": "https://arxiv.org/abs/2011.03161", "title": "What's New? Summarizing Contributions in Scientific Literature", "cites": 320}, {"url": "https://arxiv.org/abs/2004.08795", "title": "Extractive Summarization as Text Matching", "cites": 317}, {"url": "https://arxiv.org/abs/2004.12362", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "cites": 316}, {"url": "https://arxiv.org/abs/2004.00849", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "cites": 312}, {"url": "https://arxiv.org/abs/2004.06100", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "cites": 306}, {"url": "https://arxiv.org/abs/2005.04790", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "cites": 305}, {"url": "https://arxiv.org/abs/2011.01403", "title": "Supervised Contrastive Learning for Pre-trained Language Model\n  Fine-tuning", "cites": 301}]