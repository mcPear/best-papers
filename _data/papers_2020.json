[{"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 7025}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1660}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 1436}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 1223}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 1014}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 1012}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 979}, {"url": "http://arxiv.org/abs/2004.06165v5", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 818}, {"url": "http://arxiv.org/abs/2007.14062v2", "title": "Big Bird: Transformers for Longer Sequences", "cites": 744}, {"url": "http://arxiv.org/abs/2003.07082v2", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 744}, {"url": "http://arxiv.org/abs/2001.08210v2", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 725}, {"url": "http://arxiv.org/abs/2002.12327v3", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 721}, {"url": "http://arxiv.org/abs/2010.11934v3", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 638}, {"url": "http://arxiv.org/abs/2006.03654v6", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 630}, {"url": "http://arxiv.org/abs/2002.00388v4", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": 598}, {"url": "http://arxiv.org/abs/2003.08271v4", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": 583}, {"url": "http://arxiv.org/abs/2009.14794v4", "title": "Rethinking Attention with Performers", "cites": 572}, {"url": "http://arxiv.org/abs/2002.08155v4", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": 558}, {"url": "http://arxiv.org/abs/2004.10706v4", "title": "CORD-19: The COVID-19 Open Research Dataset", "cites": 541}, {"url": "http://arxiv.org/abs/2005.04118v1", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": 540}, {"url": "http://arxiv.org/abs/2003.11080v5", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization", "cites": 512}, {"url": "http://arxiv.org/abs/2002.08909v1", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": 511}, {"url": "http://arxiv.org/abs/2001.09977v3", "title": "Towards a Human-like Open-Domain Chatbot", "cites": 504}, {"url": "http://arxiv.org/abs/2001.07676v3", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": 500}, {"url": "http://arxiv.org/abs/2005.11401v4", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": 484}, {"url": "http://arxiv.org/abs/2005.14050v2", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "cites": 471}, {"url": "http://arxiv.org/abs/2006.04558v8", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": 464}, {"url": "http://arxiv.org/abs/2004.04696v5", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": 446}, {"url": "http://arxiv.org/abs/2009.14794v3", "title": "Rethinking Attention with Performers", "cites": 441}, {"url": "http://arxiv.org/abs/2004.12832v2", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "cites": 438}, {"url": "http://arxiv.org/abs/2009.06732v3", "title": "Efficient Transformers: A Survey", "cites": 432}, {"url": "http://arxiv.org/abs/2004.13637v2", "title": "Recipes for building an open-domain chatbot", "cites": 429}, {"url": "http://arxiv.org/abs/2007.00808v2", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "cites": 428}, {"url": "http://arxiv.org/abs/2012.07805v2", "title": "Extracting Training Data from Large Language Models", "cites": 395}, {"url": "http://arxiv.org/abs/2004.02984v2", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "cites": 385}, {"url": "http://arxiv.org/abs/2009.07118v2", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "cites": 382}, {"url": "http://arxiv.org/abs/2007.15779v6", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": 382}, {"url": "http://arxiv.org/abs/2002.08910v4", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "cites": 365}, {"url": "http://arxiv.org/abs/2003.00104v4", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding", "cites": 361}, {"url": "http://arxiv.org/abs/2005.10200v2", "title": "BERTweet: A pre-trained language model for English Tweets", "cites": 341}, {"url": "http://arxiv.org/abs/2005.00700v3", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "cites": 325}, {"url": "http://arxiv.org/abs/2004.09813v2", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "cites": 318}, {"url": "http://arxiv.org/abs/2002.10957v2", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "cites": 315}, {"url": "http://arxiv.org/abs/2006.16668v1", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "cites": 312}, {"url": "http://arxiv.org/abs/2002.04745v2", "title": "On Layer Normalization in the Transformer Architecture", "cites": 311}, {"url": "http://arxiv.org/abs/2006.07235v2", "title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "cites": 307}, {"url": "http://arxiv.org/abs/2005.00661v1", "title": "On Faithfulness and Factuality in Abstractive Summarization", "cites": 299}, {"url": "http://arxiv.org/abs/2007.01282v2", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain\n  Question Answering", "cites": 296}, {"url": "http://arxiv.org/abs/2002.06305v1", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "cites": 288}, {"url": "http://arxiv.org/abs/2010.01057v1", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "cites": 287}, {"url": "http://arxiv.org/abs/2004.09095v3", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "cites": 275}, {"url": "http://arxiv.org/abs/2004.10643v1", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "cites": 266}, {"url": "http://arxiv.org/abs/2007.01852v2", "title": "Language-agnostic BERT Sentence Embedding", "cites": 261}, {"url": "http://arxiv.org/abs/2009.09761v3", "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis", "cites": 258}, {"url": "http://arxiv.org/abs/2005.00796v4", "title": "A Simple Language Model for Task-Oriented Dialogue", "cites": 255}, {"url": "http://arxiv.org/abs/2010.11125v1", "title": "Beyond English-Centric Multilingual Machine Translation", "cites": 252}, {"url": "http://arxiv.org/abs/2002.02562v2", "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "cites": 251}, {"url": "http://arxiv.org/abs/2010.06467v3", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "cites": 247}, {"url": "http://arxiv.org/abs/2004.02349v2", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "cites": 246}, {"url": "http://arxiv.org/abs/2101.00027v1", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "cites": 243}, {"url": "http://arxiv.org/abs/2003.05002v1", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in\n  Typologically Diverse Languages", "cites": 242}, {"url": "http://arxiv.org/abs/2001.04063v3", "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training", "cites": 242}, {"url": "http://arxiv.org/abs/2004.09984v3", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "cites": 239}, {"url": "http://arxiv.org/abs/2004.03685v3", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "cites": 239}, {"url": "http://arxiv.org/abs/2002.01808v5", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "cites": 237}, {"url": "http://arxiv.org/abs/2011.04006v1", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "cites": 236}, {"url": "http://arxiv.org/abs/2006.13979v2", "title": "Unsupervised Cross-lingual Representation Learning for Speech\n  Recognition", "cites": 236}, {"url": "http://arxiv.org/abs/2005.00928v2", "title": "Quantifying Attention Flow in Transformers", "cites": 235}, {"url": "http://arxiv.org/abs/2004.03705v3", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "cites": 235}, {"url": "http://arxiv.org/abs/2006.06195v2", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "cites": 228}, {"url": "http://arxiv.org/abs/2102.07662v1", "title": "Overview of the TREC 2020 deep learning track", "cites": 225}, {"url": "http://arxiv.org/abs/2009.11462v2", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\n  Models", "cites": 225}, {"url": "http://arxiv.org/abs/2003.07820v2", "title": "Overview of the TREC 2019 deep learning track", "cites": 225}, {"url": "http://arxiv.org/abs/2005.00052v3", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "cites": 224}, {"url": "http://arxiv.org/abs/2005.08314v1", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "cites": 223}, {"url": "http://arxiv.org/abs/2004.13922v2", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing", "cites": 223}, {"url": "http://arxiv.org/abs/2002.12804v1", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model\n  Pre-Training", "cites": 222}, {"url": "http://arxiv.org/abs/2010.08191v2", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "cites": 221}, {"url": "http://arxiv.org/abs/2004.09456v1", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "cites": 221}, {"url": "http://arxiv.org/abs/2002.06823v1", "title": "Incorporating BERT into Neural Machine Translation", "cites": 221}, {"url": "http://arxiv.org/abs/2011.05864v1", "title": "On the Sentence Embeddings from Pre-trained Language Models", "cites": 217}, {"url": "http://arxiv.org/abs/2006.05987v3", "title": "Revisiting Few-sample BERT Fine-tuning", "cites": 215}, {"url": "http://arxiv.org/abs/2006.03659v4", "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual\n  Representations", "cites": 214}, {"url": "http://arxiv.org/abs/2004.06100v2", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "cites": 214}, {"url": "http://arxiv.org/abs/2004.01970v3", "title": "BAE: BERT-based Adversarial Examples for Text Classification", "cites": 214}, {"url": "http://arxiv.org/abs/2004.00849v2", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "cites": 213}, {"url": "http://arxiv.org/abs/2004.08795v1", "title": "Extractive Summarization as Text Matching", "cites": 212}, {"url": "http://arxiv.org/abs/2004.09297v2", "title": "MPNet: Masked and Permuted Pre-training for Language Understanding", "cites": 209}, {"url": "http://arxiv.org/abs/2005.00200v2", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "cites": 207}, {"url": "http://arxiv.org/abs/2010.12421v2", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "cites": 204}, {"url": "http://arxiv.org/abs/2009.08366v4", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "cites": 204}, {"url": "http://arxiv.org/abs/2007.07779v3", "title": "AdapterHub: A Framework for Adapting Transformers", "cites": 202}, {"url": "http://arxiv.org/abs/2005.04790v3", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "cites": 202}, {"url": "http://arxiv.org/abs/2010.02559v1", "title": "LEGAL-BERT: The Muppets straight out of Law School", "cites": 200}, {"url": "http://arxiv.org/abs/2005.07503v1", "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse\n  COVID-19 Content on Twitter", "cites": 199}, {"url": "http://arxiv.org/abs/2005.00181v3", "title": "Sparse, Dense, and Attentional Representations for Text Retrieval", "cites": 199}, {"url": "http://arxiv.org/abs/2009.09025v2", "title": "COMET: A Neural Framework for MT Evaluation", "cites": 198}, {"url": "http://arxiv.org/abs/2005.00247v3", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "cites": 195}, {"url": "http://arxiv.org/abs/2004.12362v1", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "cites": 195}, {"url": "http://arxiv.org/abs/2011.01403v3", "title": "Supervised Contrastive Learning for Pre-trained Language Model\n  Fine-tuning", "cites": 191}]