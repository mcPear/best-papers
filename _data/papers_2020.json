[{"url": "http://arxiv.org/abs/2005.14165v4", "title": "Language Models are Few-Shot Learners", "cites": 5129}, {"url": "http://arxiv.org/abs/2003.10555v1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "cites": 1299}, {"url": "http://arxiv.org/abs/2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": 936}, {"url": "http://arxiv.org/abs/2004.05150v2", "title": "Longformer: The Long-Document Transformer", "cites": 934}, {"url": "http://arxiv.org/abs/2004.10964v3", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": 814}, {"url": "http://arxiv.org/abs/2001.04451v2", "title": "Reformer: The Efficient Transformer", "cites": 798}, {"url": "http://arxiv.org/abs/2004.04906v3", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": 787}, {"url": "http://arxiv.org/abs/2004.06165v5", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": 620}, {"url": "http://arxiv.org/abs/2002.12327v3", "title": "A Primer in BERTology: What we know about how BERT works", "cites": 615}, {"url": "http://arxiv.org/abs/2003.07082v2", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": 613}, {"url": "http://arxiv.org/abs/2001.08210v2", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": 581}, {"url": "http://arxiv.org/abs/2007.14062v2", "title": "Big Bird: Transformers for Longer Sequences", "cites": 559}, {"url": "http://arxiv.org/abs/2004.10706v4", "title": "CORD-19: The COVID-19 Open Research Dataset", "cites": 473}, {"url": "http://arxiv.org/abs/2006.03654v6", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": 442}, {"url": "http://arxiv.org/abs/2009.14794v3", "title": "Rethinking Attention with Performers", "cites": 441}, {"url": "http://arxiv.org/abs/2010.11934v3", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": 440}, {"url": "http://arxiv.org/abs/2001.09977v3", "title": "Towards a Human-like Open-Domain Chatbot", "cites": 440}, {"url": "http://arxiv.org/abs/2002.00388v4", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": 438}, {"url": "http://arxiv.org/abs/2003.11080v5", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization", "cites": 435}, {"url": "http://arxiv.org/abs/2003.08271v4", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": 434}, {"url": "http://arxiv.org/abs/2002.08909v1", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": 434}, {"url": "http://arxiv.org/abs/2005.04118v1", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": 427}, {"url": "http://arxiv.org/abs/2002.08155v4", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": 418}, {"url": "http://arxiv.org/abs/2005.14050v2", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "cites": 380}, {"url": "http://arxiv.org/abs/2005.11401v4", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": 377}, {"url": "http://arxiv.org/abs/2004.04696v5", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": 354}, {"url": "http://arxiv.org/abs/2004.12832v2", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "cites": 349}, {"url": "http://arxiv.org/abs/2007.00808v2", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "cites": 347}, {"url": "http://arxiv.org/abs/2001.07676v3", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": 345}, {"url": "http://arxiv.org/abs/2004.13637v2", "title": "Recipes for building an open-domain chatbot", "cites": 339}, {"url": "http://arxiv.org/abs/2009.06732v3", "title": "Efficient Transformers: A Survey", "cites": 336}, {"url": "http://arxiv.org/abs/2006.04558v8", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": 330}, {"url": "http://arxiv.org/abs/2004.02984v2", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "cites": 325}, {"url": "http://arxiv.org/abs/2002.08910v4", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "cites": 295}, {"url": "http://arxiv.org/abs/2003.00104v4", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding", "cites": 291}, {"url": "http://arxiv.org/abs/2012.07805v2", "title": "Extracting Training Data from Large Language Models", "cites": 289}, {"url": "http://arxiv.org/abs/2009.07118v2", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "cites": 279}, {"url": "http://arxiv.org/abs/2006.07235v2", "title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "cites": 277}, {"url": "http://arxiv.org/abs/2007.15779v6", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": 267}, {"url": "http://arxiv.org/abs/2006.16668v1", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "cites": 260}, {"url": "http://arxiv.org/abs/2005.00700v3", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "cites": 257}, {"url": "http://arxiv.org/abs/2005.10200v2", "title": "BERTweet: A pre-trained language model for English Tweets", "cites": 242}, {"url": "http://arxiv.org/abs/2002.04745v2", "title": "On Layer Normalization in the Transformer Architecture", "cites": 241}, {"url": "http://arxiv.org/abs/2002.06305v1", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "cites": 237}, {"url": "http://arxiv.org/abs/2004.09813v2", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "cites": 236}, {"url": "http://arxiv.org/abs/2002.10957v2", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "cites": 235}, {"url": "http://arxiv.org/abs/2004.10643v1", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "cites": 231}, {"url": "http://arxiv.org/abs/2004.09095v3", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "cites": 231}, {"url": "http://arxiv.org/abs/2007.01282v2", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain\n  Question Answering", "cites": 220}, {"url": "http://arxiv.org/abs/2010.01057v1", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "cites": 219}, {"url": "http://arxiv.org/abs/2005.00661v1", "title": "On Faithfulness and Factuality in Abstractive Summarization", "cites": 215}, {"url": "http://arxiv.org/abs/2005.00796v4", "title": "A Simple Language Model for Task-Oriented Dialogue", "cites": 207}, {"url": "http://arxiv.org/abs/2007.01852v2", "title": "Language-agnostic BERT Sentence Embedding", "cites": 205}, {"url": "http://arxiv.org/abs/2102.07662v1", "title": "Overview of the TREC 2020 deep learning track", "cites": 204}, {"url": "http://arxiv.org/abs/2003.07820v2", "title": "Overview of the TREC 2019 deep learning track", "cites": 204}, {"url": "http://arxiv.org/abs/2010.06467v3", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "cites": 197}, {"url": "http://arxiv.org/abs/2001.04063v3", "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training", "cites": 197}, {"url": "http://arxiv.org/abs/2002.02562v2", "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "cites": 195}, {"url": "http://arxiv.org/abs/2004.03705v3", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "cites": 194}, {"url": "http://arxiv.org/abs/2011.04006v1", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "cites": 190}, {"url": "http://arxiv.org/abs/2004.09984v3", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "cites": 189}, {"url": "http://arxiv.org/abs/2002.01808v5", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "cites": 189}, {"url": "http://arxiv.org/abs/2003.05002v1", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in\n  Typologically Diverse Languages", "cites": 188}, {"url": "http://arxiv.org/abs/2004.02349v2", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "cites": 187}, {"url": "http://arxiv.org/abs/2002.06823v1", "title": "Incorporating BERT into Neural Machine Translation", "cites": 187}, {"url": "http://arxiv.org/abs/2004.03685v3", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "cites": 186}, {"url": "http://arxiv.org/abs/2002.12804v1", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model\n  Pre-Training", "cites": 183}, {"url": "http://arxiv.org/abs/2006.06195v2", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "cites": 181}, {"url": "http://arxiv.org/abs/2006.13979v2", "title": "Unsupervised Cross-lingual Representation Learning for Speech\n  Recognition", "cites": 179}, {"url": "http://arxiv.org/abs/2010.11125v1", "title": "Beyond English-Centric Multilingual Machine Translation", "cites": 176}, {"url": "http://arxiv.org/abs/2005.00052v3", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "cites": 175}, {"url": "http://arxiv.org/abs/2006.05987v3", "title": "Revisiting Few-sample BERT Fine-tuning", "cites": 174}, {"url": "http://arxiv.org/abs/2006.00206v1", "title": "Corpus Creation for Sentiment Analysis in Code-Mixed Tamil-English Text", "cites": 174}, {"url": "http://arxiv.org/abs/2004.01970v3", "title": "BAE: BERT-based Adversarial Examples for Text Classification", "cites": 173}, {"url": "http://arxiv.org/abs/2005.04790v3", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "cites": 171}, {"url": "http://arxiv.org/abs/2010.08191v2", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "cites": 169}, {"url": "http://arxiv.org/abs/2005.00928v2", "title": "Quantifying Attention Flow in Transformers", "cites": 169}, {"url": "http://arxiv.org/abs/2005.07503v1", "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse\n  COVID-19 Content on Twitter", "cites": 166}, {"url": "http://arxiv.org/abs/2004.09456v1", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "cites": 166}, {"url": "http://arxiv.org/abs/2009.11462v2", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\n  Models", "cites": 165}, {"url": "http://arxiv.org/abs/2004.13922v2", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing", "cites": 165}, {"url": "http://arxiv.org/abs/2004.00849v2", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "cites": 165}, {"url": "http://arxiv.org/abs/2006.03659v4", "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual\n  Representations", "cites": 162}, {"url": "http://arxiv.org/abs/2005.00181v3", "title": "Sparse, Dense, and Attentional Representations for Text Retrieval", "cites": 161}, {"url": "http://arxiv.org/abs/2004.06100v2", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "cites": 160}, {"url": "http://arxiv.org/abs/2002.08307v2", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer\n  Learning", "cites": 159}, {"url": "http://arxiv.org/abs/2004.08795v1", "title": "Extractive Summarization as Text Matching", "cites": 157}, {"url": "http://arxiv.org/abs/2005.08314v1", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "cites": 156}, {"url": "http://arxiv.org/abs/2009.08366v4", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "cites": 154}, {"url": "http://arxiv.org/abs/2002.03932v1", "title": "Pre-training Tasks for Embedding-based Large-scale Retrieval", "cites": 154}, {"url": "http://arxiv.org/abs/2101.00027v1", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "cites": 153}, {"url": "http://arxiv.org/abs/2009.09761v3", "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis", "cites": 153}, {"url": "http://arxiv.org/abs/2006.06666v3", "title": "VirTex: Learning Visual Representations from Textual Annotations", "cites": 152}, {"url": "http://arxiv.org/abs/2007.07779v3", "title": "AdapterHub: A Framework for Adapting Transformers", "cites": 151}, {"url": "http://arxiv.org/abs/2004.04228v1", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of\n  Summaries", "cites": 151}, {"url": "http://arxiv.org/abs/2006.16934v3", "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through\n  Scene Graph", "cites": 150}, {"url": "http://arxiv.org/abs/2004.10151v3", "title": "Experience Grounds Language", "cites": 149}, {"url": "http://arxiv.org/abs/2006.00210v1", "title": "A Sentiment Analysis Dataset for Code-Mixed Malayalam-English", "cites": 148}, {"url": "http://arxiv.org/abs/2005.00200v2", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "cites": 148}, {"url": "http://arxiv.org/abs/2010.12421v2", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "cites": 146}]