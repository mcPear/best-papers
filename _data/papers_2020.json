[{"url": "https://arxiv.org/abs/2005.14165", "title": "Language Models are Few-Shot Learners", "cites": "27 648", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.", "no": 1}, {"url": "https://arxiv.org/abs/2006.11477", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "cites": "3 945", "abstract": "We show for the first time that learning powerful representations from speech\naudio alone followed by fine-tuning on transcribed speech can outperform the\nbest semi-supervised methods while being conceptually simpler. wav2vec 2.0\nmasks the speech input in the latent space and solves a contrastive task\ndefined over a quantization of the latent representations which are jointly\nlearned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER\non the clean/other test sets. When lowering the amount of labeled data to one\nhour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour\nsubset while using 100 times less labeled data. Using just ten minutes of\nlabeled data and pre-training on 53k hours of unlabeled data still achieves\n4.8/8.2 WER. This demonstrates the feasibility of speech recognition with\nlimited amounts of labeled data.", "no": 2}, {"url": "https://arxiv.org/abs/2004.05150", "title": "Longformer: The Long-Document Transformer", "cites": "3 011", "abstract": "Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "no": 3}, {"url": "https://arxiv.org/abs/2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "cites": "2 569", "abstract": "Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.", "no": 4}, {"url": "https://arxiv.org/abs/2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "cites": "2 431", "abstract": "Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "no": 5}, {"url": "https://arxiv.org/abs/2006.03654", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", "cites": "1 942", "abstract": "Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).", "no": 6}, {"url": "https://arxiv.org/abs/2004.10964", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "cites": "1 931", "abstract": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.", "no": 7}, {"url": "https://arxiv.org/abs/2001.04451", "title": "Reformer: The Efficient Transformer", "cites": "1 864", "abstract": "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.", "no": 8}, {"url": "https://arxiv.org/abs/2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "cites": "1 855", "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.", "no": 9}, {"url": "https://arxiv.org/abs/2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "cites": "1 797", "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language\n(PL) and nat-ural language (NL). CodeBERT learns general-purpose\nrepresentations that support downstream NL-PL applications such as natural\nlanguage codesearch, code documentation generation, etc. We develop CodeBERT\nwith Transformer-based neural architecture, and train it with a hybrid\nobjective function that incorporates the pre-training task of replaced token\ndetection, which is to detect plausible alternatives sampled from generators.\nThis enables us to utilize both bimodal data of NL-PL pairs and unimodal data,\nwhere the former provides input tokens for model training while the latter\nhelps to learn better generators. We evaluate CodeBERT on two NL-PL\napplications by fine-tuning model parameters. Results show that CodeBERT\nachieves state-of-the-art performance on both natural language code search and\ncode documentation generation tasks. Furthermore, to investigate what type of\nknowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and\nevaluate in a zero-shot setting where parameters of pre-trained models are\nfixed. Results show that CodeBERT performs better than previous pre-trained\nmodels on NL-PL probing.", "no": 10}, {"url": "https://arxiv.org/abs/2009.03300", "title": "Measuring Massive Multitask Language Understanding", "cites": "1 788", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.", "no": 11}, {"url": "https://arxiv.org/abs/2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "cites": "1 617", "abstract": "Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks.", "no": 12}, {"url": "https://arxiv.org/abs/2007.14062", "title": "Big Bird: Transformers for Longer Sequences", "cites": "1 598", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.", "no": 13}, {"url": "https://arxiv.org/abs/2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "cites": "1 538", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.", "no": 14}, {"url": "https://arxiv.org/abs/2001.08210", "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "cites": "1 464", "abstract": "This paper demonstrates that multilingual denoising pre-training produces\nsignificant performance gains across a wide variety of machine translation (MT)\ntasks. We present mBART -- a sequence-to-sequence denoising auto-encoder\npre-trained on large-scale monolingual corpora in many languages using the BART\nobjective. mBART is one of the first methods for pre-training a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while\nprevious approaches have focused only on the encoder, decoder, or\nreconstructing parts of the text. Pre-training a complete model allows it to be\ndirectly fine tuned for supervised (both sentence-level and document-level) and\nunsupervised machine translation, with no task-specific modifications. We\ndemonstrate that adding mBART initialization produces performance gains in all\nbut the highest-resource settings, including up to 12 BLEU points for low\nresource MT and over 5 BLEU points for many document-level and unsupervised\nmodels. We also show it also enables new types of transfer to language pairs\nwith no bi-text or that were not in the pre-training corpus, and present\nextensive analysis of which factors contribute the most to effective\npre-training.", "no": 15}, {"url": "https://arxiv.org/abs/2002.08909", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "cites": "1 463", "abstract": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.", "no": 16}, {"url": "https://arxiv.org/abs/2002.00388", "title": "A Survey on Knowledge Graphs: Representation, Acquisition and\n  Applications", "cites": "1 430", "abstract": "Human knowledge provides a formal understanding of the world. Knowledge\ngraphs that represent structural relations between entities have become an\nincreasingly popular research direction towards cognition and human-level\nintelligence. In this survey, we provide a comprehensive review of knowledge\ngraph covering overall research topics about 1) knowledge graph representation\nlearning, 2) knowledge acquisition and completion, 3) temporal knowledge graph,\nand 4) knowledge-aware applications, and summarize recent breakthroughs and\nperspective directions to facilitate future research. We propose a full-view\ncategorization and new taxonomies on these topics. Knowledge graph embedding is\norganized from four aspects of representation space, scoring function, encoding\nmodels, and auxiliary information. For knowledge acquisition, especially\nknowledge graph completion, embedding methods, path inference, and logical rule\nreasoning, are reviewed. We further explore several emerging topics, including\nmeta relational learning, commonsense reasoning, and temporal knowledge graphs.\nTo facilitate future research on knowledge graphs, we also provide a curated\ncollection of datasets and open-source libraries on different tasks. In the\nend, we have a thorough outlook on several promising research directions.", "no": 17}, {"url": "https://arxiv.org/abs/2003.07082", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human\n  Languages", "cites": "1 390", "abstract": "We introduce Stanza, an open-source Python natural language processing\ntoolkit supporting 66 human languages. Compared to existing widely used\ntoolkits, Stanza features a language-agnostic fully neural pipeline for text\nanalysis, including tokenization, multi-word token expansion, lemmatization,\npart-of-speech and morphological feature tagging, dependency parsing, and named\nentity recognition. We have trained Stanza on a total of 112 datasets,\nincluding the Universal Dependencies treebanks and other multilingual corpora,\nand show that the same neural architecture generalizes well and achieves\ncompetitive performance on all languages tested. Additionally, Stanza includes\na native Python interface to the widely used Java Stanford CoreNLP software,\nwhich further extends its functionality to cover other tasks such as\ncoreference resolution and relation extraction. Source code, documentation, and\npretrained models for 66 languages are available at\nhttps://stanfordnlp.github.io/stanza.", "no": 18}, {"url": "https://arxiv.org/abs/2101.00027", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "cites": "1 353", "abstract": "Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.", "no": 19}, {"url": "https://arxiv.org/abs/2001.07676", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural\n  Language Inference", "cites": "1 295", "abstract": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a\npretrained language model with \"task descriptions\" in natural language (e.g.,\nRadford et al., 2019). While this approach underperforms its supervised\ncounterpart, we show in this work that the two ideas can be combined: We\nintroduce Pattern-Exploiting Training (PET), a semi-supervised training\nprocedure that reformulates input examples as cloze-style phrases to help\nlanguage models understand a given task. These phrases are then used to assign\nsoft labels to a large set of unlabeled examples. Finally, standard supervised\ntraining is performed on the resulting training set. For several tasks and\nlanguages, PET outperforms supervised training and strong semi-supervised\napproaches in low-resource settings by a large margin.", "no": 20}, {"url": "https://arxiv.org/abs/2009.01325", "title": "Learning to summarize from human feedback", "cites": "1 256", "abstract": "As language models become more powerful, training and evaluation are\nincreasingly bottlenecked by the data and metrics used for a particular task.\nFor example, summarization models are often trained to predict human reference\nsummaries and evaluated using ROUGE, but both of these metrics are rough\nproxies for what we really care about -- summary quality. In this work, we show\nthat it is possible to significantly improve summary quality by training a\nmodel to optimize for human preferences. We collect a large, high-quality\ndataset of human comparisons between summaries, train a model to predict the\nhuman-preferred summary, and use that model as a reward function to fine-tune a\nsummarization policy using reinforcement learning. We apply our method to a\nversion of the TL;DR dataset of Reddit posts and find that our models\nsignificantly outperform both human reference summaries and much larger models\nfine-tuned with supervised learning alone. Our models also transfer to CNN/DM\nnews articles, producing summaries nearly as good as the human reference\nwithout any news-specific fine-tuning. We conduct extensive analyses to\nunderstand our human feedback dataset and fine-tuned models We establish that\nour reward model generalizes to new datasets, and that optimizing our reward\nmodel results in better summaries than optimizing ROUGE according to humans. We\nhope the evidence from our paper motivates machine learning researchers to pay\ncloser attention to how their training loss affects the model behavior they\nactually want.", "no": 21}, {"url": "https://arxiv.org/abs/2012.07805", "title": "Extracting Training Data from Large Language Models", "cites": "1 254", "abstract": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.", "no": 22}, {"url": "https://arxiv.org/abs/2002.12327", "title": "A Primer in BERTology: What we know about how BERT works", "cites": "1 247", "abstract": "Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.", "no": 23}, {"url": "https://arxiv.org/abs/2007.15779", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "cites": "1 233", "abstract": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.", "no": 24}, {"url": "https://arxiv.org/abs/2003.08271", "title": "Pre-trained Models for Natural Language Processing: A Survey", "cites": "1 224", "abstract": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.", "no": 25}, {"url": "https://arxiv.org/abs/2009.14794", "title": "Rethinking Attention with Performers", "cites": "1 193", "abstract": "We introduce Performers, Transformer architectures which can estimate regular\n(softmax) full-rank-attention Transformers with provable accuracy, but using\nonly linear (as opposed to quadratic) space and time complexity, without\nrelying on any priors such as sparsity or low-rankness. To approximate softmax\nattention-kernels, Performers use a novel Fast Attention Via positive\nOrthogonal Random features approach (FAVOR+), which may be of independent\ninterest for scalable kernel methods. FAVOR+ can be also used to efficiently\nmodel kernelizable attention mechanisms beyond softmax. This representational\npower is crucial to accurately compare softmax with other kernels for the first\ntime on large-scale tasks, beyond the reach of regular Transformers, and\ninvestigate optimal attention-kernels. Performers are linear architectures\nfully compatible with regular Transformers and with strong theoretical\nguarantees: unbiased or nearly-unbiased estimation of the attention matrix,\nuniform convergence and low estimation variance. We tested Performers on a rich\nset of tasks stretching from pixel-prediction through text models to protein\nsequence modeling. We demonstrate competitive results with other examined\nefficient sparse and dense attention methods, showcasing effectiveness of the\nnovel attention-learning paradigm leveraged by Performers.", "no": 26}, {"url": "https://arxiv.org/abs/2004.04696", "title": "BLEURT: Learning Robust Metrics for Text Generation", "cites": "1 130", "abstract": "Text generation has made significant advances in the last few years. Yet,\nevaluation metrics have lagged behind, as the most popular choices (e.g., BLEU\nand ROUGE) may correlate poorly with human judgments. We propose BLEURT, a\nlearned evaluation metric based on BERT that can model human judgments with a\nfew thousand possibly biased training examples. A key aspect of our approach is\na novel pre-training scheme that uses millions of synthetic examples to help\nthe model generalize. BLEURT provides state-of-the-art results on the last\nthree years of the WMT Metrics shared task and the WebNLG Competition dataset.\nIn contrast to a vanilla BERT-based approach, it yields superior results even\nwhen the training data is scarce and out-of-distribution.", "no": 27}, {"url": "https://arxiv.org/abs/2006.04558", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "cites": "1 038", "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can\nsynthesize speech significantly faster than previous autoregressive models with\ncomparable quality. The training of FastSpeech model relies on an\nautoregressive teacher model for duration prediction (to provide more\ninformation as input) and knowledge distillation (to simplify the data\ndistribution in output), which can ease the one-to-many mapping problem (i.e.,\nmultiple speech variations correspond to the same text) in TTS. However,\nFastSpeech has several disadvantages: 1) the teacher-student distillation\npipeline is complicated and time-consuming, 2) the duration extracted from the\nteacher model is not accurate enough, and the target mel-spectrograms distilled\nfrom teacher model suffer from information loss due to data simplification,\nboth of which limit the voice quality. In this paper, we propose FastSpeech 2,\nwhich addresses the issues in FastSpeech and better solves the one-to-many\nmapping problem in TTS by 1) directly training the model with ground-truth\ntarget instead of the simplified output from teacher, and 2) introducing more\nvariation information of speech (e.g., pitch, energy and more accurate\nduration) as conditional inputs. Specifically, we extract duration, pitch and\nenergy from speech waveform and directly take them as conditional inputs in\ntraining and use predicted values in inference. We further design FastSpeech\n2s, which is the first attempt to directly generate speech waveform from text\nin parallel, enjoying the benefit of fully end-to-end inference. Experimental\nresults show that 1) FastSpeech 2 achieves a 3x training speed-up over\nFastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech\n2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even\nsurpass autoregressive models. Audio samples are available at\nhttps://speechresearch.github.io/fastspeech2/.", "no": 28}, {"url": "https://arxiv.org/abs/2009.09761", "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis", "cites": "995", "abstract": "In this work, we propose DiffWave, a versatile diffusion probabilistic model\nfor conditional and unconditional waveform generation. The model is\nnon-autoregressive, and converts the white noise signal into structured\nwaveform through a Markov chain with a constant number of steps at synthesis.\nIt is efficiently trained by optimizing a variant of variational bound on the\ndata likelihood. DiffWave produces high-fidelity audios in different waveform\ngeneration tasks, including neural vocoding conditioned on mel spectrogram,\nclass-conditional generation, and unconditional generation. We demonstrate that\nDiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44\nversus 4.43), while synthesizing orders of magnitude faster. In particular, it\nsignificantly outperforms autoregressive and GAN-based waveform models in the\nchallenging unconditional generation task in terms of audio quality and sample\ndiversity from various automatic and human evaluations.", "no": 29}, {"url": "https://arxiv.org/abs/2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "cites": "971", "abstract": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.", "no": 30}, {"url": "https://arxiv.org/abs/2007.00808", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "cites": "943", "abstract": "Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.", "no": 31}, {"url": "https://arxiv.org/abs/2005.14050", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "cites": "935", "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing \"bias\" is an inherently normative process. We\nfurther find that these papers' proposed quantitative techniques for measuring\nor mitigating \"bias\" are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing \"bias\" in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of \"bias\"---i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements---and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.", "no": 32}, {"url": "https://arxiv.org/abs/2005.04118", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "cites": "931", "abstract": "Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.", "no": 33}, {"url": "https://arxiv.org/abs/2004.03705", "title": "Deep Learning Based Text Classification: A Comprehensive Review", "cites": "900", "abstract": "Deep learning based models have surpassed classical machine learning based\napproaches in various text classification tasks, including sentiment analysis,\nnews categorization, question answering, and natural language inference. In\nthis paper, we provide a comprehensive review of more than 150 deep learning\nbased models for text classification developed in recent years, and discuss\ntheir technical contributions, similarities, and strengths. We also provide a\nsummary of more than 40 popular datasets widely used for text classification.\nFinally, we provide a quantitative analysis of the performance of different\ndeep learning models on popular benchmarks, and discuss future research\ndirections.", "no": 34}, {"url": "https://arxiv.org/abs/2004.13637", "title": "Recipes for building an open-domain chatbot", "cites": "888", "abstract": "Building open-domain chatbots is a challenging area for machine learning\nresearch. While prior work has shown that scaling neural models in the number\nof parameters and the size of the data they are trained on gives improved\nresults, we show that other ingredients are important for a high-performing\nchatbot. Good conversation requires a number of skills that an expert\nconversationalist blends in a seamless way: providing engaging talking points\nand listening to their partners, and displaying knowledge, empathy and\npersonality appropriately, while maintaining a consistent persona. We show that\nlarge scale models can learn these skills when given appropriate training data\nand choice of generation strategy. We build variants of these recipes with 90M,\n2.7B and 9.4B parameter models, and make our models and code publicly\navailable. Human evaluations show our best models are superior to existing\napproaches in multi-turn dialogue in terms of engagingness and humanness\nmeasurements. We then discuss the limitations of this work by analyzing failure\ncases of our models.", "no": 35}, {"url": "https://arxiv.org/abs/2009.06732", "title": "Efficient Transformers: A Survey", "cites": "882", "abstract": "Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.", "no": 36}, {"url": "https://arxiv.org/abs/2002.10957", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "cites": "862", "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its\nvariants) have achieved remarkable success in varieties of NLP tasks. However,\nthese models usually consist of hundreds of millions of parameters which brings\nchallenges for fine-tuning and online serving in real-life applications due to\nlatency and capacity constraints. In this work, we present a simple and\neffective approach to compress large Transformer (Vaswani et al., 2017) based\npre-trained models, termed as deep self-attention distillation. The small model\n(student) is trained by deeply mimicking the self-attention module, which plays\na vital role in Transformer networks, of the large model (teacher).\nSpecifically, we propose distilling the self-attention module of the last\nTransformer layer of the teacher, which is effective and flexible for the\nstudent. Furthermore, we introduce the scaled dot-product between values in the\nself-attention module as the new deep self-attention knowledge, in addition to\nthe attention distributions (i.e., the scaled dot-product of queries and keys)\nthat have been used in existing works. Moreover, we show that introducing a\nteacher assistant (Mirzadeh et al., 2019) also helps the distillation of large\npre-trained Transformer models. Experimental results demonstrate that our\nmonolingual model outperforms state-of-the-art baselines in different parameter\nsize of student models. In particular, it retains more than 99% accuracy on\nSQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer\nparameters and computations of the teacher model. We also obtain competitive\nresults in applying deep self-attention distillation to multilingual\npre-trained models.", "no": 37}, {"url": "https://arxiv.org/abs/2003.11080", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization", "cites": "840", "abstract": "Much recent progress in applications of machine learning models to NLP has\nbeen driven by benchmarks that evaluate models across a wide variety of tasks.\nHowever, these broad-coverage benchmarks have been mostly limited to English,\nand despite an increasing interest in multilingual models, a benchmark that\nenables the comprehensive evaluation of such methods on a diverse range of\nlanguages and tasks is still missing. To this end, we introduce the\nCross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a\nmulti-task benchmark for evaluating the cross-lingual generalization\ncapabilities of multilingual representations across 40 languages and 9 tasks.\nWe demonstrate that while models tested on English reach human performance on\nmany tasks, there is still a sizable gap in the performance of cross-lingually\ntransferred models, particularly on syntactic and sentence retrieval tasks.\nThere is also a wide spread of results across languages. We release the\nbenchmark to encourage research on cross-lingual learning methods that transfer\nlinguistic knowledge across a diverse and representative set of languages and\ntasks.", "no": 38}, {"url": "https://arxiv.org/abs/2001.09977", "title": "Towards a Human-like Open-Domain Chatbot", "cites": "839", "abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data\nmined and filtered from public domain social media conversations. This 2.6B\nparameter neural network is simply trained to minimize perplexity of the next\ntoken. We also propose a human evaluation metric called Sensibleness and\nSpecificity Average (SSA), which captures key elements of a human-like\nmulti-turn conversation. Our experiments show strong correlation between\nperplexity and SSA. The fact that the best perplexity end-to-end trained Meena\nscores high on SSA (72% on multi-turn evaluation) suggests that a human-level\nSSA of 86% is potentially within reach if we can better optimize perplexity.\nAdditionally, the full version of Meena (with a filtering mechanism and tuned\ndecoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots\nwe evaluated.", "no": 39}, {"url": "https://arxiv.org/abs/2009.07118", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "cites": "820", "abstract": "When scaled to hundreds of billions of parameters, pretrained language models\nsuch as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.\nHowever, enormous amounts of compute are required for training and applying\nsuch big models, resulting in a large carbon footprint and making it difficult\nfor researchers and practitioners to use them. We show that performance similar\nto GPT-3 can be obtained with language models that are much \"greener\" in that\ntheir parameter count is several orders of magnitude smaller. This is achieved\nby converting textual inputs into cloze questions that contain a task\ndescription, combined with gradient-based optimization; exploiting unlabeled\ndata gives further improvements. We identify key factors required for\nsuccessful natural language understanding with small language models.", "no": 40}, {"url": "https://arxiv.org/abs/2009.11462", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\n  Models", "cites": "814", "abstract": "Pretrained neural language models (LMs) are prone to generating racist,\nsexist, or otherwise toxic language which hinders their safe deployment. We\ninvestigate the extent to which pretrained LMs can be prompted to generate\ntoxic language, and the effectiveness of controllable text generation\nalgorithms at preventing such toxic degeneration. We create and release\nRealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level\nprompts derived from a large corpus of English web text, paired with toxicity\nscores from a widely-used toxicity classifier. Using RealToxicityPrompts, we\nfind that pretrained LMs can degenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation\nmethods, and find that while data- or compute-intensive methods (e.g., adaptive\npretraining on non-toxic data) are more effective at steering away from\ntoxicity than simpler solutions (e.g., banning \"bad\" words), no current method\nis failsafe against neural toxic degeneration. To pinpoint the potential cause\nof such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et. al, 2019), and find a\nsignificant amount of offensive, factually unreliable, and otherwise toxic\ncontent. Our work provides a test bed for evaluating toxic generations by LMs\nand stresses the need for better data selection processes for pretraining.", "no": 41}, {"url": "https://arxiv.org/abs/2009.08366", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "cites": "804", "abstract": "Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.", "no": 42}, {"url": "https://arxiv.org/abs/2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain\n  Question Answering", "cites": "788", "abstract": "Generative models for open domain question answering have proven to be\ncompetitive, without resorting to external knowledge. While promising, this\napproach requires to use models with billions of parameters, which are\nexpensive to train and query. In this paper, we investigate how much these\nmodels can benefit from retrieving text passages, potentially containing\nevidence. We obtain state-of-the-art results on the Natural Questions and\nTriviaQA open benchmarks. Interestingly, we observe that the performance of\nthis method significantly improves when increasing the number of retrieved\npassages. This is evidence that generative models are good at aggregating and\ncombining evidence from multiple passages.", "no": 43}, {"url": "https://arxiv.org/abs/2004.09813", "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "cites": "777", "abstract": "We present an easy and efficient method to extend existing sentence embedding\nmodels to new languages. This allows to create multilingual versions from\npreviously monolingual models. The training is based on the idea that a\ntranslated sentence should be mapped to the same location in the vector space\nas the original sentence. We use the original (monolingual) model to generate\nsentence embeddings for the source language and then train a new system on\ntranslated sentences to mimic the original model. Compared to other methods for\ntraining multilingual sentence embeddings, this approach has several\nadvantages: It is easy to extend existing models with relatively few samples to\nnew languages, it is easier to ensure desired properties for the vector space,\nand the hardware requirements for training is lower. We demonstrate the\neffectiveness of our approach for 50+ languages from various language families.\nCode to extend sentence embeddings models to more than 400 languages is\npublicly available.", "no": 44}, {"url": "https://arxiv.org/abs/2006.16668", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "cites": "762", "abstract": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.", "no": 45}, {"url": "https://arxiv.org/abs/2004.09297", "title": "MPNet: Masked and Permuted Pre-training for Language Understanding", "cites": "760", "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the\nmost successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens\nthrough permuted language modeling (vs. MLM in BERT), and takes auxiliary\nposition information as input to make the model see a full sentence and thus\nreducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a\nlarge-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet\noutperforms MLM and PLM by a large margin, and achieves better results on these\ntasks compared with previous state-of-the-art pre-trained methods (e.g., BERT,\nXLNet, RoBERTa) under the same model setting. The code and the pre-trained\nmodels are available at: https://github.com/microsoft/MPNet.", "no": 46}, {"url": "https://arxiv.org/abs/2005.00661", "title": "On Faithfulness and Factuality in Abstractive Summarization", "cites": "760", "abstract": "It is well known that the standard likelihood training and approximate\ndecoding objectives in neural text generation models lead to less human-like\nresponses for open-ended tasks such as language modeling and story generation.\nIn this paper we have analyzed limitations of these models for abstractive\ndocument summarization and found that these models are highly prone to\nhallucinate content that is unfaithful to the input document. We conducted a\nlarge scale human evaluation of several neural abstractive summarization\nsystems to better understand the types of hallucinations they produce. Our\nhuman annotators found substantial amounts of hallucinated content in all model\ngenerated summaries. However, our analysis does show that pretrained models are\nbetter summarizers not only in terms of raw metrics, i.e., ROUGE, but also in\ngenerating faithful and factual summaries as evaluated by humans. Furthermore,\nwe show that textual entailment measures better correlate with faithfulness\nthan standard metrics, potentially leading the way to automatic evaluation\nmetrics as well as training and decoding criteria.", "no": 47}, {"url": "https://arxiv.org/abs/2004.10706", "title": "CORD-19: The COVID-19 Open Research Dataset", "cites": "748", "abstract": "The COVID-19 Open Research Dataset (CORD-19) is a growing resource of\nscientific papers on COVID-19 and related historical coronavirus research.\nCORD-19 is designed to facilitate the development of text mining and\ninformation retrieval systems over its rich collection of metadata and\nstructured full text papers. Since its release, CORD-19 has been downloaded\nover 200K times and has served as the basis of many COVID-19 text mining and\ndiscovery systems. In this article, we describe the mechanics of dataset\nconstruction, highlighting challenges and key design decisions, provide an\noverview of how CORD-19 has been used, and describe several shared tasks built\naround the dataset. We hope this resource will continue to bring together the\ncomputing community, biomedical experts, and policy makers in the search for\neffective treatments and management policies for COVID-19.", "no": 48}, {"url": "https://arxiv.org/abs/2003.00104", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding", "cites": "742", "abstract": "The Arabic language is a morphologically rich language with relatively few\nresources and a less explored syntax compared to English. Given these\nlimitations, Arabic Natural Language Processing (NLP) tasks like Sentiment\nAnalysis (SA), Named Entity Recognition (NER), and Question Answering (QA),\nhave proven to be very challenging to tackle. Recently, with the surge of\ntransformers based models, language-specific BERT based models have proven to\nbe very efficient at language understanding, provided they are pre-trained on a\nvery large corpus. Such models were able to set new standards and achieve\nstate-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT\nspecifically for the Arabic language in the pursuit of achieving the same\nsuccess that BERT did for the English language. The performance of AraBERT is\ncompared to multilingual BERT from Google and other state-of-the-art\napproaches. The results showed that the newly developed AraBERT achieved\nstate-of-the-art performance on most tested Arabic NLP tasks. The pretrained\naraBERT models are publicly available on https://github.com/aub-mind/arabert\nhoping to encourage research and applications for Arabic NLP.", "no": 49}, {"url": "https://arxiv.org/abs/2005.10200", "title": "BERTweet: A pre-trained language model for English Tweets", "cites": "734", "abstract": "We present BERTweet, the first public large-scale pre-trained language model\nfor English Tweets. Our BERTweet, having the same architecture as BERT-base\n(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu\net al., 2019). Experiments show that BERTweet outperforms strong baselines\nRoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better\nperformance results than the previous state-of-the-art models on three Tweet\nNLP tasks: Part-of-speech tagging, Named-entity recognition and text\nclassification. We release BERTweet under the MIT License to facilitate future\nresearch and applications on Tweet data. Our BERTweet is available at\nhttps://github.com/VinAIResearch/BERTweet", "no": 50}, {"url": "https://arxiv.org/abs/2009.09025", "title": "COMET: A Neural Framework for MT Evaluation", "cites": "724", "abstract": "We present COMET, a neural framework for training multilingual machine\ntranslation evaluation models which obtains new state-of-the-art levels of\ncorrelation with human judgements. Our framework leverages recent breakthroughs\nin cross-lingual pretrained language modeling resulting in highly multilingual\nand adaptable MT evaluation models that exploit information from both the\nsource input and a target-language reference translation in order to more\naccurately predict MT quality. To showcase our framework, we train three models\nwith different types of human judgements: Direct Assessments, Human-mediated\nTranslation Edit Rate and Multidimensional Quality Metrics. Our models achieve\nnew state-of-the-art performance on the WMT 2019 Metrics shared task and\ndemonstrate robustness to high-performing systems.", "no": 51}, {"url": "https://arxiv.org/abs/2002.08910", "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "cites": "702", "abstract": "It has recently been observed that neural language models trained on\nunstructured text can implicitly store and retrieve knowledge using natural\nlanguage queries. In this short paper, we measure the practical utility of this\napproach by fine-tuning pre-trained models to answer questions without access\nto any external context or knowledge. We show that this approach scales with\nmodel size and performs competitively with open-domain systems that explicitly\nretrieve answers from an external knowledge source when answering questions. To\nfacilitate reproducibility and future work, we release our code and trained\nmodels at https://goo.gle/t5-cbqa.", "no": 52}, {"url": "https://arxiv.org/abs/2004.09456", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "cites": "698", "abstract": "A stereotype is an over-generalized belief about a particular group of\npeople, e.g., Asians are good at math or Asians are bad drivers. Such beliefs\n(biases) are known to hurt target groups. Since pretrained language models are\ntrained on large real world data, they are known to capture stereotypical\nbiases. In order to assess the adverse effects of these models, it is important\nto quantify the bias captured in them. Existing literature on quantifying bias\nevaluates pretrained language models on a small set of artificially constructed\nbias-assessing sentences. We present StereoSet, a large-scale natural dataset\nin English to measure stereotypical biases in four domains: gender, profession,\nrace, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and\nXLNet on our dataset and show that these models exhibit strong stereotypical\nbiases. We also present a leaderboard with a hidden test set to track the bias\nof future language models at https://stereoset.mit.edu", "no": 53}, {"url": "https://arxiv.org/abs/2002.04745", "title": "On Layer Normalization in the Transformer Architecture", "cites": "697", "abstract": "The Transformer is widely used in natural language processing tasks. To train\na Transformer however, one usually needs a carefully designed learning rate\nwarm-up stage, which is shown to be crucial to the final performance but will\nslow down the optimization and bring more hyper-parameter tunings. In this\npaper, we first study theoretically why the learning rate warm-up stage is\nessential and show that the location of layer normalization matters.\nSpecifically, we prove with mean field theory that at initialization, for the\noriginal-designed Post-LN Transformer, which places the layer normalization\nbetween the residual blocks, the expected gradients of the parameters near the\noutput layer are large. Therefore, using a large learning rate on those\ngradients makes the training unstable. The warm-up stage is practically helpful\nfor avoiding this problem. On the other hand, our theory also shows that if the\nlayer normalization is put inside the residual blocks (recently proposed as\nPre-LN Transformer), the gradients are well-behaved at initialization. This\nmotivates us to remove the warm-up stage for the training of Pre-LN\nTransformers. We show in our experiments that Pre-LN Transformers without the\nwarm-up stage can reach comparable results with baselines while requiring\nsignificantly less training time and hyper-parameter tuning on a wide range of\napplications.", "no": 54}, {"url": "https://arxiv.org/abs/2007.06225", "title": "ProtTrans: Towards Cracking the Language of Life's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "cites": "668", "abstract": "Computational biology and bioinformatics provide vast data gold-mines from\nprotein sequences, ideal for Language Models taken from NLP. These LMs reach\nfor new prediction frontiers at low inference costs. Here, we trained two\nauto-regressive models (Transformer-XL, XLNet) and four auto-encoder models\n(BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393\nbillion amino acids. The LMs were trained on the Summit supercomputer using\n5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that\nthe raw protein LM-embeddings from unlabeled data captured some biophysical\nfeatures of protein sequences. We validated the advantage of using the\nembeddings as exclusive input for several subsequent tasks. The first was a\nper-residue prediction of protein secondary structure (3-state accuracy\nQ3=81%-87%); the second were per-protein predictions of protein sub-cellular\nlocalization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble\n(2-state accuracy Q2=91%). For the per-residue predictions the transfer of the\nmost informative embeddings (ProtT5) for the first time outperformed the\nstate-of-the-art without using evolutionary information thereby bypassing\nexpensive database searches. Taken together, the results implied that protein\nLMs learned some of the grammar of the language of life. To facilitate future\nwork, we released our models at https://github.com/agemagician/ProtTrans.", "no": 55}, {"url": "https://arxiv.org/abs/2007.01852", "title": "Language-agnostic BERT Sentence Embedding", "cites": "664", "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.", "no": 56}, {"url": "https://arxiv.org/abs/2004.02984", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "cites": "663", "abstract": "Natural Language Processing (NLP) has recently achieved great success by\nusing huge pre-trained models with hundreds of millions of parameters. However,\nthese models suffer from heavy model sizes and high latency such that they\ncannot be deployed to resource-limited mobile devices. In this paper, we\npropose MobileBERT for compressing and accelerating the popular BERT model.\nLike the original BERT, MobileBERT is task-agnostic, that is, it can be\ngenerically applied to various downstream NLP tasks via simple fine-tuning.\nBasically, MobileBERT is a thin version of BERT_LARGE, while equipped with\nbottleneck structures and a carefully designed balance between self-attentions\nand feed-forward networks. To train MobileBERT, we first train a specially\ndesigned teacher model, an inverted-bottleneck incorporated BERT_LARGE model.\nThen, we conduct knowledge transfer from this teacher to MobileBERT. Empirical\nstudies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE\nwhile achieving competitive results on well-known benchmarks. On the natural\nlanguage inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6\nlower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD\nv1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n90.0/79.2 (1.5/2.1 higher than BERT_BASE).", "no": 57}, {"url": "https://arxiv.org/abs/2010.11125", "title": "Beyond English-Centric Multilingual Machine Translation", "cites": "652", "abstract": "Existing work in translation demonstrated the potential of massively\nmultilingual machine translation by training a single model able to translate\nbetween any pair of languages. However, much of this work is English-Centric by\ntraining only on data which was translated from or to English. While this is\nsupported by large sources of training data, it does not reflect translation\nneeds worldwide. In this work, we create a true Many-to-Many multilingual\ntranslation model that can translate directly between any pair of 100\nlanguages. We build and open source a training dataset that covers thousands of\nlanguage directions with supervised data, created through large-scale mining.\nThen, we explore how to effectively increase model capacity through a\ncombination of dense scaling and language-specific sparse parameters to create\nhigh quality models. Our focus on non-English-Centric models brings gains of\nmore than 10 BLEU when directly translating between non-English directions\nwhile performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and\nfinal M2M-100 model.", "no": 58}, {"url": "https://arxiv.org/abs/2005.00700", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "cites": "630", "abstract": "Question answering (QA) tasks have been posed using a variety of formats,\nsuch as extractive span selection, multiple choice, etc. This has led to\nformat-specialized models, and even to an implicit division in the QA\ncommunity. We argue that such boundaries are artificial and perhaps\nunnecessary, given the reasoning abilities we seek to teach are not governed by\nthe format. As evidence, we use the latest advances in language modeling to\nbuild a single pre-trained QA model, UnifiedQA, that performs surprisingly well\nacross 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par\nwith 9 different models that were trained on individual datasets themselves.\nEven when faced with 12 unseen datasets of observed formats, UnifiedQA performs\nsurprisingly well, showing strong generalization from its out-of-format\ntraining data. Finally, simply fine-tuning this pre-trained QA model into\nspecialized models results in a new state of the art on 6 datasets,\nestablishing UnifiedQA as a strong starting point for building QA systems.", "no": 59}, {"url": "https://arxiv.org/abs/2004.09095", "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "cites": "614", "abstract": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.", "no": 60}, {"url": "https://arxiv.org/abs/2005.00247", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "cites": "602", "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.", "no": 61}, {"url": "https://arxiv.org/abs/2006.13979", "title": "Unsupervised Cross-lingual Representation Learning for Speech\n  Recognition", "cites": "587", "abstract": "This paper presents XLSR which learns cross-lingual speech representations by\npretraining a single model from the raw waveform of speech in multiple\nlanguages. We build on wav2vec 2.0 which is trained by solving a contrastive\ntask over masked latent speech representations and jointly learns a\nquantization of the latents shared across languages. The resulting model is\nfine-tuned on labeled data and experiments show that cross-lingual pretraining\nsignificantly outperforms monolingual pretraining. On the CommonVoice\nbenchmark, XLSR shows a relative phoneme error rate reduction of 72% compared\nto the best known results. On BABEL, our approach improves word error rate by\n16% relative compared to a comparable system. Our approach enables a single\nmultilingual speech recognition model which is competitive to strong individual\nmodels. Analysis shows that the latent discrete speech representations are\nshared across languages with increased sharing for related languages. We hope\nto catalyze research in low-resource speech understanding by releasing XLSR-53,\na large model pretrained in 53 languages.", "no": 62}, {"url": "https://arxiv.org/abs/2005.00928", "title": "Quantifying Attention Flow in Transformers", "cites": "565", "abstract": "In the Transformer model, \"self-attention\" combines information from attended\nembeddings into the representation of the focal embedding in the next layer.\nThus, across layers of the Transformer, information originating from different\ntokens gets increasingly mixed. This makes attention weights unreliable as\nexplanations probes. In this paper, we consider the problem of quantifying this\nflow of information through self-attention. We propose two methods for\napproximating the attention to input tokens given attention weights, attention\nrollout and attention flow, as post hoc methods when we use attention weights\nas the relative relevance of the input tokens. We show that these methods give\ncomplementary views on the flow of information, and compared to raw attention,\nboth yield higher correlations with importance scores of input tokens obtained\nusing an ablation method and input gradients.", "no": 63}, {"url": "https://arxiv.org/abs/2010.01057", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "cites": "564", "abstract": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.", "no": 64}, {"url": "https://arxiv.org/abs/2010.12421", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "cites": "551", "abstract": "The experimental landscape in natural language processing for social media is\ntoo fragmented. Each year, new shared tasks and datasets are proposed, ranging\nfrom classics like sentiment analysis to irony detection or emoji prediction.\nTherefore, it is unclear what the current state of the art is, as there is no\nstandardized evaluation protocol, neither a strong set of baselines trained on\nsuch domain-specific data. In this paper, we propose a new evaluation framework\n(TweetEval) consisting of seven heterogeneous Twitter-specific classification\ntasks. We also provide a strong set of baselines as starting point, and compare\ndifferent language modeling pre-training strategies. Our initial experiments\nshow the effectiveness of starting off with existing pre-trained generic\nlanguage models, and continue training them on Twitter corpora.", "no": 65}, {"url": "https://arxiv.org/abs/2004.09984", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "cites": "549", "abstract": "Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.", "no": 66}, {"url": "https://arxiv.org/abs/2004.13922", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing", "cites": "543", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has shown\nmarvelous improvements across various NLP tasks, and consecutive variants have\nbeen proposed to further improve the performance of the pre-trained language\nmodels. In this paper, we target on revisiting Chinese pre-trained language\nmodels to examine their effectiveness in a non-English language and release the\nChinese pre-trained language model series to the community. We also propose a\nsimple but effective model called MacBERT, which improves upon RoBERTa in\nseveral ways, especially the masking strategy that adopts MLM as correction\n(Mac). We carried out extensive experiments on eight Chinese NLP tasks to\nrevisit the existing pre-trained language models as well as the proposed\nMacBERT. Experimental results show that MacBERT could achieve state-of-the-art\nperformances on many NLP tasks, and we also ablate details with several\nfindings that may help future research. Resources available:\nhttps://github.com/ymcui/MacBERT", "no": 67}, {"url": "https://arxiv.org/abs/2011.04006", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "cites": "535", "abstract": "Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.", "no": 68}, {"url": "https://arxiv.org/abs/2005.00052", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "cites": "515", "abstract": "The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml", "no": 69}, {"url": "https://arxiv.org/abs/2007.12626", "title": "SummEval: Re-evaluating Summarization Evaluation", "cites": "515", "abstract": "The scarcity of comprehensive up-to-date studies on evaluation metrics for\ntext summarization and the lack of consensus regarding evaluation protocols\ncontinue to inhibit progress. We address the existing shortcomings of\nsummarization evaluation methods along five dimensions: 1) we re-evaluate 14\nautomatic evaluation metrics in a comprehensive and consistent fashion using\nneural summarization model outputs along with expert and crowd-sourced human\nannotations, 2) we consistently benchmark 23 recent summarization models using\nthe aforementioned automatic evaluation metrics, 3) we assemble the largest\ncollection of summaries generated by models trained on the CNN/DailyMail news\ndataset and share it in a unified format, 4) we implement and share a toolkit\nthat provides an extensible and unified API for evaluating summarization models\nacross a broad range of automatic metrics, 5) we assemble and share the largest\nand most diverse, in terms of model types, collection of human judgments of\nmodel-generated summaries on the CNN/Daily Mail dataset annotated by both\nexpert judges and crowd-source workers. We hope that this work will help\npromote a more complete evaluation protocol for text summarization as well as\nadvance research in developing evaluation metrics that better correlate with\nhuman judgments.", "no": 70}, {"url": "https://arxiv.org/abs/2007.07779", "title": "AdapterHub: A Framework for Adapting Transformers", "cites": "511", "abstract": "The current modus operandi in NLP involves downloading and fine-tuning\npre-trained models consisting of millions or billions of parameters. Storing\nand sharing such large trained models is expensive, slow, and time-consuming,\nwhich impedes progress towards more general and versatile NLP methods that\nlearn from and for many tasks. Adapters -- small learnt bottleneck layers\ninserted within each layer of a pre-trained model -- ameliorate this issue by\navoiding full fine-tuning of the entire model. However, sharing and integrating\nadapter layers is not straightforward. We propose AdapterHub, a framework that\nallows dynamic \"stitching-in\" of pre-trained adapters for different tasks and\nlanguages. The framework, built on top of the popular HuggingFace Transformers\nlibrary, enables extremely easy and quick adaptations of state-of-the-art\npre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\nDownloading, sharing, and training adapters is as seamless as possible using\nminimal changes to the training scripts and a specialized infrastructure. Our\nframework enables scalable and easy access to sharing of task-specific models,\nparticularly in low-resource scenarios. AdapterHub includes all recent adapter\narchitectures and can be found at https://AdapterHub.ml.", "no": 71}, {"url": "https://arxiv.org/abs/2005.00547", "title": "GoEmotions: A Dataset of Fine-Grained Emotions", "cites": "507", "abstract": "Understanding emotion expressed in language has a wide range of applications,\nfrom building empathetic chatbots to detecting harmful online behavior.\nAdvancement in this area can be improved using large-scale datasets with a\nfine-grained typology, adaptable to multiple downstream tasks. We introduce\nGoEmotions, the largest manually annotated dataset of 58k English Reddit\ncomments, labeled for 27 emotion categories or Neutral. We demonstrate the high\nquality of the annotations via Principal Preserved Component Analysis. We\nconduct transfer learning experiments with existing emotion benchmarks to show\nthat our dataset generalizes well to other domains and different emotion\ntaxonomies. Our BERT-based model achieves an average F1-score of .46 across our\nproposed taxonomy, leaving much room for improvement.", "no": 72}, {"url": "https://arxiv.org/abs/2004.02349", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "cites": "506", "abstract": "Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art.", "no": 73}, {"url": "https://arxiv.org/abs/2010.00747", "title": "Contrastive Learning of Medical Visual Representations from Paired\n  Images and Text", "cites": "505", "abstract": "Learning visual representations of medical images (e.g., X-rays) is core to\nmedical image understanding but its progress has been held back by the scarcity\nof human annotations. Existing work commonly relies on fine-tuning weights\ntransferred from ImageNet pretraining, which is suboptimal due to drastically\ndifferent image characteristics, or rule-based label extraction from the\ntextual report data paired with medical images, which is inaccurate and hard to\ngeneralize. Meanwhile, several recent studies show exciting results from\nunsupervised contrastive learning from natural images, but we find these\nmethods help little on medical images because of their high inter-class\nsimilarity. We propose ConVIRT, an alternative unsupervised strategy to learn\nmedical visual representations by exploiting naturally occurring paired\ndescriptive text. Our new method of pretraining medical image encoders with the\npaired text data via a bidirectional contrastive objective between the two\nmodalities is domain-agnostic, and requires no additional expert input. We test\nConVIRT by transferring our pretrained weights to 4 medical image\nclassification tasks and 2 zero-shot retrieval tasks, and show that it leads to\nimage representations that considerably outperform strong baselines in most\nsettings. Notably, in all 4 classification tasks, our method requires only 10\\%\nas much labeled training data as an ImageNet initialized counterpart to achieve\nbetter or comparable performance, demonstrating superior data efficiency.", "no": 74}, {"url": "https://arxiv.org/abs/2002.06305", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data\n  Orders, and Early Stopping", "cites": "501", "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.", "no": 75}, {"url": "https://arxiv.org/abs/2010.06467", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "cites": "493", "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.", "no": 76}, {"url": "https://arxiv.org/abs/2010.08191", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "cites": "489", "abstract": "In open-domain question answering, dense passage retrieval has become a new\nparadigm to retrieve relevant passages for finding answers. Typically, the\ndual-encoder architecture is adopted to learn dense representations of\nquestions and passages for semantic matching. However, it is difficult to\neffectively train a dual-encoder due to the challenges including the\ndiscrepancy between training and inference, the existence of unlabeled\npositives and limited training data. To address these challenges, we propose an\noptimized training approach, called RocketQA, to improving dense passage\nretrieval. We make three major technical contributions in RocketQA, namely\ncross-batch negatives, denoised hard negatives and data augmentation. The\nexperiment results show that RocketQA significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions. We also conduct\nextensive experiments to examine the effectiveness of the three strategies in\nRocketQA. Besides, we demonstrate that the performance of end-to-end QA can be\nimproved based on our RocketQA retriever.", "no": 77}, {"url": "https://arxiv.org/abs/2002.01808", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "cites": "467", "abstract": "We study the problem of injecting knowledge into large pre-trained models\nlike BERT and RoBERTa. Existing methods typically update the original\nparameters of pre-trained models when injecting knowledge. However, when\nmultiple kinds of knowledge are injected, the historically injected knowledge\nwould be flushed away. To address this, we propose K-Adapter, a framework that\nretains the original parameters of the pre-trained model fixed and supports the\ndevelopment of versatile knowledge-infused model. Taking RoBERTa as the\nbackbone model, K-Adapter has a neural adapter for each kind of infused\nknowledge, like a plug-in connected to RoBERTa. There is no information flow\nbetween different adapters, thus multiple adapters can be efficiently trained\nin a distributed way. As a case study, we inject two kinds of knowledge in this\nwork, including (1) factual knowledge obtained from automatically aligned\ntext-triplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained\nvia dependency parsing. Results on three knowledge-driven tasks, including\nrelation classification, entity typing, and question answering, demonstrate\nthat each adapter improves the performance and the combination of both adapters\nbrings further improvements. Further analysis indicates that K-Adapter captures\nversatile knowledge than RoBERTa.", "no": 78}, {"url": "https://arxiv.org/abs/2003.05002", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in\n  Typologically Diverse Languages", "cites": "464", "abstract": "Confidently making progress on multilingual modeling requires challenging,\ntrustworthy evaluations. We present TyDi QA---a question answering dataset\ncovering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology---the set of\nlinguistic features each language expresses---such that we expect models\nperforming well on this set to generalize across a large number of the world's\nlanguages. We present a quantitative analysis of the data quality and\nexample-level qualitative linguistic analyses of observed language phenomena\nthat would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by\npeople who want to know the answer, but don't know the answer yet, and the data\nis collected directly in each language without the use of translation.", "no": 79}, {"url": "https://arxiv.org/abs/2005.00796", "title": "A Simple Language Model for Task-Oriented Dialogue", "cites": "462", "abstract": "Task-oriented dialogue is often decomposed into three tasks: understanding\nuser input, deciding actions, and generating a response. While such\ndecomposition might suggest a dedicated model for each sub-task, we find a\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\nsingle, causal language model trained on all sub-tasks recast as a single\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\nlearning from pre-trained, open domain, causal language models such as GPT-2.\nSimpleTOD improves over the prior state-of-the-art in joint goal accuracy for\ndialogue state tracking, and our analysis reveals robustness to noisy\nannotations in this setting. SimpleTOD also improves the main metrics used to\nevaluate action decisions and response generation in an end-to-end setting:\ninform rate by 8.1 points, success rate by 9.7 points, and combined score by\n7.2 points.", "no": 80}, {"url": "https://arxiv.org/abs/2005.12833", "title": "Med-BERT: pre-trained contextualized embeddings on large-scale\n  structured electronic health records for disease prediction", "cites": "453", "abstract": "Deep learning (DL) based predictive models from electronic health records\n(EHR) deliver impressive performance in many clinical tasks. Large training\ncohorts, however, are often required to achieve high accuracy, hindering the\nadoption of DL-based models in scenarios with limited training data size.\nRecently, bidirectional encoder representations from transformers (BERT) and\nrelated models have achieved tremendous successes in the natural language\nprocessing domain. The pre-training of BERT on a very large training corpus\ngenerates contextualized embeddings that can boost the performance of models\ntrained on smaller datasets. We propose Med-BERT, which adapts the BERT\nframework for pre-training contextualized embedding models on structured\ndiagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments\nare conducted on two disease-prediction tasks: (1) prediction of heart failure\nin patients with diabetes and (2) prediction of pancreatic cancer from two\nclinical databases. Med-BERT substantially improves prediction accuracy,\nboosting the area under receiver operating characteristics curve (AUC) by\n2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the\nperformance of tasks with very small fine-tuning training sets (300-500\nsamples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times\nlarger training set. We believe that Med-BERT will benefit disease-prediction\nstudies with small local training datasets, reduce data collection expenses,\nand accelerate the pace of artificial intelligence aided healthcare.", "no": 81}, {"url": "https://arxiv.org/abs/2005.08314", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "cites": "451", "abstract": "Recent years have witnessed the burgeoning of pretrained language models\n(LMs) for text-based natural language (NL) understanding tasks. Such models are\ntypically trained on free-form NL text, hence may not be suitable for tasks\nlike semantic parsing over structured data, which require reasoning over both\nfree-form NL questions and structured tabular data (e.g., database tables). In\nthis paper we present TaBERT, a pretrained LM that jointly learns\nrepresentations for NL sentences and (semi-)structured tables. TaBERT is\ntrained on a large corpus of 26 million tables and their English contexts. In\nexperiments, neural semantic parsers using TaBERT as feature representation\nlayers achieve new best results on the challenging weakly-supervised semantic\nparsing benchmark WikiTableQuestions, while performing competitively on the\ntext-to-SQL dataset Spider. Implementation of the model will be available at\nhttp://fburl.com/TaBERT .", "no": 82}, {"url": "https://arxiv.org/abs/2010.00133", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\n  Language Models", "cites": "450", "abstract": "Pretrained language models, especially masked language models (MLMs) have\nseen success across many NLP tasks. However, there is ample evidence that they\nuse the cultural biases that are undoubtedly present in the corpora they are\ntrained on, implicitly creating harm with biased representations. To measure\nsome forms of social bias in language models against protected demographic\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\nis presented with two sentences: one that is more stereotyping and another that\nis less stereotyping. The data focuses on stereotypes about historically\ndisadvantaged groups and contrasts them with advantaged groups. We find that\nall three of the widely-used MLMs we evaluate substantially favor sentences\nthat express stereotypes in every category in CrowS-Pairs. As work on building\nless biased models advances, this dataset can be used as a benchmark to\nevaluate progress.", "no": 83}, {"url": "https://arxiv.org/abs/2004.03685", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and\n  evaluate faithfulness?", "cites": "446", "abstract": "With the growing popularity of deep-learning based NLP models, comes a need\nfor interpretable systems. But what is interpretability, and what constitutes a\nhigh-quality interpretation? In this opinion piece we reflect on the current\nstate of interpretability evaluation research. We call for more clearly\ndifferentiating between different desired criteria an interpretation should\nsatisfy, and focus on the faithfulness criteria. We survey the literature with\nrespect to faithfulness evaluation, and arrange the current approaches around\nthree assumptions, providing an explicit form to how faithfulness is \"defined\"\nby the community. We provide concrete guidelines on how evaluation of\ninterpretation methods should and should not be conducted. Finally, we claim\nthat the current binary definition for faithfulness sets a potentially\nunrealistic bar for being considered faithful. We call for discarding the\nbinary notion of faithfulness in favor of a more graded one, which we believe\nwill be of greater practical utility.", "no": 84}, {"url": "https://arxiv.org/abs/2004.10643", "title": "Universal Dependencies v2: An Evergrowing Multilingual Treebank\n  Collection", "cites": "446", "abstract": "Universal Dependencies is an open community effort to create\ncross-linguistically consistent treebank annotation for many languages within a\ndependency-based lexicalist framework. The annotation consists in a\nlinguistically motivated word segmentation; a morphological layer comprising\nlemmas, universal part-of-speech tags, and standardized morphological features;\nand a syntactic layer focusing on syntactic relations between predicates,\narguments and modifiers. In this paper, we describe version 2 of the guidelines\n(UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of\nthe currently available treebanks for 90 languages.", "no": 85}, {"url": "https://arxiv.org/abs/2004.01970", "title": "BAE: BERT-based Adversarial Examples for Text Classification", "cites": "443", "abstract": "Modern text classification models are susceptible to adversarial examples,\nperturbed versions of the original text indiscernible by humans which get\nmisclassified by the model. Recent works in NLP use rule-based synonym\nreplacement strategies to generate adversarial examples. These strategies can\nlead to out-of-context and unnaturally complex token replacements, which are\neasily identifiable by humans. We present BAE, a black box attack for\ngenerating adversarial examples using contextual perturbations from a BERT\nmasked language model. BAE replaces and inserts tokens in the original text by\nmasking a portion of the text and leveraging the BERT-MLM to generate\nalternatives for the masked tokens. Through automatic and human evaluations, we\nshow that BAE performs a stronger attack, in addition to generating adversarial\nexamples with improved grammaticality and semantic coherence as compared to\nprior work.", "no": 86}, {"url": "https://arxiv.org/abs/2006.07235", "title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "cites": "440", "abstract": "We present the results and main findings of SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval\n2020). The task involves three subtasks corresponding to the hierarchical\ntaxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The\ntask featured five languages: English, Arabic, Danish, Greek, and Turkish for\nSubtask A. In addition, English also featured Subtasks B and C. OffensEval 2020\nwas one of the most popular tasks at SemEval-2020 attracting a large number of\nparticipants across all subtasks and also across all languages. A total of 528\nteams signed up to participate in the task, 145 teams submitted systems during\nthe evaluation period, and 70 submitted system description papers.", "no": 87}, {"url": "https://arxiv.org/abs/2005.04790", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "cites": "435", "abstract": "This work proposes a new challenge set for multimodal classification,\nfocusing on detecting hate speech in multimodal memes. It is constructed such\nthat unimodal models struggle and only multimodal models can succeed: difficult\nexamples (\"benign confounders\") are added to the dataset to make it hard to\nrely on unimodal signals. The task requires subtle reasoning, yet is\nstraightforward to evaluate as a binary classification problem. We provide\nbaseline performance numbers for unimodal models, as well as for multimodal\nmodels with various degrees of sophistication. We find that state-of-the-art\nmethods perform poorly compared to humans (64.73% vs. 84.7% accuracy),\nillustrating the difficulty of the task and highlighting the challenge that\nthis important problem poses to the community.", "no": 88}, {"url": "https://arxiv.org/abs/2012.14913", "title": "Transformer Feed-Forward Layers Are Key-Value Memories", "cites": "435", "abstract": "Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.", "no": 89}, {"url": "https://arxiv.org/abs/2006.06195", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "cites": "432", "abstract": "We present VILLA, the first known effort on large-scale adversarial training\nfor vision-and-language (V+L) representation learning. VILLA consists of two\ntraining stages: (i) task-agnostic adversarial pre-training; followed by (ii)\ntask-specific adversarial finetuning. Instead of adding adversarial\nperturbations on image pixels and textual tokens, we propose to perform\nadversarial training in the embedding space of each modality. To enable\nlarge-scale training, we adopt the \"free\" adversarial training strategy, and\ncombine it with KL-divergence-based regularization to promote higher invariance\nin the embedding space. We apply VILLA to current best-performing V+L models,\nand achieve new state of the art on a wide range of tasks, including Visual\nQuestion Answering, Visual Commonsense Reasoning, Image-Text Retrieval,\nReferring Expression Comprehension, Visual Entailment, and NLVR2.", "no": 90}, {"url": "https://arxiv.org/abs/2009.02252", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "cites": "430", "abstract": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.", "no": 91}, {"url": "https://arxiv.org/abs/2012.10289", "title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection", "cites": "429", "abstract": "Hate speech is a challenging issue plaguing the online social media. While\nbetter models for hate speech detection are continuously being developed, there\nis little research on the bias and interpretability aspects of hate speech. In\nthis paper, we introduce HateXplain, the first benchmark hate speech dataset\ncovering multiple aspects of the issue. Each post in our dataset is annotated\nfrom three different perspectives: the basic, commonly used 3-class\nclassification (i.e., hate, offensive or normal), the target community (i.e.,\nthe community that has been the victim of hate speech/offensive speech in the\npost), and the rationales, i.e., the portions of the post on which their\nlabelling decision (as hate, offensive or normal) is based. We utilize existing\nstate-of-the-art models and observe that even models that perform very well in\nclassification do not score high on explainability metrics like model\nplausibility and faithfulness. We also observe that models, which utilize the\nhuman rationales for training, perform better in reducing unintended bias\ntowards target communities. We have made our code and dataset public at\nhttps://github.com/punyajoy/HateXplain", "no": 92}, {"url": "https://arxiv.org/abs/2006.03659", "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual\n  Representations", "cites": "424", "abstract": "Sentence embeddings are an important component of many natural language\nprocessing (NLP) systems. Like word embeddings, sentence embeddings are\ntypically learned on large text corpora and then transferred to various\ndownstream tasks, such as clustering and retrieval. Unlike word embeddings, the\nhighest performing solutions for learning sentence embeddings require labelled\ndata, limiting their usefulness to languages and domains where labelled data is\nabundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for\nUnsupervised Textual Representations. Inspired by recent advances in deep\nmetric learning (DML), we carefully design a self-supervised objective for\nlearning universal sentence embeddings that does not require labelled training\ndata. When used to extend the pretraining of transformer-based language models,\nour approach closes the performance gap between unsupervised and supervised\npretraining for universal sentence encoders. Importantly, our experiments\nsuggest that the quality of the learned embeddings scale with both the number\nof trainable parameters and the amount of unlabelled training data. Our code\nand pretrained models are publicly available and can be easily adapted to new\ndomains or used to embed unseen text.", "no": 93}, {"url": "https://arxiv.org/abs/2004.12362", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "cites": "423", "abstract": "Aspect-based sentiment analysis aims to determine the sentiment polarity\ntowards a specific aspect in online reviews. Most recent efforts adopt\nattention-based neural network models to implicitly connect aspects with\nopinion words. However, due to the complexity of language and the existence of\nmultiple aspects in a single sentence, these models often confuse the\nconnections. In this paper, we address this problem by means of effective\nencoding of syntax information. Firstly, we define a unified aspect-oriented\ndependency tree structure rooted at a target aspect by reshaping and pruning an\nordinary dependency parse tree. Then, we propose a relational graph attention\nnetwork (R-GAT) to encode the new tree structure for sentiment prediction.\nExtensive experiments are conducted on the SemEval 2014 and Twitter datasets,\nand the experimental results confirm that the connections between aspects and\nopinion words can be better established with our approach, and the performance\nof the graph attention network (GAT) is significantly improved as a\nconsequence.", "no": 94}, {"url": "https://arxiv.org/abs/2005.00200", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "cites": "423", "abstract": "We present HERO, a novel framework for large-scale video+language\nomni-representation learning. HERO encodes multimodal inputs in a hierarchical\nstructure, where local context of a video frame is captured by a Cross-modal\nTransformer via multimodal fusion, and global video context is captured by a\nTemporal Transformer. In addition to standard Masked Language Modeling (MLM)\nand Masked Frame Modeling (MFM) objectives, we design two new pre-training\ntasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global\nand local temporal alignment; and (ii) Frame Order Modeling (FOM), where the\nmodel predicts the right order of shuffled video frames. HERO is jointly\ntrained on HowTo100M and large-scale TV datasets to gain deep understanding of\ncomplex social dynamics with multi-character interactions. Comprehensive\nexperiments demonstrate that HERO achieves new state of the art on multiple\nbenchmarks over Text-based Video/Video-moment Retrieval, Video Question\nAnswering (QA), Video-and-language Inference and Video Captioning tasks across\ndifferent domains. We also introduce two new challenging benchmarks How2QA and\nHow2R for Video QA and Retrieval, collected from diverse video content over\nmultimodalities.", "no": 95}, {"url": "https://arxiv.org/abs/2005.03545", "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal\n  Sentiment Analysis", "cites": "410", "abstract": "Multimodal Sentiment Analysis is an active area of research that leverages\nmultimodal signals for affective understanding of user-generated videos. The\npredominant approach, addressing this task, has been to develop sophisticated\nfusion techniques. However, the heterogeneous nature of the signals creates\ndistributional modality gaps that pose significant challenges. In this paper,\nwe aim to learn effective modality representations to aid the process of\nfusion. We propose a novel framework, MISA, which projects each modality to two\ndistinct subspaces. The first subspace is modality-invariant, where the\nrepresentations across modalities learn their commonalities and reduce the\nmodality gap. The second subspace is modality-specific, which is private to\neach modality and captures their characteristic features. These representations\nprovide a holistic view of the multimodal data, which is used for fusion that\nleads to task predictions. Our experiments on popular sentiment analysis\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\nmodels. We also consider the task of Multimodal Humor Detection and experiment\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\nthan strong baselines, establishing MISA as a useful multimodal framework.", "no": 96}, {"url": "https://arxiv.org/abs/2002.02562", "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "cites": "407", "abstract": "In this paper we present an end-to-end speech recognition model with\nTransformer encoders that can be used in a streaming speech recognition system.\nTransformer computation blocks based on self-attention are used to encode both\naudio and label sequences independently. The activations from both audio and\nlabel encoders are combined with a feed-forward layer to compute a probability\ndistribution over the label space for every combination of acoustic frame\nposition and label history. This is similar to the Recurrent Neural Network\nTransducer (RNN-T) model, which uses RNNs for information encoding instead of\nTransformer encoders. The model is trained with the RNN-T loss well-suited to\nstreaming decoding. We present results on the LibriSpeech dataset showing that\nlimiting the left context for self-attention in the Transformer layers makes\ndecoding computationally tractable for streaming, with only a slight\ndegradation in accuracy. We also show that the full attention version of our\nmodel beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our\nresults also show that we can bridge the gap between full attention and limited\nattention versions of our model by attending to a limited number of future\nframes.", "no": 97}, {"url": "https://arxiv.org/abs/2006.14779", "title": "Does the Whole Exceed its Parts? The Effect of AI Explanations on\n  Complementary Team Performance", "cites": "407", "abstract": "Many researchers motivate explainable AI with studies showing that human-AI\nteam performance on decision-making tasks improves when the AI explains its\nrecommendations. However, prior studies observed improvements from explanations\nonly when the AI, alone, outperformed both the human and the best team. Can\nexplanations help lead to complementary performance, where team accuracy is\nhigher than either the human or the AI working solo? We conduct mixed-method\nuser studies on three datasets, where an AI with accuracy comparable to humans\nhelps participants solve a task (explaining itself in some conditions). While\nwe observed complementary improvements from AI augmentation, they were not\nincreased by explanations. Rather, explanations increased the chance that\nhumans will accept the AI's recommendation, regardless of its correctness. Our\nresult poses new challenges for human-centered AI: Can we develop explanatory\napproaches that encourage appropriate trust in AI, and therefore help generate\n(or improve) complementary performance?", "no": 98}, {"url": "https://arxiv.org/abs/2004.08795", "title": "Extractive Summarization as Text Matching", "cites": "397", "abstract": "This paper creates a paradigm shift with regard to the way we build neural\nextractive summarization systems. Instead of following the commonly used\nframework of extracting sentences individually and modeling the relationship\nbetween sentences, we formulate the extractive summarization task as a semantic\ntext matching problem, in which a source document and candidate summaries will\nbe (extracted from the original text) matched in a semantic space. Notably,\nthis paradigm shift to semantic matching framework is well-grounded in our\ncomprehensive analysis of the inherent gap between sentence-level and\nsummary-level extractors based on the property of the dataset.\n  Besides, even instantiating the framework with a simple form of a matching\nmodel, we have driven the state-of-the-art extractive result on CNN/DailyMail\nto a new level (44.41 in ROUGE-1). Experiments on the other five datasets also\nshow the effectiveness of the matching framework. We believe the power of this\nmatching-based summarization framework has not been fully exploited. To\nencourage more instantiations in the future, we have released our codes,\nprocessed dataset, as well as generated summaries in\nhttps://github.com/maszhongming/MatchSum.", "no": 99}, {"url": "https://arxiv.org/abs/2006.05987", "title": "Revisiting Few-sample BERT Fine-tuning", "cites": "396", "abstract": "This paper is a study of fine-tuning of BERT contextual representations, with\nfocus on commonly observed instabilities in few-sample scenarios. We identify\nseveral factors that cause this instability: the common use of a non-standard\noptimization method with biased gradient estimation; the limited applicability\nof significant parts of the BERT network for down-stream tasks; and the\nprevalent practice of using a pre-determined, and small number of training\niterations. We empirically test the impact of these factors, and identify\nalternative practices that resolve the commonly observed instability of the\nprocess. In light of these observations, we re-visit recently proposed methods\nto improve few-sample fine-tuning with BERT and re-evaluate their\neffectiveness. Generally, we observe the impact of these methods diminishes\nsignificantly with our modified process.", "no": 100}]